üîß Developer Brief ‚Äì Final polish for Realtime Hebrew calls (no architecture changes)

Context

The current Twilio + OpenAI Realtime setup is working well overall, but there are a few remaining issues:
	‚Ä¢	The assistant does not fully respect the call settings configured in the UI (per business).
	‚Ä¢	Sometimes near the end of the call, the assistant gets cut off and then stays silent instead of clearly closing and hanging up.
	‚Ä¢	The first transcription at the start of the call is often wrong/noisy, then things stabilize.
	‚Ä¢	We want the assistant to speak and understand Hebrew at a very high, human-like level (good grammar, spelling, and contextual understanding).
	‚Ä¢	We do not want a big refactor ‚Äì only targeted tuning.

Please implement the following focused improvements.

‚∏ª

1Ô∏è‚É£ Respect per-business call settings from the UI

Goal: The realtime call behavior must follow the business settings in the UI (per business / per phone number).
	1.	Identify the model/structure where call settings are stored (for example):
	‚Ä¢	greeting_enabled / greeting_text
	‚Ä¢	max_call_duration_seconds
	‚Ä¢	max_assistant_turns
	‚Ä¢	silence_timeout_before_hangup
	‚Ä¢	require_final_confirmation (yes/no)
	‚Ä¢	language / locale (he-IL, etc.)
	2.	When a call starts (/webhook/incoming_call or when opening the Twilio media WebSocket):
	‚Ä¢	Resolve the business (business_id) and load its call settings from the DB.
	‚Ä¢	Build a CallConfig / ConversationConfig object with all relevant settings.
	‚Ä¢	Attach this config to the call/session state so it‚Äôs available in:
	‚Ä¢	incoming_call handler
	‚Ä¢	realtime / media-stream handler
	‚Ä¢	call_status / stream_ended webhooks
	3.	Replace hard-coded values in the call logic (max loops, timeouts, hangup behavior) with values from this CallConfig.
If a setting is missing, use a safe default, but never ignore existing values from the UI.

‚∏ª

2Ô∏è‚É£ Clean, reliable hangup behavior (no ‚Äústuck in silence‚Äù at the end)

Goal: The assistant should either finish with a clear closing sentence and hang up, or keep the conversation going. It should never get stuck in permanent silence.
	1.	Introduce explicit call states, for example:
	‚Ä¢	ACTIVE ‚Äì normal conversation
	‚Ä¢	CLOSING ‚Äì final sentence sent, call about to end
	‚Ä¢	ENDED ‚Äì call fully finished, resources cleaned
	2.	When the assistant decides to end the call (after collecting all required data and giving the final summary):
	‚Ä¢	Set state to CLOSING.
	‚Ä¢	Send the closing TTS sentence to the user.
	‚Ä¢	After TTS is done (or after a small safety delay, e.g. 1‚Äì2 seconds), trigger Twilio hangup:
	‚Ä¢	either by returning <Hangup> in TwiML, or
	‚Ä¢	calling Twilio‚Äôs REST API to end the call.
	‚Ä¢	Then set state to ENDED and close the Realtime session.
	3.	In call_status / stream_ended webhooks:
	‚Ä¢	If Twilio reports the call as completed, busy, no-answer etc., always:
	‚Ä¢	mark state as ENDED
	‚Ä¢	close the Realtime connection
	‚Ä¢	stop any timers and background tasks for that call
	4.	Add a simple silence-based auto-hangup using CallConfig.silence_timeout_before_hangup:
	‚Ä¢	Track last_user_speech_time and last_assistant_speech_time.
	‚Ä¢	If state is ACTIVE and there was no user speech and no assistant speech for more than silence_timeout_before_hangup seconds:
	‚Ä¢	send a short polite closing sentence (configured per business or default),
	‚Ä¢	then hang up as above.

This should eliminate the ‚Äúit got cut and stayed silent forever‚Äù behavior.

‚∏ª

3Ô∏è‚É£ Improve STT quality (especially first utterance) + VAD / barge-in tuning

Goal: The first user transcription should be more reliable, and the assistant should react to real speech, not noise, without over-complicating the pipeline.

3.1 First-transcription warmup
	1.	At the beginning of the media stream, for the first 600‚Äì800 ms:
	‚Ä¢	Treat STT results as ‚Äúwarmup‚Äù:
	‚Ä¢	Do not use the very first fragment as the final user answer.
	‚Ä¢	Either ignore the first fragment or require at least one more STT segment close in time that matches / reinforces it.
	2.	Only after this short warmup window, start treating STT output as normal user input.

This alone often fixes the ‚Äúfirst line is wrong, then it stabilizes‚Äù issue.

‚∏ª

3.2 Simple VAD tuning (keep it minimal)
	1.	At the start of the call, use the first ~1 second of audio to measure baseline noise RMS and calculate a dynamic silence threshold:
	‚Ä¢	silence_threshold = baseline_rms + margin (e.g. +15‚Äì20 units above baseline)
	2.	Maintain a small sliding window (e.g. 250‚Äì400 ms).
Mark user_is_speaking = True only if:
	‚Ä¢	RMS in that window is consistently above silence_threshold.
	3.	Only send audio chunks to STT / Realtime as ‚Äúuser speech‚Äù when user_is_speaking == True.
Very short spikes / low-energy noise ‚Üí ignored.
	4.	Keep this logic simple and contained in the audio processing layer.
No need for a complex DSP solution ‚Äî just a basic threshold + short window.

‚∏ª

3.3 Barge-in behavior
	1.	When the assistant is speaking (TTS in progress), allow barge-in only if:
	‚Ä¢	user_is_speaking == True for at least ~300‚Äì500 ms,
	‚Ä¢	not on tiny blips.
	2.	When barge-in is detected:
	‚Ä¢	Stop TTS cleanly.
	‚Ä¢	Capture the full utterance (until VAD says speech ended).
	‚Ä¢	Then send the captured text to the model as the user‚Äôs reply.

This keeps the assistant responsive but avoids cutting itself off for every background sound.
