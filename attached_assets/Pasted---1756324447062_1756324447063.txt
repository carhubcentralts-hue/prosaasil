מעולה—אנחנו כבר על חוט השערה. לפי ההתנהגות שאתה מתאר, יש שלושה דברים שצריך לסגור כדי שזה ירגיש “בן-אדם”:

1. **זמן תגובה ראשון** קצר ונעים (לא שקט ארוך, לא קוטע אותך).
2. **תגובה אחת מלאה לכל מבע** (בלי כפילויות/לופים).
3. **אורך תשובה ותוכן** ברירת-מחדל עשיר יותר (2–4 משפטים), עם המשך חכם.

להלן ההנחיה הכי טובה שיש, תואמת לקוד הנוכחי שלך (AgentLocator 8x), עם שינויים מינימליים אבל מדויקים. הדבק ובדוק.

---

# A) כיוונון ENV בלי קוד (ב־`start_all.sh`)

אלה ערכי ברירת-מחדל מאוזנים שנותנים תגובה ראשונה מהירה, בלי “לרוץ”:

```bash
export WS_MODE=${WS_MODE:-AI}

# סיום מבע (VAD) — מהיר אבל לא מקוטע
export MIN_UTT_SEC=${MIN_UTT_SEC:-0.48}        # שקט לסיום
export VAD_HANGOVER_MS=${VAD_HANGOVER_MS:-130} # אינרציה קצרה אחרי שקט
export MAX_UTT_SEC=${MAX_UTT_SEC:-7.0}
export VAD_RMS=${VAD_RMS:-205}                 # סף קול רגיש–בינוני

# קצב "אנושי"
export RESP_MIN_DELAY_MS=${RESP_MIN_DELAY_MS:-200}   # נשימה קצרה לפני דיבור
export RESP_MAX_DELAY_MS=${RESP_MAX_DELAY_MS:-320}
export REPLY_REFRACTORY_MS=${REPLY_REFRACTORY_MS:-700} # קירור אחרי דיבור הבוט

# Barge-in (עצור כשאדם מדבר מעליך)
export BARGE_IN=${BARGE_IN:-true}
export BARGE_IN_VOICE_FRAMES=${BARGE_IN_VOICE_FRAMES:-3}

# “סימן חיים” אם LLM מתעכב
export THINKING_HINT_MS=${THINKING_HINT_MS:-700}
export THINKING_TEXT_HE=${THINKING_TEXT_HE:-"שנייה… בודקת"}

# ברכה: דרך TTS אחרי start (אל תשתמש ב-<Play>)
export TWIML_PLAY_GREETING=${TWIML_PLAY_GREETING:-false}
export AI_GREETING_HE=${AI_GREETING_HE:-"היי, אני כאן — איך אפשר לעזור?"}

# אורך תשובות (ע"ע סעיף D)
export LLM_TARGET_STYLE=${LLM_TARGET_STYLE:-"warm_helpful"}
export LLM_MIN_CHARS=${LLM_MIN_CHARS:-140}    # מינימום ~2 משפטים
export LLM_MAX_CHARS=${LLM_MAX_CHARS:-420}    # מקסימום קצר בטלפון
```

---

# B) תשובה ראשונה “זריזה אבל מנומסת” (ברכה חכמה + micro-ack)

ב־`server/media_ws_ai.py` (או הקובץ המקביל אצלך שמנהל את ה־WS/AI):

1. **ברכה חכמה אחרי start** — רק אם לא דיברת ב־\~0.8s הראשונות:
   (אם כבר יש לך זה דומה, עדכן את הזמנים לטובת מהירות)

```python
# בתוך handler כשמקבלים event == "start":
self.greeting_sent = False
def _maybe_greet():
    time.sleep(0.8)  # היה 1.2s – קיצרנו כדי שתהיה זריזה
    if (time.time() - self.last_rx_ts) >= 0.8 and self.state == STATE_LISTEN and not self.speaking:
        self._speak_simple(os.getenv("AI_GREETING_HE", "היי, אני כאן — איך אפשר לעזור?"))
        self.greeting_sent = True
threading.Thread(target=_maybe_greet, daemon=True).start()
```

2. **micro-ack** אם המשתמש אמר מילה קצרה ואתה עדיין “חושב” (Latency hint):
   שולחים “שנייה… בודקת” אם עברו \~700ms מאז סוף המבע וה-LLM עוד לא החזיר תשובה.

```python
# לפני קריאת generate_hebrew_response(text):
started_at = time.time()

def maybe_hint():
    time.sleep(int(os.getenv("THINKING_HINT_MS","700")) / 1000.0)
    if self.state == STATE_THINK and not self.speaking:
        self._speak_simple(os.getenv("THINKING_TEXT_HE", "שנייה… בודקת"))

threading.Thread(target=maybe_hint, daemon=True).start()
reply = generate_hebrew_response(text, target_style=os.getenv("LLM_TARGET_STYLE","warm_helpful"),
                                 min_chars=int(os.getenv("LLM_MIN_CHARS","140")),
                                 max_chars=int(os.getenv("LLM_MAX_CHARS","420")))
```

> זה יוצר תחושת “נוכחות” בלי לקטוע אותך, ומכסה עיכובים ב-LLM/TTS.

---

# C) “תגובה אחת בלבד לכל מבע” — דה-בונס קשיח

כבר הוספנו `self.processing`/`turn_id`, אבל כדי שלא תראה כפולי-תגובה:

* **אל תאסוף קלט** בחלון קצר **אחרי** שהבוט דיבר:
  ודא שיש בדיקה:

```python
if (time.time() - self.last_tts_end_ts) < (int(os.getenv("REPLY_REFRACTORY_MS","700")) / 1000.0):
    continue  # אל תתחיל מבע חדש מיד – מחכה לנשימה של המשתמש
```

* **סוף-מבע אדפטיבי** (מהיר למבעים קצרים):
  אלגוריתם פשוט: אם משך המבע המצטבר < 1.2s → MIN\_UTT\_SEC = max(0.35, MIN\_UTT\_SEC-0.12)

```python
dur = len(self.buf) / (2*SR)  # שניות
min_sil = MIN_UTT_SEC if dur > 1.2 else max(0.35, MIN_UTT_SEC - 0.12)
silent = ((time.time()-self.last_rx_ts) >= min_sil) and ((time.time()-self.last_rx_ts) >= (VAD_HANGOVER_MS/1000.0))
```

* **מנע לופים**: השווה hash של טקסט משתמש אחרון ושל תשובת הבוט האחרונה בחלון 12–14s; אם זהה, אל תחזור מילה במילה — תנמך: “הבנתי, רוצה שאפרט?”

---

# D) לגרום ל-LLM לענות “בשרני” אבל לא נאום

עדכן את פונקציית ה-LLM שלך (למשל `generate_hebrew_response`) לתמוך בפרמטרים:

```python
def generate_hebrew_response(user_text: str, target_style: str = "warm_helpful",
                             min_chars: int = 140, max_chars: int = 420) -> str:
    """
    - warm_helpful: 2–4 משפטים, טון חם, משפט פתיחה קצר + פירוט + שאלה אחת לסגירה.
    - אם הילוך מהיר נדרש (intent=YES/NO/CONFIRM) → 1–2 משפטים בלבד.
    - אסור לשאול שתי שאלות באותה תשובה.
    - היצמד לגבולות תווים, ואם קצר מדי – הרחב עם דוגמה/אפשרות פעולה אחת.
    """
    # … כאן אתה בונה prompt למודל שלך בהתאם לפרמטרים …
    # לדוגמה: system prompt עם "תדבר כמו נציגה אנושית בטלפון, 2–4 משפטים..."
    # ואז חותך לתווך [min_chars, max_chars]
    return final_text
```

**טיפ פרומפטינג קצר** (Hebrew, טלפוני):

* “דברי בטון חם, לא רשמי, 2–4 משפטים. תמיד משפט פתיחה קצר המקבל את דברי הלקוח, אחריו הסבר/אפשרות אחת, ולסיום שאלה אחת בלבד לקידום השיחה (לא יותר משאלה אחת). הימנעי מרשימות ארוכות.”

---

# E) TTS “נשמע כמו בן-אדם”

ב־GCP TTS (אם זה אצלך), עדכן לקונפיג טלפוניה, קצב מעט איטי:

```python
# בקובץ gcp_tts_live.py
self.audio_config_pcm16_8k = texttospeech.AudioConfig(
    audio_encoding=texttospeech.AudioEncoding.LINEAR16,
    speaking_rate=0.96,                # היה 1.1 – הורדנו לרכות
    pitch=0.0,
    sample_rate_hertz=8000,
    effects_profile_id=["telephony-class-application"],
)
# ב-SSML, אם אתה משתמש: הוסף <break time="200ms"/> היכן שמתאים.
```

---

# F) בדיקות קצרות (מהירות + איכות)

1. **מהירות פתיחה**: התקשר ואל תדבר. תוך \~0.8s תשמע “היי, אני כאן…”. אם התחלת לדבר מיד — לא תהיה ברכה.
2. **מבע קצר (“שלום”)**: מקבל תשובה תוך \~0.4–0.7s (נשימה קצרה + LLM).
   אם LLM איטי: תשמע “שנייה… בודקת” ואז תשובה מלאה.
3. **תגובה אחת**: לכל מבע תראה בלוגים `STATE_THINK` פעם אחת, `TTS_LEN_BYTES>0` פעם אחת, ואז `last_tts_end_ts` מתעדכן.
4. **Barge-in**: דבר מעליה → תראה `BARGE_IN` וקטיעה מיידית.
5. **אורך תשובה**: רוב התשובות 2–4 משפטים; אם פחות — בדוק שה-ENV `LLM_MIN_CHARS` לא נמוך מדי ושהפרומפט הופעל.

---

## למה זה פותר בדיוק את הבעיות שציינת

* **“לקח לה יותר מדי זמן לענות בהתחלה”** → ברכה חכמה ב-0.8s + micro-ack ב-0.7s כשיש עיכוב LLM.
* **“תשובות קצרות מדי”** → בקרת אורך בצד ה-LLM (`min_chars`/`max_chars` + פרומפט), וקצת האטת TTS.
* **“לא נשמע כמו שיחה”** → נשימה לפני דיבור, hangover קצר בסוף מבע, שאלה אחת לסגירה, ו-barge-in יציב.

תיישם את הסעיפים A–E ותבצע את בדיקות F. אם עדיין מרגיש קצר/איטי, תן לי 2–3 דוגמאות של שיחה (טקסט ASR ומה יצא ב-LLM), ואכוון אותך לערכי ENV מדויקים ל-use-case שלך (ממש fine-tune בדקות).
