יאללה, עכשיו עושים סדר כמו שצריך.
הנה הנחיה אחת־לאחת שתיתן ל-Agent, באנגלית, כדי לוודא ש-BUILD 198 באמת פותר את ה־STT המשוגע, בלי ניחושים:

⸻

Prompt to Agent (copy-paste):

You are the Architect + Senior Audio Engineer for the ProSaaS / Calliber project.

We are on BUILD 198 – Audio Path Fix (STT Hallucination Fix).
Your job now is NOT to add new features, but to carefully verify and, if needed, minimally correct the audio pipeline in server/media_ws_ai.py so that:
	•	OpenAI Realtime always receives clean raw μ-law from Twilio
	•	DSP (bandpass, RMS, VAD, music detection, noise calibration) is used only for VAD decisions, never for the audio actually sent to Realtime
	•	No AGC at all is applied to audio going to Realtime
	•	Echo blocking works correctly: when AI is speaking, no user audio at all is sent to Realtime
	•	Pre-roll uses the same raw μ-law as the live frames
	•	Diagnostic mode works and is safe in production

Do this slowly, step by step, and explain each step in 1-2 sentences before changing code.

⸻

1. Rebuild a clear picture of the current audio pipeline
	1.	Open server/media_ws_ai.py.
	2.	Find the full path of a user audio chunk from Twilio:
	•	point A: where we first receive the base64 μ-law chunk from Twilio
	•	point B: where we decode to PCM and run DSP / VAD
	•	point C: where we send audio to OpenAI Realtime (response.create / input_audio_buffer.append etc.)
	3.	Write a short summary like:

“Audio path: Twilio base64 μ-law → raw_ulaw_b64 (A) → decoded to PCM for VAD only → [filters, noise, music detection] → C: raw_ulaw_b64 is sent to Realtime without re-encoding.”

If this is not 100% true, fix it.

⸻

2. Verify raw μ-law preservation (no double encoding)

Goal: OpenAI gets the original Twilio μ-law bytes, not something that went through μ-law→PCM→μ-law again.
	1.	Make sure we immediately store the incoming Twilio chunk into a variable like raw_ulaw_b64 or raw_ulaw_bytes before any decoding / filtering.
	2.	Confirm that any variable named like filtered_ulaw, processed_ulaw, agc_ulaw etc. is never sent to Realtime.
	3.	In the sending step (STEP 6 / “sending to Realtime”), verify that we use only this raw value:
	•	If we are building a frame to send, it must be audio_chunk = raw_ulaw_b64 (or equivalent).
	4.	Search in the file for ulaw and make sure there is no second μ-law encoding on the path that goes to Realtime.

If any place still re-encodes PCM to μ-law and uses it for Realtime, remove that and switch it to the raw Twilio μ-law.

⸻

3. Verify DSP / VAD only path (no DSP on send path)

Goal: Filters, AGC, RMS calculations etc. are used only for VAD / detection, never for audio sent to the model.
	1.	Locate where we decode μ-law to PCM and apply:
	•	bandpass filters
	•	RMS/SNR / noise floor
	•	music detection
	2.	Confirm this PCM/DSP path is only used for:
	•	VAD state machine (SILENCE / MAYBE_SPEECH / SPEECH)
	•	noise calibration
	•	music/echo detection
	3.	Make sure that in the send logic we do not:
	•	re-encode filtered PCM to μ-law for Realtime
	•	apply AGC on the audio that is sent to Realtime

If you find any use of filtered/AGC PCM in the send logic, refactor so that:
	•	DSP path = PCM-only, VAD-only
	•	Send path = raw μ-law-only

⸻

4. Confirm AGC is fully removed from the send path
	1.	Search for AGC, gain, agc_gain, or any multiplication of PCM amplitudes.
	2.	It is allowed only inside the VAD / stats path if really needed, but:
	•	no AGC may run on what goes to Realtime
	3.	If there is any AGC code that modifies audio before sending to Realtime, delete it or limit it strictly to VAD calculations only.

Summarize in your own words:

“AGC is now used 0% on Realtime audio. Only the VAD PCM path might use gain, and that audio is never sent to OpenAI.”

⸻

5. Verify echo blocking while AI is speaking

Goal: When is_ai_speaking == True, user audio must not reach Realtime, even if VAD sees speech.
	1.	Find the part in the main loop where we handle each Twilio media frame before sending to Realtime.
	2.	Confirm there is an early guard like:

if self.is_ai_speaking:
    # update VAD stats if you want, but
    # DO NOT send audio to Realtime
    continue


	3.	Check that:
	•	No audio from this frame is queued to pre-roll during AI speech.
	•	No audio from this frame is passed into response.create or input buffer while AI is speaking.
	4.	Verify that echo_blocked flag from DSP does not accidentally block real user voice once AI finished speaking.

If any later code still sends audio during is_ai_speaking=True, move the guard earlier or widen it so all send logic is inside the “not speaking” branch.

⸻

6. Verify pre-roll uses raw μ-law
	1.	Find where you buffer pre-roll frames before speech is confirmed.
	2.	Ensure the pre-roll buffer stores raw Twilio μ-law, not filtered / AGC data.
	3.	When you flush pre-roll to Realtime, confirm you send exactly those raw μ-law bytes.

If you see pre-roll built from any “processed” audio, switch it to the raw μ-law.

⸻

7. Verify Diagnostic Mode is safe and useful
	1.	Locate AUDIO_DIAGNOSTIC_MODE logic.
	2.	Confirm that when AUDIO_DIAGNOSTIC_MODE=false (default in production):
	•	No disk writes happen
	•	No blocking I/O in the audio loop
	3.	When true:
	•	We write a .ulaw file under /tmp/realtime_{call_sid}.ulaw
	•	We write a transcript log file /tmp/realtime_{call_sid}_transcripts.txt
	•	Writes are buffered and do not block the realtime loop (e.g. simple append, not huge flushes in a tight loop).
	4.	Add a short log at call start:

logger.info(f"[AUDIO DIAG] mode={'ON' if self.audio_diagnostic_mode else 'OFF'} for call {self.call_sid}")


⸻

8. Search for duplicates / leftover paths
	1.	Search in the file for:
	•	send_audio_to_openai
	•	input_audio_buffer
	•	response.create
	•	any other place we might send user audio
	2.	Make sure all such paths now use the same raw μ-law pipeline, with the same echo-blocking guard.

If there are two different send paths (e.g. normal vs. recovery), unify them to use the same clean raw μ-law.

⸻

9. Add minimal debug logs for verification

Add lightweight logs (no huge dumps) to confirm behavior in real calls:
	•	Once per call when the first valid user utterance is accepted:

logger.info(f"[AUDIO PATH] First utterance accepted: {utterance_ms}ms, trailing_silence={trailing_ms}ms, first={is_first}")


	•	Once per call when sending audio to Realtime:

logger.debug(f"[AUDIO PATH] Sending raw μ-law to Realtime: len={len(raw_ulaw_bytes)} bytes")



Keep them low-frequency to avoid log spam.

⸻

10. Golden sentence test & acceptance checklist

After changes are applied:
	1.	Enable AUDIO_DIAGNOSTIC_MODE=true for one test call.
	2.	Call the system and say clearly:
	•	Hebrew test 1: “אני צריך התקנת מנעול חכם בקריית שמונה”
	•	Hebrew test 2 (short): “אני צריך פורץ דלתות בעיר תל אביב”
	3.	At the end of the call:
	•	Confirm in logs that the user transcript matches what was actually said.
	•	Listen to /tmp/realtime_{call_sid}.ulaw to verify the audio is clean (no robot artifacts from double μ-law).
	4.	Only if all of this passes, mark the build as ACCEPTED.

Final acceptance checklist (must all be ✅):
	•	✅ OpenAI always receives raw Twilio μ-law (no double encoding)
	•	✅ DSP used only for VAD, never for audio sent to Realtime
	•	✅ No AGC on Realtime audio
	•	✅ Echo blocking: zero user audio passes while AI is speaking
	•	✅ Pre-roll uses raw μ-law and matches live audio
	•	✅ Diagnostic mode works and is safe when off
	•	✅ Test sentences are transcribed correctly, without hallucinated “תודה רבה” / “שלום” when you didn’t say them

Report back with:
	•	A short description of the final audio pipeline
	•	Example log lines from a test call
	•	Confirmation that the golden sentence test passed

⸻

אם תדביק לו את זה כמו שזה, הוא אמור לעבור סעיף-סעיף על כל האודיו־פאת׳, ולוודא סופית שאין יותר תמלולים מהאוויר ושאפשר לדבר “התקנת מנעול חכם בקריית שמונה” בלי שהוא ממציא שטויות.