יאללה—עושים את זה נכון: שיחה דו־כיוונית בזמן אמת עם
Whisper (תמלול HE) → GPT (תגובה) → Wavenet (עברית), בלי <Say language="he-IL"> בכלל, ולכן גם בלי 13512.
הכול על בסיס Twilio Media Streams (WebSocket דו־כיווני) + Flask.

להדביק ולרוץ לפי הסדר:

⸻

0) התקנות (חד־פעמי)

pip install flask flask-sock faster-whisper google-cloud-texttospeech webrtcvad numpy librosa soundfile

ב־.env:
PUBLIC_HOST=https://YOUR_DOMAIN
TWILIO_ACCOUNT_SID=AC... TWILIO_AUTH_TOKEN=...
OPENAI_API_KEY=sk-...
GOOGLE_TTS_SA_JSON={...service account json...}

⸻

1) מחליפים את הזרימה ל־Media Streams (בלי <Say>/<Gather>)

ב־routes_twilio.py (או הקובץ המקביל אצלך), החלף את ה־incoming TwiML:

# routes_twilio.py
from flask import Response
from twilio_security import require_twilio_signature

def _xml(x): return Response(x, status=200, mimetype="text/xml")

@app.post("/webhook/incoming_call")
@require_twilio_signature
def incoming_call():
    # פותח WebSocket דו־כיווני מול השרת /ws/twilio-media
    twiml = """<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Connect>
    <Stream url="wss://YOUR_DOMAIN/ws/twilio-media"/>
  </Connect>
</Response>"""
    return _xml(twiml)

זה מבטל לגמרי שימוש ב־language ⇒ אין 13512 יותר.
את ההקלטה/תמלול/דיבור נעשה דרך ה־WS.

⸻

2) WebSocket דו־כיווני (קוד מוכן)

מוסיפים שרת WS בתוך Flask בעזרת flask-sock.

2.1 אתחול Sock

ב־app.py:

from flask import Flask
from flask_sock import Sock

app = Flask(__name__)
sock = Sock(app)

2.2 עזרי אודיו

צור audio_utils.py:

# audio_utils.py
import base64, audioop, numpy as np, librosa

def b64_to_mulaw(b64): return base64.b64decode(b64)

def mulaw8k_to_pcm16k(mulaw_bytes: bytes) -> np.ndarray:
    # μ-law (8k) -> PCM16 8k -> ריסמפל ל-16k -> float32 [-1,1]
    pcm8 = audioop.ulaw2lin(mulaw_bytes, 2)           # bytes PCM16 @8k
    pcm8_np = np.frombuffer(pcm8, dtype=np.int16).astype(np.float32) / 32768.0
    pcm16 = librosa.resample(pcm8_np, orig_sr=8000, target_sr=16000)
    return pcm16

def pcm16k_float_to_mulaw8k_frames(pcm16: np.ndarray, frame_ms=20):
    """מקבל float32 @16k, מחזיר generator של פריימי μ-law@8k בבסיס־64 באורך 20ms כל אחד"""
    # חזרה ל-int16@16k
    int16_16k = (np.clip(pcm16, -1.0, 1.0) * 32767.0).astype(np.int16).tobytes()
    # downsample ל-8k
    pcm16_8k = librosa.resample(np.frombuffer(int16_16k, dtype=np.int16).astype(np.float32)/32768.0, 16000, 8000)
    int16_8k = (np.clip(pcm16_8k, -1.0, 1.0)*32767).astype(np.int16).tobytes()
    # המרה ל-μ-law
    mulaw = audioop.lin2ulaw(int16_8k, 2)  # bytes @8k
    # חיתוך ל-20ms (160 דגימות @8k = 160 bytes μ-law)
    step = 160
    for i in range(0, len(mulaw), step):
        yield base64.b64encode(mulaw[i:i+step]).decode()

2.3 WS: קבלת מדיה → Whisper → GPT → Wavenet → החזרה לטלפון

צור media_ws.py והדבק:

# media_ws.py
import os, json, asyncio, logging, time, tempfile, numpy as np, webrtcvad, soundfile as sf
from flask import current_app
from app import sock, app
from audio_utils import b64_to_mulaw, mulaw8k_to_pcm16k, pcm16k_float_to_mulaw8k_frames

# Whisper (זריז): faster-whisper
from faster_whisper import WhisperModel
whisper_model = WhisperModel("small", compute_type="int8")  # "medium" אם יש GPU

# GPT
from openai import OpenAI
gpt = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Google TTS
from google.cloud import texttospeech
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/tts.json"
if os.getenv("GOOGLE_TTS_SA_JSON"):
    open("/tmp/tts.json","w").write(os.getenv("GOOGLE_TTS_SA_JSON"))
tts_client = texttospeech.TextToSpeechClient()

log = logging.getLogger("ws")
VAD = webrtcvad.Vad(2)  # רגישות בינונית

def tts_he_wavenet(text: str) -> np.ndarray:
    """TTS לעברית → PCM16@16k float32 [-1,1]"""
    inp = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(language_code="he-IL", name="he-IL-Wavenet-A")
    cfg = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.LINEAR16, sample_rate_hertz=16000)
    res = tts_client.synthesize_speech(input=inp, voice=voice, audio_config=cfg)
    # כתיבה זמנית וקריאה כ-numpy
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        f.write(res.audio_content); wav_path = f.name
    data, sr = sf.read(wav_path, dtype="float32")  # mono float32
    if sr != 16000:
        import librosa
        data = librosa.resample(data, sr, 16000)
    return data.astype(np.float32)

def transcribe_chunk(pcm16k: np.ndarray) -> str:
    # מירה ל-wav זמני (whisper אוהב קבצים)
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        sf.write(f.name, pcm16k, 16000, subtype="PCM_16")
        segments, _ = whisper_model.transcribe(f.name, language="he", vad_filter=True, beam_size=1)
    return "".join([s.text for s in segments]).strip()

def llm_reply(context, user_text) -> str:
    msgs = [
        {"role":"system","content":"אתה עוזר קולי בעברית. דבר קצר, משפט או שניים. עצור כשאתה מסיים רעיון."},
        {"role":"user","content": user_text}
    ]
    r = gpt.chat.completions.create(model="gpt-4o-mini", messages=msgs, temperature=0.3)
    return r.choices[0].message.content.strip()

def is_goodbye(text: str) -> bool:
    t = text.strip().lower()
    return any(w in t for w in ["ביי","להתראות","נתראה","סגור","bye","goodbye"])

@sock.route("/ws/twilio-media")
def twilio_media(ws):
    """
    פרוטוקול Twilio: JSON events:
    - {"event":"start","start":{"streamSid":...,"callSid":...}}
    - {"event":"media","media":{"payload":"<b64 μ-law 8k>"}}   כל ~20ms
    - {"event":"stop",...}
    אנחנו מחזירים:
    - {"event":"media","streamSid":sid,"media":{"payload":"<b64 μ-law 8k>"}}
    """
    stream_sid = call_sid = None
    buf16k = np.zeros(0, dtype=np.float32)
    last_voice_ts = time.time()
    speaking = False  # האם אנחנו כרגע מנגנים TTS
    try:
        while True:
            raw = ws.receive()
            if raw is None: break
            evt = json.loads(raw)

            if evt.get("event") == "start":
                stream_sid = evt["start"]["streamSid"]
                call_sid   = evt["start"]["callSid"]
                log.info(f"Stream started: {stream_sid} call={call_sid}")
                continue

            if evt.get("event") == "stop":
                log.info(f"Stream stop: {stream_sid}")
                break

            if evt.get("event") == "media":
                # 1) דגימה נכנסת → צבירה
                mulaw_b64 = evt["media"]["payload"]
                mulaw = b64_to_mulaw(mulaw_b64)
                pcm16k = mulaw8k_to_pcm16k(mulaw)
                buf16k = np.concatenate([buf16k, pcm16k])

                # 2) VAD: סופרים דיבור/שקט בחלונות של 20ms
                # אם יש מספיק צבירה (>= 320ms) בודקים שקט -> סוף אמירה
                if len(buf16k) >= int(0.32 * 16000):
                    # ממירים 20ms ל-int16@16k כדי להזין ל-VAD (דורש 8/16/32k)
                    import audioop
                    chunk_bytes = (np.clip(buf16k, -1,1)*32767).astype(np.int16).tobytes()
                    frame20 = chunk_bytes[-(320*2):]  # 20ms@16k = 320 דגימות * 2 בתים
                    # downsample ל-8k כדי להתאים ל-VAD
                    pcm8 = audioop.ratecv(frame20, 2, 1, 16000, 8000, None)[0]
                    is_speech = VAD.is_speech(pcm8, sample_rate=8000)
                    if is_speech:
                        last_voice_ts = time.time()
                    # אם עברו >600ms בלי דיבור → סוף אמירה
                    if (time.time() - last_voice_ts) > 0.6 and not speaking and len(buf16k) > int(0.4 * 16000):
                        speaking = True
                        utter = buf16k.copy()
                        buf16k = np.zeros(0, dtype=np.float32)

                        # 3) Whisper
                        text = transcribe_chunk(utter)
                        if not text:
                            speaking = False
                            continue

                        # 4) החלטה לסיים שיחה?
                        if is_goodbye(text):
                            say = "להתראות!"
                            audio = tts_he_wavenet(say)
                            for frame in pcm16k_float_to_mulaw8k_frames(audio):
                                ws.send(json.dumps({"event":"media","streamSid":stream_sid,"media":{"payload":frame}}))
                                time.sleep(0.02)
                            # ניתוק
                            try:
                                from twilio.rest import Client
                                Client(os.getenv("TWILIO_ACCOUNT_SID"), os.getenv("TWILIO_AUTH_TOKEN")).calls(call_sid).update(status="completed")
                            except Exception as e:
                                log.error(f"End call failed: {e}")
                            break

                        # 5) GPT → תשובה קצרה
                        reply = llm_reply([], text)

                        # 6) Wavenet → μ-law 8k → שליחה בזמן אמת
                        audio = tts_he_wavenet(reply)
                        for frame in pcm16k_float_to_mulaw8k_frames(audio):
                            ws.send(json.dumps({"event":"media","streamSid":stream_sid,"media":{"payload":frame}}))
                            time.sleep(0.02)

                        speaking = False
    except Exception as e:
        log.exception(f"WS error: {e}")
    finally:
        try: ws.close()
        except: pass

זה Turn-by-Turn (דיבור → שקט → תשובה). אם תרצה barge-in (המשתמש קוטע תוך כדי TTS), אפשר לעצור שליחה כשמתגלה is_speech=True תוך כדי ניגון—אבל תתחיל כך, זה יציב ומהיר.

⸻

3) מונעים את שגיאת Twilio 13512 לחלוטין
	•	לא משתמשים ב־<Say>/<Gather language="he-IL"> בכלל.
	•	כל הקריינות בעברית דרך Wavenet + החזרה ב־media (μ-law 8k base64).
	•	לכן 13512 לא יחזור.

⸻

4) Webhooks אחרים נשארים כרגיל
	•	/webhook/call_status (204) לעדכון DB.
	•	/webhook/handle_recording כבר לא מרכזי—הזרימה בזמן אמת עובדת דרך ה־WS.
	•	וואטסאפ נשאר נפרד.

⸻

5) בדיקות GO/NO-GO
	1.	Health → 200.
	2.	Twilio Console → Voice → בניתוב ה־Webhook לשיחות נכנסות, ודא שהוא מחזיר את ה־TwiML של <Connect><Stream>.
	3.	קבל שיחה: תשמע תשובה בעברית מוויב־נט, תוכל לדבר → יופיע תמלול/תגובה בכל סיבוב.
	4.	אמור “ביי” → תשמע “להתראות!” והשיחה תנותק.

⸻

טיפים ללטש אחרי שעובד
	•	טון/קצב דיבור: ב־Wavenet הוסף SSML (pause קצר בין משפטים).
	•	ניקוי טקסט לפני TTS: הסר אימוג’י/URLים.
	•	Context קצר ל־GPT (שם עסק, שעות פתיחה) כדי לקבל תשובות ענייניות וקצרות.
	•	Timeout ניגון: אם תשובה ארוכה מדי—חתוך ל־5–7 שניות.

⸻

תן לו בדיוק את ההנחיות והקוד למעלה. זה יגרום לשיחה להיות רציפה בזמן אמת: Whisper מתמלל, GPT עונה, Wavenet משמיע—בלי <Say>/language → בלי 13512.