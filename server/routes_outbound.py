"""
BUILD 174: Outbound Calls API Routes
×©×™×—×•×ª ×™×•×¦××•×ª - API ×œ×”×ª×—×œ×ª ×©×™×—×•×ª AI ×™×•×¦××•×ª ×œ×œ×™×“×™×

Endpoints:
- POST /api/outbound_calls/start - Start outbound calls to leads
- GET /api/outbound_calls/templates - Get outbound call templates
- GET /api/outbound_calls/counts - Get current active call counts
"""
import os
import re
import logging
import json
import uuid
from datetime import datetime, timedelta
from threading import Thread
from urllib.parse import quote  # ğŸ”§ BUILD 177: URL encode Hebrew characters
from sqlalchemy import func
from sqlalchemy import text
from flask import Blueprint, jsonify, request, g, session
from server.models_sql import db, CallLog, Lead, Business, OutboundCallTemplate, BusinessSettings, OutboundCallRun, OutboundCallJob, RecordingRun, ContactIdentity
from server.auth_api import require_api_auth
from server.security.permissions import require_page_access
from server.services.call_limiter import check_call_limits, get_call_counts, MAX_TOTAL_CALLS_PER_BUSINESS, MAX_OUTBOUND_CALLS_PER_BUSINESS
from twilio.rest import Client

log = logging.getLogger(__name__)

# âœ… Compile regex pattern once at module level for performance
STATUS_FILTER_PATTERN = re.compile(r'^[a-zA-Z0-9_-]+$')

# Background job stale threshold - jobs stuck longer than this are marked as failed
BACKGROUND_JOB_STALE_THRESHOLD_MINUTES = 10


def check_and_handle_duplicate_background_job(job_type: str, business_id: int, error_message: str):
    """
    Check for existing active background job and handle it appropriately.
    
    This function prevents duplicate key violations on the idx_background_jobs_unique_active
    constraint by checking for existing active jobs before creating a new one.
    
    Args:
        job_type: The type of background job (e.g., 'enqueue_outbound_calls')
        business_id: The business ID
        error_message: Hebrew error message to show user if active job exists
        
    Returns:
        tuple: (can_proceed: bool, error_response: dict or None, status_code: int or None)
        - If can_proceed is True, caller should create the new job
        - If can_proceed is False, caller should return the error_response with status_code
    """
    from server.models_sql import BackgroundJob
    
    # ğŸ”’ CHECK: Look for existing active job to avoid unique constraint violation
    # The idx_background_jobs_unique_active constraint prevents multiple active jobs
    # (status in 'queued', 'running', 'paused') of the same type per business
    existing_job = BackgroundJob.query.filter_by(
        business_id=business_id,
        job_type=job_type
    ).filter(
        BackgroundJob.status.in_(['queued', 'running', 'paused'])
    ).first()
    
    if not existing_job:
        # No existing job, safe to proceed
        return (True, None, None)
    
    # Check if job is stale (stuck for more than threshold without heartbeat)
    now = datetime.utcnow()
    stale_threshold = timedelta(minutes=BACKGROUND_JOB_STALE_THRESHOLD_MINUTES)
    
    # Determine if job is stale based on heartbeat or created_at
    last_activity = existing_job.heartbeat_at or existing_job.created_at
    is_stale = (now - last_activity) > stale_threshold
    
    if is_stale:
        # Mark stale job as failed and allow new job to be created
        log.warning(f"âš ï¸ Found stale background job {existing_job.id} (status={existing_job.status}, last_activity={last_activity}). Marking as failed.")
        existing_job.status = 'failed'
        existing_job.last_error = 'Job marked as stale - exceeded timeout without heartbeat'
        existing_job.finished_at = now
        db.session.commit()
        return (True, None, None)
    else:
        # Active job exists, cannot create new one
        log.error(f"âŒ Active background job {existing_job.id} already exists for business {business_id}")
        db.session.rollback()
        error_response = {
            "error": error_message,
            "active_job_id": existing_job.id,
            "active_job_status": existing_job.status
        }
        # Add success: False for endpoints that expect it
        if 'success' not in error_response:
            error_response['success'] = False
        return (False, error_response, 409)


def normalize_israeli_phone(phone: str) -> str:
    """
    Normalize Israeli phone number to E.164 format (+972XXXXXXXXX)
    
    Handles:
    - +972XXXXXXXXX -> +972XXXXXXXXX (already E.164)
    - 972XXXXXXXXX -> +972XXXXXXXXX (missing +)
    - 0XXXXXXXXX -> +972XXXXXXXXX (local format)
    - 05XXXXXXXX -> +9725XXXXXXXX (mobile local)
    
    Returns the original string if it can't be normalized
    """
    if not phone:
        return phone
    
    # Remove all non-digit characters except +
    cleaned = re.sub(r'[^\d+]', '', phone.strip())
    
    # Already in E.164 format with +972
    if cleaned.startswith('+972'):
        return cleaned
    
    # Has 972 prefix but missing +
    if cleaned.startswith('972') and len(cleaned) >= 12:
        return '+' + cleaned
    
    # Israeli local format (starts with 0)
    if cleaned.startswith('0') and len(cleaned) >= 10:
        # Remove leading 0 and add +972
        return '+972' + cleaned[1:]
    
    # If it already has a different country code (starts with +), keep it
    if cleaned.startswith('+'):
        return cleaned
    
    # Fallback: assume it's missing the leading 0 for Israeli mobile
    if len(cleaned) == 9 and cleaned[0] in '5':  # Israeli mobile without 0
        return '+972' + cleaned
    
    # Return original if we can't normalize
    log.warning(f"Could not normalize phone number: {phone}")
    return phone

outbound_bp = Blueprint("outbound", __name__)


def _get_lead_display_name(lead) -> str | None:
    """
    Get display name for lead - SSOT for name extraction from Lead model.
    
    ğŸ”¥ NAME SSOT: Single function for lead name extraction
    Used when storing lead name in CallLog.customer_name or OutboundCallJob.lead_name
    
    Returns:
        - Lead.full_name (if available)
        - "first_name last_name" (if both available)
        - first_name or last_name (if only one available)
        - None (if no name available)
    """
    if not lead:
        return None
    
    # Try full_name property first
    full_name = getattr(lead, 'full_name', None)
    if full_name and full_name != "×œ×œ× ×©×":  # Don't use default placeholder
        return full_name
    
    # Fallback: Construct from first_name + last_name
    first = (lead.first_name or "").strip()
    last = (lead.last_name or "").strip()
    
    if first and last:
        return f"{first} {last}"
    elif first:
        return first
    elif last:
        return last
    
    return None


def get_business_phone(business_id: int) -> str | None:
    """Get the business phone number for outbound calls"""
    business = Business.query.get(business_id)
    if business and business.phone_e164:
        return business.phone_e164
    
    # ğŸ”¥ BUILD 186 FIX: Handle missing columns gracefully
    try:
        settings = BusinessSettings.query.filter_by(tenant_id=business_id).first()
        if settings and settings.phone_number:
            return settings.phone_number
    except Exception as db_err:
        log.warning(f"âš ï¸ Could not load settings for {business_id} (DB schema issue): {db_err}")
    
    return None


def get_public_host() -> str:
    """
    Get public host for webhook URLs
    
    CRITICAL: Must return valid host for Twilio callbacks to work
    Fails fast if no valid host configured (prevents silent failures)
    
    Returns:
        Valid hostname (never empty/None)
    
    Raises:
        RuntimeError: If no valid host is configured
    """
    public_host = os.environ.get('PUBLIC_HOST', '').replace('https://', '').replace('http://', '').rstrip('/')
    if public_host:
        return public_host
    
    # Fallback to Replit domains
    replit_host = os.environ.get('REPLIT_DEV_DOMAIN') or os.environ.get('REPLIT_DOMAINS', '').split(',')[0]
    if replit_host:
        return replit_host
    
    # Localhost fallback only for development
    if os.environ.get('FLASK_ENV') == 'development' or os.environ.get('DEBUG') == '0':
        log.warning("âš ï¸ Using localhost for webhooks - this will not work in production!")
        return 'localhost'
    
    # FAIL FAST: No valid host in production
    error_msg = (
        "âŒ CRITICAL: No public host configured for webhooks!\n"
        "Set PUBLIC_HOST environment variable to your domain (e.g., myapp.com)\n"
        "Without this, Twilio callbacks will fail and calls will not work."
    )
    log.error(error_msg)
    raise RuntimeError(error_msg)


@outbound_bp.route("/api/outbound_calls/templates", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_outbound_templates():
    """
    Get outbound call templates for current business
    
    Returns list of active templates with their prompts
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    # For system_admin without tenant context, return empty list (they need to view a specific business)
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"templates": [], "message": "×‘×—×¨ ×¢×¡×§ ×œ×¦×¤×™×™×” ×‘×ª×‘× ×™×•×ª"})
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        templates = OutboundCallTemplate.query.filter_by(
            business_id=tenant_id,
            is_active=True
        ).order_by(OutboundCallTemplate.name).all()
        
        result = []
        for t in templates:
            result.append({
                "id": t.id,
                "name": t.name,
                "description": t.description,
                "prompt_text": t.prompt_text,
                "greeting_template": t.greeting_template
            })
        
        return jsonify({"templates": result})
    
    except Exception as e:
        log.error(f"Error fetching templates: {e}")
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª ×ª×‘× ×™×•×ª"}), 500


@outbound_bp.route("/api/outbound_calls/counts", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_call_counts_endpoint():
    """
    Get current active call counts for the business
    
    Returns:
        active_total: Total active calls
        active_outbound: Active outbound calls
        max_total: Maximum allowed total calls
        max_outbound: Maximum allowed outbound calls
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    # For system_admin without tenant context, return zero counts
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({
                "active_total": 0,
                "active_outbound": 0,
                "max_total": MAX_TOTAL_CALLS_PER_BUSINESS,
                "max_outbound": MAX_OUTBOUND_CALLS_PER_BUSINESS,
                "message": "×‘×—×¨ ×¢×¡×§ ×œ×¦×¤×™×™×” ×‘×©×™×—×•×ª ×¤×¢×™×œ×•×ª"
            })
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        counts = get_call_counts(tenant_id)
        return jsonify(counts)
    except Exception as e:
        log.error(f"Error getting call counts: {e}")
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª × ×ª×•× ×™×"}), 500


@outbound_bp.route("/api/inbound_calls/counts", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_inbound')
def get_inbound_call_counts_endpoint():
    """
    Get current active call counts for the business (inbound endpoint)
    
    This endpoint provides the same data as /api/outbound_calls/counts
    but is used by the inbound calls page for consistency.
    
    Returns:
        active_total: Total active calls
        active_outbound: Active outbound calls
        max_total: Maximum allowed total calls
        max_outbound: Maximum allowed outbound calls
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    # For system_admin without tenant context, return zero counts
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({
                "active_total": 0,
                "active_outbound": 0,
                "max_total": MAX_TOTAL_CALLS_PER_BUSINESS,
                "max_outbound": MAX_OUTBOUND_CALLS_PER_BUSINESS,
                "message": "×‘×—×¨ ×¢×¡×§ ×œ×¦×¤×™×™×” ×‘×©×™×—×•×ª ×¤×¢×™×œ×•×ª"
            })
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        counts = get_call_counts(tenant_id)
        return jsonify(counts)
    except Exception as e:
        log.error(f"Error getting call counts: {e}")
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª × ×ª×•× ×™×"}), 500


def _validate_project_access(project_id: int, tenant_id: int) -> bool:
    """
    Validate that project exists and belongs to tenant.
    
    Args:
        project_id: Project ID to validate
        tenant_id: Tenant ID to check ownership
        
    Returns:
        True if project exists and belongs to tenant, False otherwise
    """
    if not project_id:
        return True  # No project_id is valid (not all calls are from projects)
    
    project_exists = db.session.execute(text("""
        SELECT id FROM outbound_projects WHERE id = :project_id AND tenant_id = :tenant_id
    """), {'project_id': project_id, 'tenant_id': tenant_id}).scalar()
    
    return bool(project_exists)


def _start_bulk_queue(tenant_id: int, lead_ids: list, project_id: int = None) -> tuple:
    """
    Helper function to start bulk call queue with concurrency control
    
    Creates a run with concurrency=3 and starts background worker
    Returns JSON response with run_id and queued count
    
    Args:
        tenant_id: Business/tenant ID
        lead_ids: List of lead IDs to call
        project_id: Optional project ID to associate calls with
    """
    try:
        # Verify all leads belong to this tenant
        leads = Lead.query.filter(
            Lead.id.in_(lead_ids),
            Lead.tenant_id == tenant_id
        ).all()
        
        if len(leads) != len(lead_ids):
            return jsonify({"error": "×œ× × ××¦××• ×›×œ ×”×œ×™×“×™× ×©× ×‘×—×¨×•"}), 404
        
        # Create run with concurrency=3 and status=pending
        # Worker will update to "running" when it picks up the run
        run = OutboundCallRun()
        run.business_id = tenant_id
        run.concurrency = MAX_OUTBOUND_CALLS_PER_BUSINESS  # 3
        run.total_leads = len(lead_ids)
        run.queued_count = len(lead_ids)
        run.status = "pending"  # ğŸ”¥ FIX: Start as pending, worker will update to running
        db.session.add(run)
        db.session.flush()
        
        # Create jobs for each lead
        for lead_id in lead_ids:
            job = OutboundCallJob()
            job.run_id = run.id
            job.lead_id = lead_id
            job.status = "queued"
            # ğŸ”¥ NAME SSOT: Store lead name for NAME_ANCHOR system
            lead_obj = next((l for l in leads if l.id == lead_id), None)
            if lead_obj:
                job.lead_name = _get_lead_display_name(lead_obj)
            # ğŸ¯ PROJECT TRACKING: Associate job with project if provided
            if project_id:
                job.project_id = project_id
            db.session.add(job)
        
        db.session.commit()
        
        log.info(f"âœ… Created bulk call run {run.id} with {len(lead_ids)} leads, concurrency={MAX_OUTBOUND_CALLS_PER_BUSINESS}, project_id={project_id}")
        
        # ğŸ”¥ CRITICAL FIX: DO NOT start thread-based consumer
        # Instead, enqueue to RQ worker to ensure single consumer pattern
        # This prevents duplicate processing when both thread AND RQ worker run
        try:
            from rq import Queue
            import redis
            import os
            
            REDIS_URL = os.getenv('REDIS_URL')
            if not REDIS_URL:
                log.error("REDIS_URL not configured, cannot enqueue outbound calls job")
                # Fallback: Mark run as failed
                run.status = "failed"
                run.last_error = "Redis not configured"
                db.session.commit()
                return jsonify({"error": "×ª×•×¨ ×”×©×™×—×•×ª ××™× ×• ×–××™×Ÿ"}), 503
            
            redis_conn = redis.from_url(REDIS_URL)
            queue = Queue('default', connection=redis_conn)
            
            # Create BackgroundJob for tracking
            from server.models_sql import BackgroundJob
            bg_job = BackgroundJob()
            bg_job.business_id = tenant_id
            bg_job.job_type = 'enqueue_outbound_calls'
            bg_job.status = 'queued'
            bg_job.total = len(lead_ids)
            bg_job.cursor = json.dumps({'run_id': run.id})
            bg_job.created_at = datetime.utcnow()
            db.session.add(bg_job)
            db.session.flush()
            
            # Enqueue to RQ worker
            from server.jobs.enqueue_outbound_calls_job import enqueue_outbound_calls_batch_job
            job = queue.enqueue(
                enqueue_outbound_calls_batch_job,
                bg_job.id,
                job_timeout='2h',
                failure_ttl=86400
            )
            
            bg_job.rq_job_id = job.id
            db.session.commit()
            
            log.info(f"âœ… Enqueued outbound calls job {bg_job.id} (RQ job {job.id}) for run {run.id}")
            
        except Exception as e:
            log.error(f"Failed to enqueue outbound calls job: {e}")
            # Mark run as failed
            run.status = "failed"
            run.last_error = f"Failed to enqueue: {str(e)}"
            db.session.commit()
            return jsonify({"error": "×©×’×™××” ×‘×”×•×¡×¤×ª ×©×™×—×•×ª ×œ×ª×•×¨"}), 500
        
        return jsonify({
            "success": True,
            "message": f"×”×•×¤×¢×œ×• {len(lead_ids)} ×©×™×—×•×ª ×‘×ª×•×¨ ({MAX_OUTBOUND_CALLS_PER_BUSINESS} ×‘××§×‘×™×œ)",
            "run_id": run.id,
            "queued": len(lead_ids),
            "mode": "bulk_queue"
        }), 201
        
    except Exception as e:
        log.error(f"Error creating bulk call run: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": f"×©×’×™××” ×‘×”×¤×¢×œ×ª ×”×©×™×—×•×ª: {str(e)}"}), 500


@outbound_bp.route("/api/outbound_calls/start", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def start_outbound_calls():
    """
    Start outbound AI calls to selected leads.
    Uses outbound_ai_prompt from business settings.
    
    For 1-3 leads: Starts calls immediately in parallel
    For >3 leads: Uses bulk queue system with concurrency control (max 3 concurrent)
    
    Request body:
    {
        "lead_ids": [123, 456, 789, ...]  // Any number of leads
    }
    
    Returns (for 1-3 leads):
    {
        "success": true,
        "calls": [
            {"lead_id": 123, "call_sid": "CA...", "status": "initiated"},
            ...
        ]
    }
    
    Returns (for >3 leads):
    {
        "success": true,
        "message": "×”×•×¤×¢×œ×• 100 ×©×™×—×•×ª ×‘×ª×•×¨ (3 ×‘××§×‘×™×œ)",
        "run_id": 123,
        "queued": 100,
        "mode": "bulk_queue"
    }
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ×”×¤×¢×œ×ª ×©×™×—×•×ª ×™×•×¦××•×ª"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "× ×ª×•× ×™× ×—×¡×¨×™×"}), 400
    
    lead_ids = data.get("lead_ids", [])
    project_id = data.get("project_id")  # Optional project_id for tracking
    
    if not lead_ids or not isinstance(lead_ids, list):
        return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×œ×¤×—×•×ª ×œ×™×“ ××—×“"}), 400
    
    # Validate project_id if provided
    if project_id and not _validate_project_access(project_id, tenant_id):
        return jsonify({"error": "×¤×¨×•×™×§×˜ ×œ× × ××¦×"}), 404
    
    # âœ… FIX: If more than 3 leads, use bulk queue system with concurrency control
    # This ensures only 3 calls run in parallel, and as each completes, the next one starts
    if len(lead_ids) > MAX_OUTBOUND_CALLS_PER_BUSINESS:
        log.info(f"ğŸ“ Starting bulk queue for {len(lead_ids)} leads (concurrency={MAX_OUTBOUND_CALLS_PER_BUSINESS})")
        return _start_bulk_queue(tenant_id, lead_ids, project_id=project_id)
    
    # For 1-3 leads, use immediate parallel start (original behavior)
    allowed, error_msg = check_call_limits(tenant_id, len(lead_ids))
    if not allowed:
        return jsonify({"error": error_msg}), 429
    
    try:
        leads = Lead.query.filter(
            Lead.id.in_(lead_ids),
            Lead.tenant_id == tenant_id
        ).all()
        
        if len(leads) != len(lead_ids):
            return jsonify({"error": "×œ× × ××¦××• ×›×œ ×”×œ×™×“×™× ×©× ×‘×—×¨×•"}), 404
        
        from_phone = get_business_phone(tenant_id)
        if not from_phone:
            return jsonify({"error": "××¡×¤×¨ ×˜×œ×¤×•×Ÿ ×©×œ ×”×¢×¡×§ ×œ× ××•×’×“×¨"}), 400
        
        business = Business.query.get(tenant_id)
        business_name = business.name if business else "×”×¢×¡×§"
        
        host = get_public_host()
        
        results = []
        
        for lead in leads:
            if not lead.phone_e164:
                results.append({
                    "lead_id": lead.id,
                    "lead_name": lead.full_name,
                    "status": "failed",
                    "error": "××™×Ÿ ××¡×¤×¨ ×˜×œ×¤×•×Ÿ ×œ×œ×™×“"
                })
                continue
            
            # Normalize phone number to E.164 format
            normalized_phone = normalize_israeli_phone(lead.phone_e164)
            log.info(f"ğŸ“ Phone normalization: {lead.phone_e164} -> {normalized_phone}")
            
            try:
                call_log = CallLog()
                call_log.business_id = tenant_id
                call_log.lead_id = lead.id
                call_log.from_number = from_phone
                call_log.to_number = normalized_phone  # Use normalized phone
                call_log.direction = "outbound"
                call_log.status = "initiated"
                call_log.call_status = "initiated"
                # ğŸ”¥ NAME SSOT: Store lead name for NAME_ANCHOR system
                call_log.customer_name = _get_lead_display_name(lead)
                # ğŸ¯ PROJECT TRACKING: Associate call with project if provided
                if project_id:
                    call_log.project_id = project_id
                db.session.add(call_log)
                db.session.flush()
                
                lead_name = lead.full_name or "×œ×§×•×—"
                
                # ğŸ”¥ SSOT: Use centralized outbound call service (prevents duplicates + saves cost)
                from server.services.twilio_outbound_service import create_outbound_call
                
                result = create_outbound_call(
                    to_phone=normalized_phone,
                    from_phone=from_phone,
                    business_id=tenant_id,
                    host=host,
                    lead_id=lead.id,
                    business_name=business_name,
                    lead_name=lead_name
                )
                
                call_sid = result["call_sid"]
                is_duplicate = result.get("is_duplicate", False)
                
                if is_duplicate:
                    log.warning(f"âš ï¸ [DEDUP] Skipping duplicate call for lead={lead.id}")
                    continue
                
                call_log.call_sid = call_sid
                call_log.status = "ringing"
                call_log.call_status = "ringing"
                db.session.commit()
                
                log.info(f"ğŸ“ Outbound call started: lead={lead.id}, call_sid={call_sid}")
                
                results.append({
                    "lead_id": lead.id,
                    "lead_name": lead_name,
                    "call_sid": call_sid,
                    "status": "initiated"
                })
            
            except Exception as e:
                log.error(f"Failed to start call to lead {lead.id}: {e}")
                db.session.rollback()
                results.append({
                    "lead_id": lead.id,
                    "lead_name": lead.full_name,
                    "status": "failed",
                    "error": str(e)
                })
        
        successful_calls = [r for r in results if r.get("status") == "initiated"]
        
        return jsonify({
            "success": len(successful_calls) > 0,
            "message": f"×”×•×¤×¢×œ×• {len(successful_calls)} ×©×™×—×•×ª ××ª×•×š {len(lead_ids)}",
            "calls": results
        })
        
    except Exception as e:
        log.error(f"Error starting outbound calls: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": f"×©×’×™××” ×‘×”×¤×¢×œ×ª ×”×©×™×—×•×ª: {str(e)}"}), 500


@outbound_bp.route("/api/outbound_calls/jobs/<int:job_id>/status", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_outbound_job_status(job_id: int):
    """
    Get real-time status of an outbound call queue job (OutboundCallRun)
    
    Returns:
    {
        "job_id": 123,
        "status": "running",
        "total": 45,
        "processed": 12,
        "success": 10,
        "failed": 2,
        "in_progress": 3,
        "queued": 30,
        "progress_pct": 26.7,
        "can_cancel": true,
        "cancel_requested": false
    }
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can view any job
            pass
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        run = OutboundCallRun.query.get(job_id)
        
        if not run:
            return jsonify({"error": "×ª×•×¨ ×œ× × ××¦×"}), 404
        
        # Verify access if not system admin
        if tenant_id and run.business_id != tenant_id:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×ª×•×¨ ×–×”"}), 403
        
        # Calculate processed count
        processed = run.completed_count + run.failed_count
        
        # Calculate progress percentage
        progress_pct = 0
        if run.total_leads > 0:
            progress_pct = round((processed / run.total_leads) * 100, 1)
        
        # Can cancel if running or queued
        can_cancel = run.status in ('running', 'queued') and not run.cancel_requested
        
        return jsonify({
            "job_id": run.id,
            "status": run.status,
            "total": run.total_leads,
            "processed": processed,
            "success": run.completed_count,
            "failed": run.failed_count,
            "in_progress": run.in_progress_count,
            "queued": run.queued_count,
            "progress_pct": progress_pct,
            "can_cancel": can_cancel,
            "cancel_requested": run.cancel_requested,
            "created_at": run.created_at.isoformat() if run.created_at else None,
            "completed_at": run.completed_at.isoformat() if run.completed_at else None
        })
        
    except Exception as e:
        log.error(f"Error getting job status: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "×©×’×™××” ×‘×§×‘×œ×ª ××¦×‘ ×”×ª×•×¨"}), 500


@outbound_bp.route("/api/outbound_calls/jobs/<int:job_id>/cancel", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def cancel_outbound_job(job_id: int):
    """
    Request cancellation of an outbound call queue job/run
    
    ğŸ”’ SECURITY: Enforces business isolation - users can only cancel runs for their business
    
    ğŸ”¥ SMART CANCEL: If queue is stale (>15min no heartbeat), force-cancels immediately
    Otherwise, sets cancel_requested=True for graceful shutdown.
    
    The worker will:
    1. Stop starting new calls
    2. Mark remaining queued jobs as cancelled
    3. Set run status to cancelled
    
    Returns:
    {
        "success": true,
        "message": "×‘×§×©×ª ×‘×™×˜×•×œ × ×©×œ×—×”"
    }
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can cancel any job
            pass
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        run = OutboundCallRun.query.get(job_id)
        
        if not run:
            return jsonify({"error": "×ª×•×¨ ×œ× × ××¦×"}), 404
        
        # ğŸ”’ SECURITY: Verify access if not system admin
        if tenant_id and run.business_id != tenant_id:
            # ğŸ”’ SECURITY: Log potential cross-business access attempt
            log.warning(f"[SECURITY] User from business {tenant_id} attempted to cancel run {job_id} which belongs to business {run.business_id}")
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×ª×•×¨ ×–×”"}), 403
        
        # ğŸ”’ SECURITY: Double-check business_id matches (defensive programming)
        if tenant_id:
            if run.business_id != tenant_id:
                log.error(f"[SECURITY] Business ID mismatch in cancel: run.business_id={run.business_id} != tenant_id={tenant_id}")
                return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×ª×•×¨ ×–×”"}), 403
        
        # Check if already cancelled or completed
        if run.status in ('cancelled', 'completed', 'failed', 'stopped'):
            return jsonify({
                "success": False,
                "message": f"×”×ª×•×¨ ×›×‘×¨ ×‘××¦×‘ {run.status}"
            }), 400
        
        # ğŸ”¥ SMART CANCEL: Check if queue is stale (force-cancel if needed)
        now = datetime.utcnow()
        stale_ttl = timedelta(minutes=15)
        last_activity = run.last_heartbeat_at or run.lock_ts or run.updated_at
        
        is_stale = False
        if last_activity:
            time_since_activity = now - last_activity
            is_stale = time_since_activity > stale_ttl
        
        if is_stale:
            # Force cancel immediately since worker appears dead
            log.warning(f"ğŸ”¥ [SMART_CANCEL] Run {job_id} is stale - force-cancelling")
            
            run.status = 'cancelled'
            run.cancel_requested = True
            run.ended_at = now
            run.completed_at = now  # ğŸ”¥ AUTO-FINALIZE
            run.updated_at = now
            run.locked_by_worker = None
            run.lock_ts = None
            run.last_heartbeat_at = None  # Clear for consistency
            
            # Mark all pending jobs as failed
            from sqlalchemy import text
            result = db.session.execute(text("""
                UPDATE outbound_call_jobs 
                SET status='failed',
                    error_message='Queue stale - force cancelled',
                    completed_at=NOW()
                WHERE run_id=:run_id 
                    AND business_id=:business_id
                    AND status IN ('queued', 'dialing')
            """), {"run_id": job_id, "business_id": run.business_id})
            
            cancelled_count = result.rowcount
            db.session.commit()
            
            return jsonify({
                "success": True,
                "message": f"×”×ª×•×¨ ×‘×•×˜×œ ××™×™×“×™×ª (×ª×•×¨ ×ª×§×•×¢ - {cancelled_count} ×©×™×—×•×ª ×‘×•×˜×œ×•)",
                "force_cancelled": True
            })
        else:
            # Graceful cancel - let worker shut down properly
            run.cancel_requested = True
            run.updated_at = now
            db.session.commit()
            
            log.info(f"ğŸ“ [business_id={run.business_id}] Cancellation requested for run {job_id} by business {tenant_id or 'system_admin'}")
            
            return jsonify({
                "success": True,
                "message": "×‘×§×©×ª ×‘×™×˜×•×œ × ×©×œ×—×” - ×”×ª×•×¨ ×™×•×¤×¡×§ ×‘×§×¨×•×‘"
            })
        
    except Exception as e:
        log.error(f"Error cancelling job: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘×‘×™×˜×•×œ ×”×ª×•×¨"}), 500


@outbound_bp.route("/api/outbound_calls/jobs/<int:job_id>/force-cancel", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def force_cancel_outbound_job(job_id: int):
    """
    Force cancel an outbound call queue job/run
    
    ğŸ”¥ FORCE CANCEL: Works even if worker is dead/unresponsive
    - Immediately marks run as 'cancelled'
    - Marks all queued jobs as failed
    - Clears worker lock
    - Cleans up Redis semaphore slots
    
    ğŸ”’ SECURITY: Enforces business isolation - users can only cancel runs for their business
    
    Returns:
    {
        "success": true,
        "message": "×”×ª×•×¨ ×‘×•×˜×œ ××™×™×“×™×ª",
        "jobs_cancelled": 15
    }
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can force cancel any job
            pass
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        run = OutboundCallRun.query.get(job_id)
        
        if not run:
            return jsonify({"error": "×ª×•×¨ ×œ× × ××¦×"}), 404
        
        # ğŸ”’ SECURITY: Verify access if not system admin
        if tenant_id and run.business_id != tenant_id:
            # ğŸ”’ SECURITY: Log potential cross-business access attempt
            log.warning(f"[SECURITY] User from business {tenant_id} attempted to force-cancel run {job_id} which belongs to business {run.business_id}")
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×ª×•×¨ ×–×”"}), 403
        
        # Check if already in terminal state
        if run.status in ('cancelled', 'completed', 'failed', 'stopped'):
            return jsonify({
                "success": True,
                "message": f"×”×ª×•×¨ ×›×‘×¨ ×‘××¦×‘ {run.status}",
                "jobs_cancelled": 0
            })
        
        # ğŸ”¥ FORCE CANCEL: Immediately mark as cancelled
        now = datetime.utcnow()
        run.status = 'cancelled'
        run.cancel_requested = True
        run.ended_at = now
        run.completed_at = now  # ğŸ”¥ AUTO-FINALIZE: Set completed_at to prevent stuck progress bar
        run.updated_at = now
        
        # Clear worker lock and heartbeat for consistency
        run.locked_by_worker = None
        run.lock_ts = None
        run.last_heartbeat_at = None
        
        # Mark all queued/dialing jobs as failed
        from sqlalchemy import text
        result = db.session.execute(text("""
            UPDATE outbound_call_jobs 
            SET status='failed',
                error_message='Force cancelled by user',
                completed_at=NOW()
            WHERE run_id=:run_id 
                AND business_id=:business_id
                AND status IN ('queued', 'dialing')
        """), {"run_id": job_id, "business_id": run.business_id})
        
        cancelled_count = result.rowcount
        db.session.commit()
        
        # ğŸ”¥ CLEANUP: Clean up Redis semaphore slots for this business
        try:
            from server.services.outbound_semaphore import cleanup_expired_slots
            cleaned_slots = cleanup_expired_slots(run.business_id)
            if cleaned_slots > 0:
                log.info(f"ğŸ§¹ [FORCE_CANCEL] Cleaned {cleaned_slots} Redis slots for business {run.business_id}")
        except Exception as e:
            log.warning(f"âš ï¸ [FORCE_CANCEL] Failed to cleanup Redis slots: {e}")
        
        log.info(f"ğŸ”¥ [FORCE_CANCEL] Run {job_id} force-cancelled by business {tenant_id or 'system_admin'}. Cancelled {cancelled_count} jobs.")
        
        return jsonify({
            "success": True,
            "message": f"×”×ª×•×¨ ×‘×•×˜×œ ××™×™×“×™×ª ({cancelled_count} ×©×™×—×•×ª ×‘×•×˜×œ×•)",
            "jobs_cancelled": cancelled_count
        })
        
    except Exception as e:
        log.error(f"Error force-cancelling job: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘×‘×™×˜×•×œ ×”×ª×•×¨"}), 500


@outbound_bp.route("/api/outbound_calls/jobs/active", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_active_outbound_job():
    """
    Get the currently active outbound call job for this business
    
    ğŸ”’ IRON RULE: Only return truly active queues to prevent stuck progress bars
    
    Returns a queue ONLY if ALL conditions are met:
    - status = 'running' (not queued/stopped/completed/failed/cancelled)
    - cancel_requested = false
    - completed_at is null (not finished)
    - last_heartbeat_at/updated_at within 15 minutes TTL
    
    ğŸ”¥ STALE AUTO-FINALIZE: Automatically marks stale runs as 'failed' if:
    - status='running' but last_heartbeat_at > 15 minutes old
    - This prevents 2 businesses from having stuck progress bars
    
    Returns:
    {
        "ok": true,
        "active": true/false,
        "job_id": 123 or null,
        "run_id": 123 or null,
        "status": "running" or null,
        ...same fields as get_outbound_job_status...
    }
    
    Always returns 200 (never 404) to prevent UI error loops
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    # Helper function for consistent inactive response
    def return_inactive_response():
        return jsonify({
            "ok": True,
            "active": False,
            "job_id": None,
            "run_id": None,
            "status": None,
            "total": 0,
            "processed": 0,
            "success": 0,
            "failed": 0,
            "in_progress": 0,
            "queued": 0,
            "queue_len": 0,
            "progress_pct": 0,
            "can_cancel": False,
            "cancel_requested": False,
            "concurrency": None,
            "created_at": None,
            "completed_at": None,
            "last_activity": None
        }), 200
    
    try:
        # ğŸ”’ IRON RULE: Strict "active queue" definition
        # Only return queues that are TRULY active (not stuck/stale/finished)
        now = datetime.utcnow()
        stale_ttl = timedelta(minutes=15)  # 15 minute TTL as per requirements
        
        # Find most recent run matching ALL active criteria
        run = OutboundCallRun.query.filter(
            OutboundCallRun.business_id == tenant_id,
            OutboundCallRun.status == 'running',  # ONLY running (not queued)
            OutboundCallRun.cancel_requested.is_(False),  # NOT cancelled (SQLAlchemy explicit False check)
            OutboundCallRun.completed_at.is_(None),  # NOT finished (use completed_at for compatibility)
            OutboundCallRun.ended_at.is_(None)  # NOT finished (use ended_at as primary)
        ).order_by(OutboundCallRun.created_at.desc()).first()
        
        if not run:
            # ğŸ”¥ FIX: Return 200 with active=false instead of 404 to prevent UI loops
            return return_inactive_response()
        
        # ğŸ”¥ STALE AUTO-FINALIZE: Check if run is stale (TTL exceeded)
        # Determine last activity time
        last_activity = run.last_heartbeat_at or run.lock_ts or run.updated_at
        
        if not last_activity:
            # Edge case: No activity timestamp at all (should never happen but defensive)
            log.warning(f"ğŸ”¥ [STALE_AUTO_FINALIZE] Run {run.id} has no activity timestamp - marking as failed")
            run.status = 'failed'
            run.ended_at = now
            run.completed_at = now
            run.last_error = "Queue has no activity timestamp"
            run.locked_by_worker = None
            run.lock_ts = None
            run.last_heartbeat_at = None
            db.session.commit()
            # ğŸ”¥ FIX: Return 200 with active=false instead of 404 to prevent UI loops
            return return_inactive_response()
        
        time_since_activity = now - last_activity
        
        if time_since_activity > stale_ttl:
                # AUTO-FINALIZE: Mark as failed and clean up
                elapsed_minutes = int(time_since_activity.total_seconds() / 60)
                log.warning(
                    f"ğŸ”¥ [STALE_AUTO_FINALIZE] Run {run.id} exceeded TTL "
                    f"(last_activity={last_activity}, {elapsed_minutes}m ago). "
                    f"Auto-marking as failed."
                )
                
                run.status = 'failed'
                run.ended_at = now
                run.completed_at = now  # Set both for compatibility
                run.last_error = f"Queue stale - no activity for {elapsed_minutes} minutes (TTL={stale_ttl.total_seconds()/60:.0f}m)"
                
                # Clear lock fields and heartbeat for consistency
                run.locked_by_worker = None
                run.lock_ts = None
                run.last_heartbeat_at = None
                
                # Mark remaining queued jobs as failed
                from sqlalchemy import text
                result = db.session.execute(text("""
                    UPDATE outbound_call_jobs 
                    SET status='failed',
                        error_message='Queue stale - auto-finalized',
                        completed_at=NOW()
                    WHERE run_id=:run_id 
                        AND business_id=:business_id
                        AND status IN ('queued', 'dialing')
                """), {"run_id": run.id, "business_id": run.business_id})
                
                cancelled_count = result.rowcount
                log.info(f"ğŸ”¥ [STALE_AUTO_FINALIZE] Marked {cancelled_count} pending jobs as failed")
                
                db.session.commit()
                
                # ğŸ”¥ FIX: Return 200 with active=false instead of 404 to prevent UI loops
                # This fixes the 2 businesses with stuck progress bars
                return return_inactive_response()
        
        # Calculate processed count
        processed = run.completed_count + run.failed_count
        
        # Calculate progress percentage
        progress_pct = 0
        if run.total_leads > 0:
            progress_pct = round((processed / run.total_leads) * 100, 1)
        
        # Can cancel if running and not already cancelled
        can_cancel = run.status == 'running' and not run.cancel_requested
        
        return jsonify({
            "ok": True,
            "active": True,
            "job_id": run.id,
            "run_id": run.id,  # Alias for compatibility
            "status": run.status,
            "total": run.total_leads,
            "processed": processed,
            "success": run.completed_count,
            "failed": run.failed_count,
            "in_progress": run.in_progress_count,
            "queued": run.queued_count,
            "queue_len": run.queued_count,  # Alias for compatibility
            "progress_pct": progress_pct,
            "can_cancel": can_cancel,
            "cancel_requested": run.cancel_requested,
            "concurrency": run.concurrency,
            "created_at": run.created_at.isoformat() if run.created_at else None,
            "completed_at": run.completed_at.isoformat() if run.completed_at else None,
            "last_activity": last_activity.isoformat() if last_activity else None  # Include for debugging
        })
        
    except Exception as e:
        log.error(f"Error getting active job: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "×©×’×™××” ×‘×§×‘×œ×ª ×ª×•×¨ ×¤×¢×™×œ"}), 500


@outbound_bp.route("/api/outbound_calls/templates", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def create_outbound_template():
    """
    Create a new outbound call template
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ×™×¦×™×¨×ª ×ª×‘× ×™×ª"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "× ×ª×•× ×™× ×—×¡×¨×™×"}), 400
    
    name = data.get("name", "").strip()
    description = data.get("description", "").strip()
    prompt_text = data.get("prompt_text", "").strip()
    greeting_template = data.get("greeting_template", "").strip()
    
    if not name:
        return jsonify({"error": "×©× ×”×ª×‘× ×™×ª ×—×•×‘×”"}), 400
    
    if not prompt_text:
        return jsonify({"error": "×”× ×—×™×•×ª ×œ×©×™×—×” ×—×•×‘×”"}), 400
    
    try:
        template = OutboundCallTemplate()
        template.business_id = tenant_id
        template.name = name
        template.description = description
        template.prompt_text = prompt_text
        template.greeting_template = greeting_template
        template.is_active = True
        db.session.add(template)
        db.session.commit()
        
        log.info(f"âœ… Created outbound template: id={template.id}, name={name}")
        
        return jsonify({
            "success": True,
            "template": {
                "id": template.id,
                "name": template.name,
                "description": template.description,
                "prompt_text": template.prompt_text,
                "greeting_template": template.greeting_template
            }
        }), 201
        
    except Exception as e:
        log.error(f"Error creating template: {e}")
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘×™×¦×™×¨×ª ×”×ª×‘× ×™×ª"}), 500


@outbound_bp.route("/api/outbound_calls/templates/<int:template_id>", methods=["DELETE"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def delete_outbound_template(template_id: int):
    """
    Soft delete (deactivate) an outbound call template
    """
    from flask import session
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ××—×™×§×ª ×ª×‘× ×™×ª"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        template = OutboundCallTemplate.query.filter_by(
            id=template_id,
            business_id=tenant_id
        ).first()
        
        if not template:
            return jsonify({"error": "×ª×‘× ×™×ª ×œ× × ××¦××”"}), 404
        
        template.is_active = False
        db.session.commit()
        
        return jsonify({"success": True, "message": "×”×ª×‘× ×™×ª × ××—×§×”"})
        
    except Exception as e:
        log.error(f"Error deleting template: {e}")
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘××—×™×§×ª ×”×ª×‘× ×™×ª"}), 500


# ========================================================
# BUILD 182: Outbound Import Leads API
# ×™×™×‘×•× ×œ×™×“×™× ××§×•×‘×¥ CSV ×œ×©×™×—×•×ª ×™×•×¦××•×ª
# ========================================================

MAX_IMPORTED_LEADS_PER_BUSINESS = 5000


@outbound_bp.route("/api/outbound/import-leads", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def import_outbound_leads():
    """
    BUILD 182: Import leads from CSV for outbound calls
    
    Accepts CSV/Excel/JSON and tries to auto-map columns. Only phone is required.
    Maximum 5000 leads per business total.
    
    Returns:
    {
        "success": true,
        "list_id": 123,
        "imported_count": 50,
        "skipped_count": 2,
        "errors_sample": ["×©×•×¨×” 3: ×˜×œ×¤×•×Ÿ ×œ× ×ª×§×™×Ÿ"],
        "mapping": {"phone": "×˜×œ×¤×•×Ÿ", "name": "×©×", "method": "header|content|fallback"}
    }
    """
    import csv
    import io
    from flask import session
    from server.models_sql import OutboundLeadList
    from typing import Any
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ×™×™×‘×•× ×œ×™×“×™×"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        # Count existing imported leads for this business
        existing_count = Lead.query.filter_by(
            tenant_id=tenant_id,
            source="imported_outbound"
        ).count()

        # -----------------------------
        # Forgiving parsing + auto-mapping
        # -----------------------------
        header_synonyms = {
            "phone": [
                "phone", "phone_number", "mobile", "cell", "tel", "telephone",
                "×˜×œ×¤×•×Ÿ", "× ×™×™×“", "××¡×¤×¨", "××¡ ×˜×œ×¤×•×Ÿ", "××¡×¤×¨ ×˜×œ×¤×•×Ÿ"
            ],
            "name": [
                "name", "full_name", "full name", "customer", "client",
                "×©×", "×©× ××œ×", "×œ×§×•×—"
            ],
            "city": ["city", "address", "×¢×™×¨", "×›×ª×•×‘×ª"],
            "notes": ["notes", "comment", "comments", "remark", "×”×¢×¨×•×ª", "×”×¢×¨×”"],
        }

        def _norm_header(h: str) -> str:
            return re.sub(r"\s+", " ", (h or "").strip().lower())

        def _looks_like_header_cell(cell: str) -> bool:
            c = (cell or "").strip()
            if not c:
                return False
            # headers are usually short and do not contain many digits
            digits = sum(ch.isdigit() for ch in c)
            letters = sum(ch.isalpha() for ch in c)
            return digits == 0 and (letters > 0 or len(c) <= 20)

        def _decode_bytes(raw: bytes) -> str:
            for enc in ("utf-8-sig", "utf-8", "cp1255", "iso-8859-8"):
                try:
                    return raw.decode(enc)
                except Exception:
                    continue
            # last resort
            return raw.decode("utf-8", errors="replace")

        def _strip_phone_chars(s: str) -> str:
            return re.sub(r"[^\d+]", "", (s or "").strip())

        def _normalize_phone_forgiving(raw_phone: str) -> str | None:
            """
            Forgiving phone normalization.
            Returns E.164-like string (e.g. +9725XXXXXXXX) or None if invalid.
            """
            if not raw_phone:
                return None

            cleaned = _strip_phone_chars(raw_phone)
            if not cleaned:
                return None

            # If starts with + and not Israeli, accept if it's plausibly E.164
            if cleaned.startswith("+") and not cleaned.startswith("+972"):
                digits_only = re.sub(r"\D", "", cleaned)
                if 10 <= len(digits_only) <= 15:
                    return cleaned  # keep as provided (+country...)
                return None

            digits = re.sub(r"\D", "", cleaned)

            # Israeli: +972XXXXXXXXX / 972XXXXXXXXX
            if digits.startswith("972"):
                national = digits[3:]
                # some exports include leading 0 after 972 -> strip it
                if national.startswith("0"):
                    national = national[1:]
                if 8 <= len(national) <= 10:
                    return "+972" + national
                return None

            # Israeli local: 0XXXXXXXX (landline 9) / 05XXXXXXXX (mobile 10)
            if digits.startswith("0"):
                national = digits[1:]
                if 8 <= len(national) <= 9:
                    return "+972" + national
                return None

            # Missing leading 0 (common in exports)
            # Mobile without 0: 5XXXXXXXX (9 digits)
            if len(digits) == 9 and digits.startswith("5"):
                return "+972" + digits
            # Landline without 0: 2/3/4/8/9 + 7 digits (8 digits total)
            if len(digits) == 8 and digits[0] in "23489":
                return "+972" + digits

            return None

        def _is_phone_like(value: Any) -> bool:
            if value is None:
                return False
            v = str(value).strip()
            if not v:
                return False
            return _normalize_phone_forgiving(v) is not None

        def _guess_phone_col(data_rows: list[list[str]]) -> int | None:
            if not data_rows:
                return None
            col_count = max(len(r) for r in data_rows)
            if col_count == 0:
                return None
            sample = data_rows[:200]
            best_idx = None
            best_ratio = 0.0
            for i in range(col_count):
                values = [r[i] if i < len(r) else "" for r in sample]
                non_empty = [v for v in values if str(v).strip()]
                if not non_empty:
                    continue
                phone_like = sum(1 for v in non_empty if _is_phone_like(v))
                ratio = phone_like / max(1, len(non_empty))
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_idx = i
            # Accept if at least half looks like phone OR it's the only plausible column
            if best_idx is not None and best_ratio >= 0.35:
                return best_idx
            # fallback: if only 2 columns, pick the more phone-like one even if ratio is low
            if col_count == 2:
                scores = []
                for i in range(2):
                    values = [r[i] if i < len(r) else "" for r in sample]
                    non_empty = [v for v in values if str(v).strip()]
                    phone_like = sum(1 for v in non_empty if _is_phone_like(v))
                    scores.append(phone_like)
                return 0 if scores[0] >= scores[1] else 1
            return best_idx

        def _guess_name_col(data_rows: list[list[str]], phone_idx: int | None) -> int | None:
            if not data_rows:
                return None
            col_count = max(len(r) for r in data_rows)
            if col_count == 0:
                return None
            sample = data_rows[:200]
            best_idx = None
            best_score = -1.0
            for i in range(col_count):
                if phone_idx is not None and i == phone_idx:
                    continue
                values = [str(r[i]).strip() if i < len(r) and r[i] is not None else "" for r in sample]
                non_empty = [v for v in values if v]
                if not non_empty:
                    continue
                # "name-like": mostly letters/spaces, not phone-like, short-ish
                name_like = 0
                for v in non_empty:
                    if _is_phone_like(v):
                        continue
                    if len(v) > 60:
                        continue
                    if any(ch.isalpha() for ch in v):
                        name_like += 1
                score = name_like / max(1, len(non_empty))
                if score > best_score:
                    best_score = score
                    best_idx = i
            # If only 2 cols and phone col exists, assume the other is name
            if best_idx is None and col_count == 2 and phone_idx is not None:
                return 1 - phone_idx
            return best_idx

        def _find_header_index(headers: list[str], target: str) -> int | None:
            syns = {_norm_header(s) for s in header_synonyms.get(target, [])}
            for i, h in enumerate(headers):
                if _norm_header(h) in syns:
                    return i
            return None

        def _auto_name_from_phone(normalized_phone: str) -> str:
            digits_only = re.sub(r"\D", "", normalized_phone or "")
            last4 = digits_only[-4:] if len(digits_only) >= 4 else ""
            return f"×œ×™×“ {last4}" if last4 else "×œ×™×“ ×—×“×©"

        def _parse_rows_from_request() -> tuple[list[list[str]], dict[str, Any]]:
            """
            Returns (table_rows, meta)
            - table_rows: list of rows (each row is list of strings), including header row if present.
            - meta: parsing metadata (filename, kind)
            """
            # JSON array support
            if request.is_json:
                payload = request.get_json(silent=True)
                if isinstance(payload, dict) and isinstance(payload.get("rows"), list):
                    payload_rows = payload.get("rows")
                    meta = {"kind": "json", "filename": None, "list_name": payload.get("list_name")}
                else:
                    payload_rows = payload
                    meta = {"kind": "json", "filename": None, "list_name": None}

                if not isinstance(payload_rows, list):
                    raise ValueError("JSON ×—×™×™×‘ ×œ×”×™×•×ª ××¢×¨×š ×©×œ ×©×•×¨×•×ª (array) ××• ××•×‘×™×™×§×˜ ×¢× rows")

                # If it's a list of dicts, convert to a table with headers
                if payload_rows and isinstance(payload_rows[0], dict):
                    keys: list[str] = []
                    seen = set()
                    for item in payload_rows[:1000]:
                        if not isinstance(item, dict):
                            continue
                        for k in item.keys():
                            if k not in seen:
                                seen.add(k)
                                keys.append(str(k))
                    header = keys
                    out: list[list[str]] = [header]
                    for item in payload_rows:
                        if not isinstance(item, dict):
                            continue
                        out.append([str(item.get(k, "")).strip() if item.get(k) is not None else "" for k in header])
                    return out, meta

                # If it's list-of-lists, assume it's already a table
                out_rows: list[list[str]] = []
                for item in payload_rows:
                    if isinstance(item, list):
                        out_rows.append([str(v).strip() if v is not None else "" for v in item])
                    else:
                        # scalar -> single-column
                        out_rows.append([str(item).strip()])
                return out_rows, meta

            # Multipart file upload
            if 'file' not in request.files:
                raise ValueError("×œ× × ×‘×—×¨ ×§×•×‘×¥")

            file = request.files['file']
            if not file or file.filename == '':
                raise ValueError("×œ× × ×‘×—×¨ ×§×•×‘×¥")

            filename = file.filename or ""
            lower = filename.lower()
            raw = file.read()

            if lower.endswith(".csv"):
                content = _decode_bytes(raw)
                reader = csv.reader(io.StringIO(content))
                rows = [[(c or "").strip() for c in r] for r in reader]
                return rows, {"kind": "csv", "filename": filename, "list_name": request.form.get('list_name')}

            if lower.endswith(".xlsx"):
                try:
                    import openpyxl  # type: ignore
                except Exception:
                    raise ValueError("×§×•×‘×¥ Excel (.xlsx) × ×ª××š ×¨×§ ×× openpyxl ××•×ª×§×Ÿ ×‘×©×¨×ª")

                wb = openpyxl.load_workbook(io.BytesIO(raw), read_only=True, data_only=True)
                ws = wb.active
                rows = []
                for r in ws.iter_rows(values_only=True):
                    rows.append([str(v).strip() if v is not None else "" for v in r])
                return rows, {"kind": "xlsx", "filename": filename, "list_name": request.form.get('list_name')}

            raise ValueError("×™×© ×œ×”×¢×œ×•×ª ×§×•×‘×¥ CSV ××• Excel (.xlsx) ××• ×œ×©×œ×•×— JSON")

        table_rows, parse_meta = _parse_rows_from_request()
        # Remove fully empty rows
        table_rows = [r for r in table_rows if any((c or "").strip() for c in r)]
        if not table_rows:
            return jsonify({"error": "×”×§×•×‘×¥ ×¨×™×§ ××• ×œ× ××›×™×œ ×©×•×¨×•×ª"}), 400

        # Decide if first row is header
        first_row = table_rows[0]
        first_has_digits = any(any(ch.isdigit() for ch in (c or "")) for c in first_row)
        has_header = False
        if not first_has_digits:
            # if it looks like header-ish text in most cells, treat as header
            headerish = sum(1 for c in first_row if _looks_like_header_cell(c))
            has_header = headerish >= max(1, int(len(first_row) * 0.5))

        headers = first_row if has_header else [f"col_{i+1}" for i in range(max(len(r) for r in table_rows))]
        data_rows = table_rows[1:] if has_header else table_rows

        # Normalize row lengths
        col_count = max(len(headers), *(len(r) for r in data_rows)) if data_rows else len(headers)
        headers = (headers + [""] * col_count)[:col_count]
        padded_rows: list[list[str]] = []
        for r in data_rows:
            rr = (r + [""] * col_count)[:col_count]
            padded_rows.append(rr)

        # Mapping: by header first, then content, then fallback
        phone_idx = _find_header_index(headers, "phone") if has_header else None
        name_idx = _find_header_index(headers, "name") if has_header else None
        city_idx = _find_header_index(headers, "city") if has_header else None
        notes_idx = _find_header_index(headers, "notes") if has_header else None

        mapping_method = "header" if phone_idx is not None else "content"
        if phone_idx is None:
            phone_idx = _guess_phone_col(padded_rows)
        if name_idx is None:
            name_idx = _guess_name_col(padded_rows, phone_idx)

        # Fallback for 2 columns: assume name+phone but still validate phone-like
        if phone_idx is None and col_count == 2:
            # pick the more phone-like one
            phone_idx = _guess_phone_col(padded_rows)
            name_idx = 1 - phone_idx if phone_idx is not None else 0
            mapping_method = "fallback"

        rows_to_import: list[dict[str, Any]] = []
        errors: list[str] = []
        errors_sample: list[str] = []

        def _add_error(msg: str) -> None:
            errors.append(msg)
            if len(errors_sample) < 20:
                errors_sample.append(msg)

        # Row numbers: +1 for header line, +1 to make 1-based
        base_row_number = 2 if has_header else 1
        for idx, row in enumerate(padded_rows):
            row_num = base_row_number + idx

            raw_phone = row[phone_idx] if phone_idx is not None and phone_idx < len(row) else ""
            normalized_phone = _normalize_phone_forgiving(raw_phone)
            if not normalized_phone:
                if str(raw_phone).strip():
                    _add_error(f"×©×•×¨×” {row_num}: ×˜×œ×¤×•×Ÿ ×œ× ×ª×§×™×Ÿ - {raw_phone}")
                else:
                    _add_error(f"×©×•×¨×” {row_num}: ×—×¡×¨ ×˜×œ×¤×•×Ÿ")
                continue

            raw_name = row[name_idx].strip() if name_idx is not None and name_idx < len(row) else ""
            name = raw_name if raw_name else _auto_name_from_phone(normalized_phone)
            city = row[city_idx].strip() if city_idx is not None and city_idx < len(row) else None
            notes = row[notes_idx].strip() if notes_idx is not None and notes_idx < len(row) else None

            rows_to_import.append({
                "name": name,
                "phone": normalized_phone,
                "city": city,
                "notes": notes,
            })
        
        # Check 5000 limit
        if existing_count + len(rows_to_import) > MAX_IMPORTED_LEADS_PER_BUSINESS:
            available = MAX_IMPORTED_LEADS_PER_BUSINESS - existing_count
            return jsonify({
                "error": f"×œ× × ×™×ª×Ÿ ×œ×™×™×‘× ×™×•×ª×¨ ×-{MAX_IMPORTED_LEADS_PER_BUSINESS} ×œ×™×“×™× ×‘×¨×©×™××ª ×”×©×™×—×•×ª ×”×™×•×¦××•×ª. ×™×© ×œ×š ××§×•× ×œ-{available} ×œ×™×“×™× × ×•×¡×¤×™×."
            }), 400
        
        if len(rows_to_import) == 0:
            # Keep 400 only when we truly couldn't extract a single valid phone
            return jsonify({
                "error": "×œ× × ××¦××• ×˜×œ×¤×•× ×™× ×ª×§×™× ×™× ×œ×™×™×‘×•×",
                "errors_sample": errors_sample,
                "errors": errors_sample,  # backwards-compat for existing UI
            }), 400
        
        # Create the import list
        list_name = (parse_meta or {}).get("list_name") or f"×™×™×‘×•× {datetime.now().strftime('%d/%m/%Y %H:%M')}"
        
        outbound_list = OutboundLeadList()
        outbound_list.tenant_id = tenant_id
        outbound_list.name = list_name
        outbound_list.file_name = (parse_meta or {}).get("filename") or "import"
        outbound_list.total_leads = len(rows_to_import)
        db.session.add(outbound_list)
        db.session.flush()  # Get the list ID
        
        # Import leads
        imported_count = 0
        for row_data in rows_to_import:
            # Split name into first/last
            name_parts = row_data['name'].split(' ', 1)
            first_name = name_parts[0]
            last_name = name_parts[1] if len(name_parts) > 1 else ''
            
            lead = Lead()
            lead.tenant_id = tenant_id
            lead.first_name = first_name
            lead.last_name = last_name
            lead.phone_e164 = row_data['phone']
            lead.notes = row_data['notes']
            lead.source = "imported_outbound"
            lead.outbound_list_id = outbound_list.id
            lead.status = "new"
            db.session.add(lead)
            imported_count += 1
        
        db.session.commit()
        
        log.info(f"âœ… Imported {imported_count} leads for business {tenant_id}, list_id={outbound_list.id}")
        
        return jsonify({
            "success": True,
            "list_id": outbound_list.id,
            "list_name": list_name,
            "imported_count": imported_count,
            "skipped_count": len(errors),
            "errors_sample": errors_sample,
            "errors": errors_sample,  # keep existing UI contract
            "mapping": {
                "phone": headers[phone_idx] if phone_idx is not None and phone_idx < len(headers) else None,
                "name": headers[name_idx] if name_idx is not None and name_idx < len(headers) else None,
                "method": mapping_method,
                "has_header": has_header,
                "kind": (parse_meta or {}).get("kind"),
            }
        })
        
    except Exception as e:
        log.error(f"Error importing leads: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        # Some parsing errors should be user-facing 400
        msg = str(e) or "×©×’×™××” ×‘×™×™×‘×•× ×”×œ×™×“×™×"
        if any(s in msg for s in ["×œ× × ×‘×—×¨ ×§×•×‘×¥", "CSV", "Excel", "JSON", "×¨×™×§"]):
            return jsonify({"error": msg}), 400
        return jsonify({"error": f"×©×’×™××” ×‘×™×™×‘×•× ×”×œ×™×“×™×: {msg}"}), 500


@outbound_bp.route("/api/outbound/import-leads", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_imported_leads():
    """
    BUILD 182: Get imported leads for outbound calls
    
    Query params:
    - page: Page number (default 1)
    - page_size: Items per page (default 50, max 100)
    - list_id: Optional filter by list ID
    - search: Optional search query
    - statuses[]: Optional multi-status filter (e.g., ?statuses[]=new&statuses[]=contacted)
    
    Returns:
    {
        "total": 1234,
        "limit": 5000,
        "current_count": 1234,
        "page": 1,
        "page_size": 50,
        "items": [...]
    }
    """
    from flask import session
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"items": [], "total": 0, "limit": MAX_IMPORTED_LEADS_PER_BUSINESS, "message": "×‘×—×¨ ×¢×¡×§ ×œ×¦×¤×™×™×”"})
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        page = max(1, int(request.args.get('page', 1)))
        # ğŸ”¥ FIX: Increase max page size to 10,000 for project creation
        page_size = min(10000, max(1, int(request.args.get('page_size', 50))))
        list_id = request.args.get('list_id', type=int)
        search = request.args.get('search', '').strip()
        statuses_filter = request.args.getlist('statuses[]')  # âœ… Multi-status filter
        
        # âœ… Validate status filter values against allowed statuses
        # Sanitize: only allow alphanumeric, underscore, dash (prevent injection)
        if statuses_filter:
            statuses_filter = [
                s for s in statuses_filter 
                if s and STATUS_FILTER_PATTERN.match(s) and len(s) <= 64
            ]
        
        # Build query
        query = Lead.query.filter_by(
            tenant_id=tenant_id,
            source="imported_outbound"
        )
        
        if list_id:
            query = query.filter_by(outbound_list_id=list_id)
        
        # âœ… Status filter: Support multi-status filtering with case-insensitive matching
        if statuses_filter:
            query = query.filter(func.lower(Lead.status).in_([s.lower() for s in statuses_filter]))
        
        if search:
            search_term = f"%{search}%"
            query = query.filter(
                db.or_(
                    Lead.first_name.ilike(search_term),
                    Lead.last_name.ilike(search_term),
                    Lead.phone_e164.ilike(search_term)
                )
            )
        
        # Get total count
        total = query.count()
        
        # Paginate
        leads = query.order_by(Lead.created_at.desc()).offset((page - 1) * page_size).limit(page_size).all()
        
        items = []
        for lead in leads:
            items.append({
                "id": lead.id,
                "name": lead.full_name,
                "first_name": lead.first_name,
                "last_name": lead.last_name,
                "phone": lead.phone_e164,
                "status": lead.status,
                "notes": lead.notes,
                "list_id": lead.outbound_list_id,
                "created_at": lead.created_at.isoformat() if lead.created_at else None
            })
        
        return jsonify({
            "total": total,
            "limit": MAX_IMPORTED_LEADS_PER_BUSINESS,
            "current_count": total,
            "page": page,
            "page_size": page_size,
            "items": items
        })
        
    except Exception as e:
        log.error(f"Error fetching imported leads: {e}")
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª ×”×œ×™×“×™×"}), 500


@outbound_bp.route("/api/outbound/import-leads/<int:lead_id>", methods=["DELETE"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def delete_imported_lead(lead_id: int):
    """
    BUILD 182: Delete a single imported lead
    
    Only allows deletion of leads with source="imported_outbound"
    Regular CRM leads cannot be deleted from this endpoint.
    """
    from flask import session
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ××—×™×§×ª ×œ×™×“"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        lead = Lead.query.filter_by(
            id=lead_id,
            tenant_id=tenant_id
        ).first()
        
        if not lead:
            return jsonify({"error": "×œ×™×“ ×œ× × ××¦×"}), 404
        
        # Only allow deletion of imported leads
        if lead.source != "imported_outbound":
            return jsonify({"error": "× ×™×ª×Ÿ ×œ××—×•×§ ×¨×§ ×œ×™×“×™× ××™×•×‘××™× ××¨×©×™××ª ×©×™×—×•×ª ×™×•×¦××•×ª"}), 403
        
        # Delete contact identities (prevents NOT NULL constraint violation)
        ContactIdentity.query.filter_by(lead_id=lead_id).delete()
        
        db.session.delete(lead)
        db.session.commit()
        
        log.info(f"ğŸ—‘ï¸ Deleted imported lead {lead_id} for business {tenant_id}")
        
        return jsonify({"success": True, "message": "×”×œ×™×“ × ××—×§ ×‘×”×¦×œ×—×”"})
        
    except Exception as e:
        log.error(f"Error deleting imported lead: {e}")
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘××—×™×§×ª ×”×œ×™×“"}), 500


@outbound_bp.route("/api/outbound/import-leads/bulk-delete", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def bulk_delete_imported_leads():
    """
    BUILD 182: Bulk delete imported leads - uses RQ worker for batch processing
    
    Request body:
    {
        "lead_ids": [1, 2, 3] OR
        "delete_all": true (delete all imported leads)
    }
    """
    from flask import session
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ××—×™×§×”"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "× ×ª×•× ×™× ×—×¡×¨×™×"}), 400
    
    delete_all = data.get('delete_all', False)
    lead_ids = data.get('lead_ids', [])
    
    if not delete_all and not lead_ids:
        return jsonify({"error": "×œ× × ×‘×—×¨×• ×œ×™×“×™× ×œ××—×™×§×”"}), 400
    
    user = session.get('user', {})
    
    # ğŸ”¥ USE BULK GATE: Check if enqueue is allowed
    try:
        import redis
        REDIS_URL = os.getenv('REDIS_URL')
        redis_conn = redis.from_url(REDIS_URL) if REDIS_URL else None
        
        if redis_conn:
            from server.services.bulk_gate import get_bulk_gate
            bulk_gate = get_bulk_gate(redis_conn)
            
            if bulk_gate:
                # Check if enqueue is allowed
                allowed, reason = bulk_gate.can_enqueue(
                    business_id=tenant_id,
                    operation_type='delete_imported_leads',
                    user_id=user.get('id')
                )
                
                if not allowed:
                    return jsonify({"error": reason}), 429
    except Exception as e:
        log.warning(f"BulkGate check failed (proceeding anyway): {e}")
    
    try:
        # Count total leads to delete
        if delete_all:
            total = Lead.query.filter_by(
                tenant_id=tenant_id,
                source="imported_outbound"
            ).count()
        else:
            total = Lead.query.filter(
                Lead.id.in_(lead_ids),
                Lead.tenant_id == tenant_id,
                Lead.source == "imported_outbound"
            ).count()
        
        if total == 0:
            return jsonify({"error": "×œ× × ××¦××• ×œ×™×“×™× ×œ××—×™×§×”"}), 404
        
        # Create BackgroundJob record
        from server.models_sql import BackgroundJob
        from rq import Queue
        import redis
        
        # Check for existing active job and handle duplicates
        can_proceed, error_response, status_code = check_and_handle_duplicate_background_job(
            job_type='delete_imported_leads',
            business_id=tenant_id,
            error_message="××—×™×§×” ×”××•× ×™×ª ×¤×¢×™×œ×” ×›×‘×¨ ×§×™×™××ª. ×× × ×”××ª×Ÿ ×œ×¡×™×•× ×”××—×™×§×” ×”× ×•×›×—×™×ª."
        )
        
        if not can_proceed:
            return jsonify(error_response), status_code
        
        bg_job = BackgroundJob()
        bg_job.business_id = tenant_id
        bg_job.requested_by_user_id = user.get('id')
        bg_job.job_type = 'delete_imported_leads'
        bg_job.status = 'queued'
        bg_job.total = total
        bg_job.processed = 0
        bg_job.succeeded = 0
        bg_job.failed_count = 0
        bg_job.cursor = json.dumps({
            'delete_all': delete_all,
            'lead_ids': lead_ids,
            'last_id': 0
        })
        db.session.add(bg_job)
        db.session.commit()
        
        # Enqueue to RQ maintenance queue
        REDIS_URL = os.getenv('REDIS_URL')
        if REDIS_URL:
            redis_conn = redis.from_url(REDIS_URL)
            queue = Queue('maintenance', connection=redis_conn)
            
            # Acquire lock and record enqueue BEFORE actually enqueuing
            if redis_conn:
                try:
                    from server.services.bulk_gate import get_bulk_gate
                    bulk_gate = get_bulk_gate(redis_conn)
                    
                    if bulk_gate:
                        # Acquire lock for this operation
                        lock_acquired = bulk_gate.acquire_lock(
                            business_id=tenant_id,
                            operation_type='delete_imported_leads',
                            job_id=bg_job.id
                        )
                        
                        # Record the enqueue
                        bulk_gate.record_enqueue(
                            business_id=tenant_id,
                            operation_type='delete_imported_leads'
                        )
                except Exception as e:
                    log.warning(f"BulkGate lock/record failed (proceeding anyway): {e}")
            
            # Import and enqueue the job function
            from server.jobs.delete_imported_leads_job import delete_imported_leads_batch_job
            rq_job = queue.enqueue(
                delete_imported_leads_batch_job,
                bg_job.id,
                job_timeout='30m',
                job_id=f"delete_imported_leads_{bg_job.id}"
            )
            
            log.info(f"ğŸš€ Enqueued RQ job for delete imported leads, job_id={bg_job.id}, rq_job_id={rq_job.id}")
        else:
            log.warning(f"âš ï¸ REDIS_URL not set, cannot enqueue delete job")
            bg_job.status = 'failed'
            bg_job.last_error = 'Redis not configured'
            db.session.commit()
            return jsonify({"error": "Job queue not available"}), 503
        
        return jsonify({
            "success": True,
            "message": f"× ×•×¦×¨×” ××©×™××ª ××—×™×§×” ×¢×‘×•×¨ {total} ×œ×™×“×™×",
            "job_id": bg_job.id,
            "total_leads": total
        }), 202  # 202 Accepted - processing in background
        
    except Exception as e:
        log.error(f"Error creating bulk delete job: {e}")
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘×™×¦×™×¨×ª ××©×™××ª ××—×™×§×”"}), 500


@outbound_bp.route("/api/outbound/import-lists", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_import_lists():
    """
    BUILD 182: Get all import lists for the business
    """
    from flask import session
    from server.models_sql import OutboundLeadList
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"lists": []})
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        lists = OutboundLeadList.query.filter_by(
            tenant_id=tenant_id
        ).order_by(OutboundLeadList.created_at.desc()).all()
        
        result = []
        for lst in lists:
            # Count current leads in list
            lead_count = Lead.query.filter_by(
                tenant_id=tenant_id,
                outbound_list_id=lst.id
            ).count()
            
            result.append({
                "id": lst.id,
                "name": lst.name,
                "file_name": lst.file_name,
                "total_leads": lst.total_leads,
                "current_leads": lead_count,
                "created_at": lst.created_at.isoformat() if lst.created_at else None
            })
        
        return jsonify({"lists": result})
        
    except Exception as e:
        log.error(f"Error fetching import lists: {e}")
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª ×¨×©×™××•×ª ×”×™×™×‘×•×"}), 500


@outbound_bp.route("/api/outbound/bulk-enqueue", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def bulk_enqueue_outbound_calls():
    """
    Bulk enqueue outbound calls with concurrency control
    
    Request body:
    {
        "lead_ids": [1, 2, 3, ...],
        "concurrency": 3,
        "outbound_list_id": 12 (optional)
    }
    
    Returns:
    {
        "run_id": "123",
        "queued": 500
    }
    """
    from flask import session
    from server.models_sql import OutboundCallRun, OutboundCallJob
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ×”×¤×¢×œ×ª ×©×™×—×•×ª"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "× ×ª×•× ×™× ×—×¡×¨×™×"}), 400
    
    lead_ids = data.get("lead_ids", [])
    concurrency = data.get("concurrency", 3)
    outbound_list_id = data.get("outbound_list_id")
    project_id = data.get("project_id")  # Optional project_id for tracking
    
    if not lead_ids or not isinstance(lead_ids, list):
        return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×œ×¤×—×•×ª ×œ×™×“ ××—×“"}), 400
    
    # Validate concurrency
    if concurrency < 1 or concurrency > 10:
        return jsonify({"error": "××¡×¤×¨ ×©×™×—×•×ª ×‘××§×‘×™×œ ×—×™×™×‘ ×œ×”×™×•×ª ×‘×™×Ÿ 1 ×œ-10"}), 400
    
    # Validate project_id if provided
    if project_id and not _validate_project_access(project_id, tenant_id):
        return jsonify({"error": "×¤×¨×•×™×§×˜ ×œ× × ××¦×"}), 404
    
    user = session.get('user', {})
    
    # ğŸ”¥ USE BULK GATE: Check if enqueue is allowed
    try:
        import redis
        REDIS_URL = os.getenv('REDIS_URL')
        redis_conn = redis.from_url(REDIS_URL) if REDIS_URL else None
        
        if redis_conn:
            from server.services.bulk_gate import get_bulk_gate
            bulk_gate = get_bulk_gate(redis_conn)
            
            if bulk_gate:
                # Check if enqueue is allowed
                allowed, reason = bulk_gate.can_enqueue(
                    business_id=tenant_id,
                    operation_type='enqueue_outbound_calls',
                    user_id=user.get('id')
                )
                
                if not allowed:
                    return jsonify({"error": reason}), 429
    except Exception as e:
        log.warning(f"BulkGate check failed (proceeding anyway): {e}")
    
    try:
        # Verify all leads belong to this tenant
        leads = Lead.query.filter(
            Lead.id.in_(lead_ids),
            Lead.tenant_id == tenant_id
        ).all()
        
        if len(leads) != len(lead_ids):
            return jsonify({"error": "×œ× × ××¦××• ×›×œ ×”×œ×™×“×™× ×©× ×‘×—×¨×•"}), 404
        
        # ğŸ”’ SECURITY: Create run with proper audit trail
        run = OutboundCallRun()
        run.business_id = tenant_id
        run.created_by_user_id = user.get('id')  # Audit trail
        run.outbound_list_id = outbound_list_id
        run.concurrency = concurrency
        run.total_leads = len(lead_ids)
        run.queued_count = len(lead_ids)
        run.cursor_position = 0  # Start at beginning
        run.status = "pending"  # Start as pending, worker will update to running when processing starts
        db.session.add(run)
        db.session.flush()
        
        # ğŸ”’ SECURITY: Create jobs with business_id for isolation
        for lead_id in lead_ids:
            job = OutboundCallJob()
            job.run_id = run.id
            job.lead_id = lead_id
            job.business_id = tenant_id  # Business isolation
            job.status = "queued"
            # ğŸ”¥ NAME SSOT: Store lead name for NAME_ANCHOR system
            lead_obj = next((l for l in leads if l.id == lead_id), None)
            if lead_obj:
                job.lead_name = _get_lead_display_name(lead_obj)
            # ğŸ¯ PROJECT TRACKING: Associate job with project if provided
            if project_id:
                job.project_id = project_id
            db.session.add(job)
        
        db.session.commit()
        
        log.info(f"âœ… Created bulk call run {run.id} with {len(lead_ids)} leads, concurrency={concurrency}, project_id={project_id}")
        
        # Create BackgroundJob record and enqueue to RQ
        from server.models_sql import BackgroundJob
        from rq import Queue
        import redis
        
        # Check for existing active job and handle duplicates
        can_proceed, error_response, status_code = check_and_handle_duplicate_background_job(
            job_type='enqueue_outbound_calls',
            business_id=tenant_id,
            error_message="×ª×•×¨ ×©×™×—×•×ª ×¤×¢×™×œ ×›×‘×¨ ×§×™×™×. ×× × ×”××ª×Ÿ ×œ×¡×™×•× ×”×ª×•×¨ ×”× ×•×›×—×™ ××• ×¦×•×¨ ×§×©×¨ ×¢× ×”×ª××™×›×”."
        )
        
        if not can_proceed:
            return jsonify(error_response), status_code
        
        bg_job = BackgroundJob()
        bg_job.business_id = tenant_id
        bg_job.requested_by_user_id = user.get('id')
        bg_job.job_type = 'enqueue_outbound_calls'
        bg_job.status = 'queued'
        bg_job.total = len(lead_ids)
        bg_job.processed = 0
        bg_job.succeeded = 0
        bg_job.failed_count = 0
        bg_job.cursor = json.dumps({
            'run_id': run.id,
            'last_id': 0
        })
        db.session.add(bg_job)
        db.session.commit()
        
        # Enqueue to RQ default queue (for call processing)
        REDIS_URL = os.getenv('REDIS_URL')
        if REDIS_URL:
            redis_conn = redis.from_url(REDIS_URL)
            queue = Queue('default', connection=redis_conn)
            
            # Acquire lock and record enqueue BEFORE actually enqueuing
            if redis_conn:
                try:
                    from server.services.bulk_gate import get_bulk_gate
                    bulk_gate = get_bulk_gate(redis_conn)
                    
                    if bulk_gate:
                        # Acquire lock for this operation
                        lock_acquired = bulk_gate.acquire_lock(
                            business_id=tenant_id,
                            operation_type='enqueue_outbound_calls',
                            job_id=bg_job.id
                        )
                        
                        # Record the enqueue
                        bulk_gate.record_enqueue(
                            business_id=tenant_id,
                            operation_type='enqueue_outbound_calls'
                        )
                except Exception as e:
                    log.warning(f"BulkGate lock/record failed (proceeding anyway): {e}")
            
            # Import and enqueue the job function
            from server.jobs.enqueue_outbound_calls_job import enqueue_outbound_calls_batch_job
            rq_job = queue.enqueue(
                enqueue_outbound_calls_batch_job,
                bg_job.id,
                job_timeout='60m',  # Longer timeout for calls
                job_id=f"outbound_calls_{bg_job.id}"
            )
            
            log.info(f"ğŸš€ Enqueued RQ job for outbound calls, job_id={bg_job.id}, rq_job_id={rq_job.id}")
        else:
            log.warning(f"âš ï¸ REDIS_URL not set, cannot enqueue outbound calls job")
            bg_job.status = 'failed'
            bg_job.last_error = 'Redis not configured'
            run.status = 'failed'
            db.session.commit()
            return jsonify({"error": "Job queue not available"}), 503
        
        return jsonify({
            "success": True,
            "run_id": run.id,
            "job_id": bg_job.id,
            "queued": len(lead_ids)
        }), 202  # 202 Accepted - processing in background
        
    except Exception as e:
        log.error(f"Error creating bulk call run: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": f"×©×’×™××” ×‘×™×¦×™×¨×ª ×”×¨×¦×”: {str(e)}"}), 500


@outbound_bp.route("/api/outbound/runs/<int:run_id>", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_run_status(run_id: int):
    """
    Get status of bulk call run
    
    ğŸ”’ SECURITY: Enforces business isolation - users can only see runs for their business
    
    Returns:
    {
        "run_id": 123,
        "status": "running",
        "queued": 450,
        "in_progress": 3,
        "completed": 47,
        "failed": 0,
        "cursor_position": 47,
        "last_error": null
    }
    """
    from flask import session
    from server.models_sql import OutboundCallRun
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can view any run, get tenant from run
            run = OutboundCallRun.query.get(run_id)
            if not run:
                return jsonify({"error": "×”×¨×¦×” ×œ× × ××¦××”"}), 404
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    else:
        # ğŸ”’ SECURITY: Regular user - MUST verify run belongs to their tenant
        run = OutboundCallRun.query.filter_by(
            id=run_id,
            business_id=tenant_id
        ).first()
        
        if not run:
            # ğŸ”’ SECURITY: Log potential cross-business access attempt
            log.warning(f"[SECURITY] User from business {tenant_id} attempted to access run {run_id} which doesn't exist or belongs to different business")
            return jsonify({"error": "×”×¨×¦×” ×œ× × ××¦××”"}), 404
        
        # ğŸ”’ SECURITY: Double-check business_id matches (defensive programming)
        if run.business_id != tenant_id:
            log.error(f"[SECURITY] Business ID mismatch in get_run_status: run.business_id={run.business_id} != tenant_id={tenant_id}")
            return jsonify({"error": "×”×¨×¦×” ×œ× × ××¦××”"}), 404
    
    return jsonify({
        "run_id": run.id,
        "status": run.status,
        "queued": run.queued_count,
        "in_progress": run.in_progress_count,
        "completed": run.completed_count,
        "failed": run.failed_count,
        "cursor_position": run.cursor_position or 0,
        "last_error": run.last_error,
        "total_leads": run.total_leads,
        "concurrency": run.concurrency,
        "created_at": run.created_at.isoformat() if run.created_at else None,
        "started_at": run.started_at.isoformat() if run.started_at else None,
        "ended_at": run.ended_at.isoformat() if run.ended_at else None,
        "completed_at": run.completed_at.isoformat() if run.completed_at else None,
        "can_cancel": run.status in ('pending', 'running') and not run.cancel_requested,
        "cancel_requested": run.cancel_requested
    })


@outbound_bp.route("/api/outbound/stop-queue", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def stop_queue():
    """
    Stop an active bulk call queue/run
    
    ğŸ”’ SECURITY: Enforces business isolation - users can only stop runs for their business
    
    Request body:
    {
        "run_id": 123
    }
    """
    from flask import session
    from server.models_sql import OutboundCallRun
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can stop any run
            pass
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "× ×ª×•× ×™× ×—×¡×¨×™×"}), 400
    
    run_id = data.get('run_id')
    if not run_id:
        return jsonify({"error": "×—×¡×¨ run_id"}), 400
    
    try:
        # ğŸ”’ SECURITY: Get run and verify it belongs to tenant if not system admin
        if tenant_id:
            run = OutboundCallRun.query.filter_by(
                id=run_id,
                business_id=tenant_id
            ).first()
            
            if not run:
                # ğŸ”’ SECURITY: Log potential cross-business access attempt
                log.warning(f"[SECURITY] User from business {tenant_id} attempted to stop run {run_id} which doesn't exist or belongs to different business")
                return jsonify({"error": "×”×¨×¦×” ×œ× × ××¦××”"}), 404
            
            # ğŸ”’ SECURITY: Double-check business_id matches (defensive programming)
            if run.business_id != tenant_id:
                log.error(f"[SECURITY] Business ID mismatch: run.business_id={run.business_id} != tenant_id={tenant_id}")
                return jsonify({"error": "×”×¨×¦×” ×œ× × ××¦××”"}), 404
        else:
            run = OutboundCallRun.query.get(run_id)
            if not run:
                return jsonify({"error": "×”×¨×¦×” ×œ× × ××¦××”"}), 404
        
        # Check if already stopped/completed
        if run.status in ('stopped', 'completed', 'cancelled', 'failed'):
            return jsonify({
                "success": True,
                "message": f"×”×ª×•×¨ ×›×‘×¨ ×”×¡×ª×™×™× (×¡×˜×˜×•×¡: {run.status})",
                "cancelled_jobs": 0
            })
        
        # ğŸ”’ STATE MACHINE: Mark as stopped and set ended_at + cancel_requested
        run.status = "stopped"
        run.cancel_requested = True  # Also set cancel flag so worker detects stop
        run.ended_at = datetime.utcnow()
        run.completed_at = datetime.utcnow()  # Legacy field
        
        # ğŸ”’ SECURITY: Cancel all queued jobs - only for this business
        # Use raw SQL for performance with large queues
        cancelled_count = db.session.execute(text("""
            UPDATE outbound_call_jobs 
            SET status='cancelled',
                error_message='Queue stopped by user',
                completed_at=NOW()
            WHERE run_id=:run_id 
                AND business_id=:business_id
                AND status='queued'
        """), {"run_id": run_id, "business_id": run.business_id}).rowcount
        
        # ğŸ”’ STATE MACHINE: Update run counters to reflect reality
        # Move queued jobs to failed count (they won't run)
        run.failed_count += cancelled_count
        run.queued_count = 0
        
        # Note: Jobs in 'calling' or 'dialing' state will complete naturally
        # We don't forcefully terminate active calls
        
        db.session.commit()
        
        log.info(f"âœ… [business_id={run.business_id}] Stopped queue run {run_id}: cancelled {cancelled_count} queued jobs")
        
        return jsonify({
            "success": True,
            "message": f"×”×ª×•×¨ × ×¢×¦×¨ ({cancelled_count} ×©×™×—×•×ª ×‘×•×˜×œ×•, ×©×™×—×•×ª ×¤×¢×™×œ×•×ª ×™×¡×ª×™×™××•)",
            "cancelled_jobs": cancelled_count
        })
        
    except Exception as e:
        log.error(f"Error stopping queue {run_id}: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘×¢×¦×™×¨×ª ×”×ª×•×¨"}), 500


@outbound_bp.route("/api/outbound/bulk/active", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_active_bulk_run():
    """
    Get active bulk call run for the current business
    
    Returns:
    {
        "active": true/false,
        "run": {
            "run_id": 123,
            "status": "running",
            "queued": 450,
            "in_progress": 3,
            "completed": 47,
            "failed": 0,
            "total_leads": 500,
            "concurrency": 3
        }
    }
    or
    {
        "active": false
    }
    """
    from flask import session
    from server.models_sql import OutboundCallRun
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        # ğŸ”¥ FIX: Proper "active run" definition per user requirements
        # A run is active ONLY if:
        # 1. status = 'running' (not queued/stopped/completed)
        # 2. created within last 30 minutes (prevents ancient stuck runs)
        # 3. Has actual activity (queued > 0 OR in_progress > 0)
        
        cutoff_time = datetime.utcnow() - timedelta(minutes=30)
        
        active_run = OutboundCallRun.query.filter(
            OutboundCallRun.business_id == tenant_id,
            OutboundCallRun.status == 'running',
            OutboundCallRun.created_at >= cutoff_time,
            db.or_(
                OutboundCallRun.queued_count > 0,
                OutboundCallRun.in_progress_count > 0
            )
        ).order_by(OutboundCallRun.created_at.desc()).first()
        
        if not active_run:
            return jsonify({"active": False})
        
        return jsonify({
            "active": True,
            "run": {
                "run_id": active_run.id,
                "status": active_run.status,
                "queued": active_run.queued_count,
                "in_progress": active_run.in_progress_count,
                "completed": active_run.completed_count,
                "failed": active_run.failed_count,
                "total_leads": active_run.total_leads,
                "concurrency": active_run.concurrency,
                "created_at": active_run.created_at.isoformat() if active_run.created_at else None
            }
        })
        
    except Exception as e:
        log.error(f"Error getting active bulk run: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "×©×’×™××” ×‘×§×‘×œ×ª ××™×“×¢"}), 500


@outbound_bp.route("/api/outbound/recent-calls", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_recent_calls():
    """
    Get recent outbound calls, sorted by most recent first
    
    Query params:
    - page: Page number (default 1)
    - page_size: Items per page (default 50, max 100)
    - status: Optional status filter
    - search: Optional search query (phone or lead name)
    - run_id: Optional filter by specific run
    
    Returns:
    {
        "total": 123,
        "page": 1,
        "page_size": 50,
        "items": [
            {
                "call_sid": "CA...",
                "to_number": "+972...",
                "lead_id": 123,
                "lead_name": "John Doe",
                "status": "completed",
                "started_at": "2024-01-01T12:00:00Z",
                "ended_at": "2024-01-01T12:05:00Z",
                "duration": 300,
                "recording_sid": "RE...",
                "transcript": "...",
                "summary": "..."
            }
        ]
    }
    """
    from flask import session
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({
                "items": [], 
                "total": 0, 
                "page": 1, 
                "page_size": 50,
                "message": "×‘×—×¨ ×¢×¡×§ ×œ×¦×¤×™×™×” ×‘×©×™×—×•×ª"
            })
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        page = max(1, int(request.args.get('page', 1)))
        # ğŸ”¥ FIX: Increase max page size to 10,000 for better UI experience
        page_size = min(10000, max(1, int(request.args.get('page_size', 50))))
        status_filter = request.args.get('status', '').strip()
        search = request.args.get('search', '').strip()
        run_id = request.args.get('run_id', type=int)
        
        # Build query for outbound calls
        query = CallLog.query.filter_by(
            business_id=tenant_id,
            direction="outbound"
        )
        
        # Filter by run_id if provided
        if run_id:
            from server.models_sql import OutboundCallJob
            # Get call_log_ids from jobs in this run
            job_call_ids = db.session.query(OutboundCallJob.call_log_id).filter_by(
                run_id=run_id
            ).filter(
                OutboundCallJob.call_log_id.isnot(None)
            ).all()
            call_log_ids = [cid[0] for cid in job_call_ids]
            if call_log_ids:
                query = query.filter(CallLog.id.in_(call_log_ids))
            else:
                # No calls for this run yet
                return jsonify({
                    "items": [],
                    "total": 0,
                    "page": page,
                    "page_size": page_size
                })
        
        # Status filter
        if status_filter and STATUS_FILTER_PATTERN.match(status_filter):
            query = query.filter(func.lower(CallLog.call_status) == status_filter.lower())
        
        # Search filter
        if search:
            # Join with Lead to search by name
            query = query.outerjoin(Lead, CallLog.lead_id == Lead.id)
            search_term = f"%{search}%"
            query = query.filter(
                db.or_(
                    CallLog.to_number.ilike(search_term),
                    Lead.first_name.ilike(search_term),
                    Lead.last_name.ilike(search_term)
                )
            )
        
        # Get total count
        total = query.count()
        
        # Sort by most recent first (created_at is when call record was created, which is when call started)
        query = query.order_by(CallLog.created_at.desc())
        
        # Paginate
        calls = query.offset((page - 1) * page_size).limit(page_size).all()
        
        # Format response
        items = []
        for call in calls:
            # Get lead info if available
            lead_name = None
            lead_status = None
            if call.lead_id:
                lead = Lead.query.get(call.lead_id)
                if lead:
                    lead_name = lead.full_name
                    lead_status = lead.status
            
            # âœ… Enrich with recording status
            recording_status = None
            recording_run_id = None
            recording_run = RecordingRun.query.filter_by(call_sid=call.call_sid).order_by(RecordingRun.created_at.desc()).first()
            
            if recording_run:
                recording_status = recording_run.status  # Use RecordingRun status
                recording_run_id = recording_run.id
            elif call.recording_url:
                recording_status = 'completed'  # Has URL means completed
            elif call.recording_sid:
                recording_status = 'processing'  # Has SID but no URL yet
            
            items.append({
                "call_sid": call.call_sid,
                "to_number": call.to_number,
                "lead_id": call.lead_id,
                "lead_name": lead_name,
                "lead_status": lead_status,
                "status": call.call_status,
                "started_at": call.created_at.isoformat() if call.created_at else None,
                "ended_at": call.updated_at.isoformat() if call.updated_at and call.updated_at != call.created_at else None,
                "duration": call.duration,
                "recording_sid": call.recording_sid,
                "recording_status": recording_status,
                "recording_run_id": recording_run_id,
                "hasRecording": bool(call.recording_url),
                "transcript": call.final_transcript or call.transcription,
                "summary": call.summary
            })
        
        return jsonify({
            "total": total,
            "page": page,
            "page_size": page_size,
            "items": items
        })
        
    except Exception as e:
        log.error(f"Error fetching recent calls: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª ×”×©×™×—×•×ª ×”××—×¨×•× ×•×ª"}), 500


@outbound_bp.route("/api/inbound/recent-calls", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_inbound')
def get_recent_inbound_calls():
    """
    Get recent inbound calls, sorted by most recent first
    
    Query params:
    - page: Page number (default 1)
    - page_size: Items per page (default 50, max 100)
    - status: Optional status filter
    - search: Optional search query (phone or lead name)
    
    Returns:
    {
        "total": 123,
        "page": 1,
        "page_size": 50,
        "items": [
            {
                "call_sid": "CA...",
                "to_number": "+972...",
                "lead_id": 123,
                "lead_name": "John Doe",
                "lead_status": "new",
                "status": "completed",
                "started_at": "2024-01-01T12:00:00Z",
                "ended_at": "2024-01-01T12:05:00Z",
                "duration": 300,
                "recording_sid": "RE...",
                "transcript": "...",
                "summary": "..."
            }
        ]
    }
    """
    from flask import session
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({
                "items": [], 
                "total": 0, 
                "page": 1, 
                "page_size": 50,
                "message": "×‘×—×¨ ×¢×¡×§ ×œ×¦×¤×™×™×” ×‘×©×™×—×•×ª"
            })
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    try:
        page = max(1, int(request.args.get('page', 1)))
        # ğŸ”¥ FIX: Increase max page size to 10,000 for better UI experience
        page_size = min(10000, max(1, int(request.args.get('page_size', 50))))
        status_filter = request.args.get('status', '').strip()
        search = request.args.get('search', '').strip()
        
        # Build query for inbound calls only
        query = CallLog.query.filter_by(
            business_id=tenant_id,
            direction="inbound"
        )
        
        # Status filter
        if status_filter and STATUS_FILTER_PATTERN.match(status_filter):
            query = query.filter(func.lower(CallLog.call_status) == status_filter.lower())
        
        # Search filter
        if search:
            # Join with Lead to search by name
            query = query.outerjoin(Lead, CallLog.lead_id == Lead.id)
            search_term = f"%{search}%"
            query = query.filter(
                db.or_(
                    CallLog.to_number.ilike(search_term),
                    Lead.first_name.ilike(search_term),
                    Lead.last_name.ilike(search_term)
                )
            )
        
        # Get total count
        total = query.count()
        
        # Sort by most recent first (created_at is when call record was created, which is when call started)
        query = query.order_by(CallLog.created_at.desc())
        
        # Paginate
        calls = query.offset((page - 1) * page_size).limit(page_size).all()
        
        # Format response
        items = []
        for call in calls:
            # Get lead info if available
            lead_name = None
            lead_status = None
            if call.lead_id:
                lead = Lead.query.get(call.lead_id)
                if lead:
                    lead_name = lead.full_name
                    lead_status = lead.status
            
            # âœ… Enrich with recording status
            recording_status = None
            recording_run_id = None
            recording_run = RecordingRun.query.filter_by(call_sid=call.call_sid).order_by(RecordingRun.created_at.desc()).first()
            
            if recording_run:
                recording_status = recording_run.status  # Use RecordingRun status
                recording_run_id = recording_run.id
            elif call.recording_url:
                recording_status = 'completed'  # Has URL means completed
            elif call.recording_sid:
                recording_status = 'processing'  # Has SID but no URL yet
            
            items.append({
                "call_sid": call.call_sid,
                "to_number": call.to_number,
                "lead_id": call.lead_id,
                "lead_name": lead_name,
                "lead_status": lead_status,
                "status": call.call_status,
                "started_at": call.created_at.isoformat() if call.created_at else None,
                "ended_at": call.updated_at.isoformat() if call.updated_at and call.updated_at != call.created_at else None,
                "duration": call.duration,
                "recording_sid": call.recording_sid,
                "recording_status": recording_status,
                "recording_run_id": recording_run_id,
                "hasRecording": bool(call.recording_url),
                "transcript": call.final_transcript or call.transcription,
                "summary": call.summary
            })
        
        return jsonify({
            "total": total,
            "page": page,
            "page_size": page_size,
            "items": items
        })
        
    except Exception as e:
        log.error(f"Error fetching recent inbound calls: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": "×©×’×™××” ×‘×˜×¢×™× ×ª ×”×©×™×—×•×ª ×”× ×›× ×¡×•×ª ×”××—×¨×•× ×•×ª"}), 500


def process_next_queued_job(job_id: int, run_id: int):
    """
    ğŸ”¥ NEW: Process a single job from the Redis queue
    
    Called by webhook when a call completes and Redis queue returns next job.
    This is the event-driven approach that works with Redis semaphore.
    
    Args:
        job_id: The job_id to process (returned by release_slot())
        run_id: The run_id this job belongs to
        
    Note: This function MUST release the Redis slot if it cannot process the job,
          since release_slot() already assigned this job_id a slot.
    """
    from server.app_factory import get_process_app
    from server.models_sql import OutboundCallRun, OutboundCallJob, Lead, Business, CallLog
    from datetime import datetime
    import uuid
    from sqlalchemy import text
    
    app = get_process_app()
    
    def release_and_process_next(business_id: int, job_id: int, run_id: int):
        """Helper to release slot and process next job via RQ worker (not thread)"""
        try:
            from server.services.outbound_semaphore import release_slot
            next_job_id = release_slot(business_id, job_id)
            if next_job_id:
                # ğŸ”¥ FIX: Use RQ worker instead of Thread to prevent dual background execution
                # This prevents duplicates, stuck runs, and UI issues after restart
                try:
                    import redis
                    from rq import Queue
                    REDIS_URL = os.getenv('REDIS_URL')
                    if REDIS_URL:
                        redis_conn = redis.from_url(REDIS_URL)
                        queue = Queue('default', connection=redis_conn)
                        
                        log.info(f"[ProcessNext] Released slot for job {job_id}, enqueuing next job {next_job_id} to RQ worker")
                        
                        # Enqueue to RQ worker instead of spawning thread
                        # Use function reference for consistency with other enqueue calls
                        queue.enqueue(
                            process_next_queued_job,
                            next_job_id,
                            run_id,
                            job_timeout='10m',  # Single call should complete quickly
                            job_id=f"process_next_{next_job_id}"
                        )
                    else:
                        # CRITICAL: REDIS_URL not set - cannot process next job
                        # This would cause the queue to stall
                        log.error(f"[ProcessNext] REDIS_URL not set, cannot enqueue next job {next_job_id}. Queue may stall!")
                        # Note: The slot is already released, so we can't easily retry
                        # This is a configuration error that should be fixed
                except Exception as e:
                    log.error(f"[ProcessNext] Failed to enqueue next job {next_job_id} to RQ: {e}")
            else:
                log.info(f"[ProcessNext] Released slot for job {job_id}, no more jobs in queue")
        except Exception as e:
            log.error(f"[ProcessNext] Error releasing slot: {e}")
    
    with app.app_context():
        try:
            # Get the job
            job = OutboundCallJob.query.get(job_id)
            if not job:
                log.warning(f"[ProcessNext] Job {job_id} not found - cannot determine business_id to release slot")
                # Cannot release slot without business_id - this is a rare edge case
                return
            
            business_id = job.business_id
            
            # Verify it's queued
            if job.status != "queued":
                log.warning(f"[ProcessNext] Job {job_id} is not queued (status={job.status}), releasing slot")
                release_and_process_next(business_id, job_id, run_id)
                return
            
            # Get the run
            run = OutboundCallRun.query.get(run_id)
            if not run or run.status != "running":
                log.info(f"[ProcessNext] Run {run_id} not running, releasing slot for job {job_id}")
                release_and_process_next(business_id, job_id, run_id)
                return
            
            log.info(f"[ProcessNext] Processing job {job_id} from Redis queue for run {run_id}")
            
            # Get business details
            business = Business.query.get(run.business_id)
            if not business or not business.phone_e164:
                log.error(f"[ProcessNext] Business {run.business_id} has no phone")
                job.status = "failed"
                job.error_message = "No phone number configured for business"
                job.completed_at = datetime.utcnow()
                run.failed_count += 1
                run.queued_count -= 1
                db.session.commit()
                # ğŸ”¥ CRITICAL: Release slot and process next
                release_and_process_next(business_id, job_id, run_id)
                return
            
            from_phone = business.phone_e164
            business_name = business.name or "×”×¢×¡×§"
            
            # Get public host for webhooks
            try:
                host = get_public_host()
            except RuntimeError as e:
                log.error(f"[ProcessNext] No public host: {e}")
                job.status = "failed"
                job.error_message = "No public host configured"
                job.completed_at = datetime.utcnow()
                run.failed_count += 1
                run.queued_count -= 1
                db.session.commit()
                # ğŸ”¥ CRITICAL: Release slot and process next
                release_and_process_next(business_id, job_id, run_id)
                return
            
            # Get lead
            lead = Lead.query.get(job.lead_id)
            if not lead or not lead.phone_e164:
                job.status = "failed"
                job.error_message = "No phone number for lead"
                job.completed_at = datetime.utcnow()
                run.failed_count += 1
                run.queued_count -= 1
                db.session.commit()
                # ğŸ”¥ CRITICAL: Release slot and process next
                release_and_process_next(business_id, job_id, run_id)
                return
            
            # ğŸ”’ ATOMIC LOCKING: Acquire lock before dialing
            lock_token = str(uuid.uuid4())
            
            # Atomic UPDATE: Only proceed if status='queued' AND twilio_call_sid IS NULL AND dial_lock_token IS NULL
            result = db.session.execute(text("""
                UPDATE outbound_call_jobs 
                SET status='dialing', 
                    dial_started_at=NOW(), 
                    dial_lock_token=:lock_token
                WHERE id=:job_id 
                    AND status='queued' 
                    AND twilio_call_sid IS NULL
                    AND dial_lock_token IS NULL
            """), {"job_id": job.id, "lock_token": lock_token})
            
            db.session.commit()
            
            # Check if we acquired the lock
            if result.rowcount == 0:
                # Someone else already started this call or it's no longer queued
                log.warning(f"[ProcessNext] Job {job.id} already being processed, releasing slot")
                # ğŸ”¥ CRITICAL: Release slot and process next since lock acquisition failed
                release_and_process_next(business_id, job_id, run_id)
                return
            
            # Successfully acquired lock, proceed with call
            run.in_progress_count += 1
            run.queued_count -= 1
            db.session.commit()
            
            # Normalize phone
            normalized_phone = normalize_israeli_phone(lead.phone_e164)
            
            # Create call log
            call_log = CallLog()
            call_log.business_id = run.business_id
            call_log.lead_id = lead.id
            call_log.from_number = from_phone
            call_log.to_number = normalized_phone
            call_log.direction = "outbound"
            call_log.status = "initiated"
            call_log.call_status = "initiated"
            # ğŸ”¥ NAME SSOT: Store lead name for NAME_ANCHOR system
            call_log.customer_name = _get_lead_display_name(lead)
            # ğŸ¯ PROJECT TRACKING: Associate call with project if job has project_id
            if job.project_id:
                call_log.project_id = job.project_id
            db.session.add(call_log)
            db.session.flush()
            
            job.call_log_id = call_log.id
            db.session.commit()
            
            # ğŸ”¥ SSOT: Use centralized outbound call service
            from server.services.twilio_outbound_service import create_outbound_call
            
            # ğŸ”¥ NAME SSOT: Extract lead name
            lead_name = _get_lead_display_name(lead)
            
            try:
                result = create_outbound_call(
                    to_phone=normalized_phone,
                    from_phone=from_phone,
                    business_id=run.business_id,
                    host=host,
                    lead_id=lead.id,
                    job_id=job.id,
                    business_name=business_name,
                    lead_name=lead_name
                )
                
                call_sid = result["call_sid"]
                is_duplicate = result.get("is_duplicate", False)
                
                if is_duplicate:
                    log.warning(f"[ProcessNext] [DEDUP] Duplicate call detected for job {job.id}, marking as failed")
                    job.status = "failed"
                    job.error_message = "Duplicate call detected"
                    job.completed_at = datetime.utcnow()
                    run.in_progress_count -= 1
                    run.failed_count += 1
                    db.session.commit()
                    # ğŸ”¥ CRITICAL: Release slot and process next when duplicate detected
                    release_and_process_next(business_id, job_id, run_id)
                    return
                
                # ğŸ”’ ATOMIC UPDATE: Update with Twilio call SID only if lock token matches
                update_result = db.session.execute(text("""
                    UPDATE outbound_call_jobs 
                    SET twilio_call_sid=:twilio_sid, 
                        call_sid=:twilio_sid,
                        status='calling',
                        started_at=NOW()
                    WHERE id=:job_id 
                        AND dial_lock_token=:lock_token
                """), {
                    "job_id": job.id, 
                    "twilio_sid": call_sid, 
                    "lock_token": lock_token
                })
                
                if update_result.rowcount == 0:
                    log.error(f"[ProcessNext] Lock token mismatch for job {job.id}, call may be duplicate")
                
                call_log.call_sid = call_sid
                db.session.commit()
                
                log.info(f"[ProcessNext] âœ… Started call for lead {lead.id}, job {job.id}, call_sid={call_sid}")
                
            except Exception as twilio_error:
                # ğŸ”’ DEDUPLICATION: Handle Twilio timeout/exception
                log.error(f"[ProcessNext] Twilio error for job {job.id}: {twilio_error}")
                
                # Check if call was actually created despite the error
                db.session.refresh(job)
                if not job.twilio_call_sid:
                    # Call was not created - reset to queued for retry and release slot
                    result = db.session.execute(text("""
                        UPDATE outbound_call_jobs 
                        SET status='queued',
                            dial_lock_token=NULL,
                            dial_started_at=NULL
                        WHERE id=:job_id 
                            AND twilio_call_sid IS NULL
                            AND dial_lock_token=:lock_token
                    """), {"job_id": job.id, "lock_token": lock_token})
                    
                    run.in_progress_count -= 1
                    run.queued_count += 1
                    db.session.commit()
                    
                    log.warning(f"[ProcessNext] Reset job {job.id} to queued for retry, releasing slot")
                    # ğŸ”¥ CRITICAL: Release slot using helper (RQ instead of thread)
                    release_and_process_next(business_id, job.id, run_id)
                # else: Call was created, webhook will handle slot release when it completes
        
        except Exception as e:
            log.error(f"[ProcessNext] Error processing job {job_id}: {e}")
            import traceback
            traceback.print_exc()
            
            # Try to mark job as failed if it hasn't started yet and release slot
            try:
                job = OutboundCallJob.query.get(job_id)
                if job:
                    business_id = job.business_id
                    # Only update if call never started (no dial_lock_token or no twilio_call_sid)
                    if not job.twilio_call_sid and job.status not in ["failed", "completed"]:
                        # Check if we acquired the atomic lock
                        if job.dial_lock_token:
                            # We acquired lock and incremented in_progress - must decrement
                            run = OutboundCallRun.query.get(run_id)
                            if run:
                                run.in_progress_count = max(0, run.in_progress_count - 1)
                                run.failed_count += 1
                        else:
                            # Never acquired lock, just mark as failed
                            run = OutboundCallRun.query.get(run_id)
                            if run:
                                run.failed_count += 1
                                run.queued_count = max(0, run.queued_count - 1)
                        
                        job.status = "failed"
                        job.error_message = str(e)[:200]
                        job.completed_at = datetime.utcnow()
                        db.session.commit()
                        
                        # ğŸ”¥ CRITICAL: Release the slot using helper (RQ instead of thread)
                        log.info(f"[ProcessNext] Job {job_id} failed, releasing slot")
                        release_and_process_next(business_id, job_id, run_id)
            except Exception as cleanup_err:
                log.error(f"[ProcessNext] Cleanup error: {cleanup_err}")


def process_bulk_call_run(run_id: int):
    """
    Background worker to process bulk call run
    Respects concurrency limits and processes queue
    
    ğŸ”’ STATE MACHINE: pending â†’ running â†’ completed/cancelled/failed
    ğŸ”’ SECURITY: Enforces business isolation throughout processing
    """
    from server.app_factory import get_process_app
    from server.models_sql import OutboundCallRun, OutboundCallJob, Lead, Business, CallLog
    import time
    import socket
    
    app = get_process_app()
    
    with app.app_context():
        try:
            # ğŸ”’ CLEANUP: Reset any stuck jobs and runs from previous failures
            cleanup_stuck_dialing_jobs()
            cleanup_stuck_runs()
            
            run = OutboundCallRun.query.get(run_id)
            if not run:
                log.error(f"[WORKER] Run {run_id} not found - consumer_source=rq")
                return
            
            # ğŸ”’ STATE MACHINE: Update from pending to running and set started_at
            worker_id = f"{socket.gethostname()}:{os.getpid()}"
            
            # ğŸ”¥ ANTI-DUPLICATE LOGGING: Track worker ID and lock acquisition
            log.info("=" * 70)
            log.info(f"[WORKER] OUTBOUND_CONSUMER_START")
            log.info(f"  â†’ WORKER_ID: {worker_id}")
            log.info(f"  â†’ run_id: {run_id}")
            log.info(f"  â†’ business_id: {run.business_id}")
            log.info(f"  â†’ consumer_source: rq_worker")
            log.info(f"  â†’ current_run_status: {run.status}")
            log.info(f"  â†’ current_locked_by: {run.locked_by_worker}")
            log.info("=" * 70)
            
            if run.status == "pending":
                run.status = "running"
                run.started_at = datetime.utcnow()
                
                # ğŸ”’ WORKER LOCK: Set worker lock with hostname+pid
                run.locked_by_worker = worker_id
                run.lock_ts = datetime.utcnow()
                run.last_heartbeat_at = datetime.utcnow()  # Initialize heartbeat
                db.session.commit()
                
                log.info(f"[WORKER] lock_acquired=true run_id={run_id} worker={worker_id} concurrency={run.concurrency}")
            else:
                # ğŸ”’ WORKER LOCK: Update lock fields when resuming
                run.locked_by_worker = worker_id
                run.lock_ts = datetime.utcnow()
                run.last_heartbeat_at = datetime.utcnow()  # Initialize heartbeat on resume
                db.session.commit()
                log.info(f"[WORKER] lock_acquired=true (resume) run_id={run_id} worker={worker_id} concurrency={run.concurrency}")
            
            # Get business details
            business = Business.query.get(run.business_id)
            if not business or not business.phone_e164:
                run.status = "failed"
                run.ended_at = datetime.utcnow()
                run.last_error = "××¡×¤×¨ ×˜×œ×¤×•×Ÿ ×©×œ ×”×¢×¡×§ ×œ× ××•×’×“×¨"
                db.session.commit()
                return
            
            from_phone = business.phone_e164
            business_name = business.name or "×”×¢×¡×§"
            
            # ğŸ”’ CRITICAL: Verify host is available for webhooks (fail fast if missing)
            try:
                host = get_public_host()
                log.info(f"[BulkCall] Run {run_id}: public_host={host} (for Twilio webhooks)")
            except RuntimeError as e:
                log.error(f"[BulkCall] Run {run_id} FAILED: {e}")
                run.status = "failed"
                run.ended_at = datetime.utcnow()
                run.last_error = "No public host configured for webhooks"
                db.session.commit()
                return
            
            # ğŸ”’ CRITICAL: Import business-level limiter to prevent exceeding 3 calls per business
            from server.services.call_limiter import count_active_outbound_calls, MAX_OUTBOUND_CALLS_PER_BUSINESS
            # ğŸ”¥ SEMAPHORE: Import Redis-based semaphore for hard 3-concurrent limit
            from server.services.outbound_semaphore import try_acquire_slot, release_slot
            
            # Process jobs in queue
            while True:
                # ğŸ”’ STATE MACHINE: Check if queue was stopped or cancellation requested
                db.session.refresh(run)
                
                # ğŸ”’ WORKER HEARTBEAT: Update lock timestamp to show worker is alive
                run.lock_ts = datetime.utcnow()
                run.last_heartbeat_at = datetime.utcnow()  # Heartbeat for stale detection
                run.updated_at = datetime.utcnow()
                db.session.commit()
                
                # ğŸ”’ CANCEL CHECK: Check if user requested cancellation BEFORE processing next call
                if run.cancel_requested and run.status != "cancelled":
                    log.info(f"[BulkCall] Run {run_id} cancellation requested, stopping...")
                    
                    # ğŸ”’ SECURITY: Mark all queued jobs as cancelled - only for this business
                    result = db.session.execute(text("""
                        UPDATE outbound_call_jobs 
                        SET status='failed',
                            error_message='Cancelled by user',
                            completed_at=NOW()
                        WHERE run_id=:run_id 
                            AND business_id=:business_id
                            AND status='queued'
                    """), {"run_id": run_id, "business_id": run.business_id})
                    
                    cancelled_count = result.rowcount
                    
                    # ğŸ”’ STATE MACHINE: Update run status to cancelled
                    run.status = "cancelled"
                    run.ended_at = datetime.utcnow()
                    run.completed_at = datetime.utcnow()  # Legacy field
                    run.queued_count = 0
                    run.failed_count += cancelled_count
                    db.session.commit()
                    
                    log.info(f"[BulkCall] Run {run_id} cancelled - marked {cancelled_count} jobs as cancelled")
                    break
                
                if run.status in ("stopping", "stopped", "cancelled"):
                    log.info(f"[BulkCall] Run {run_id} was stopped (status={run.status}), exiting")
                    # ğŸ”’ STATE MACHINE: Mark as stopped if it was in stopping state
                    if run.status == "stopping":
                        run.status = "stopped"
                        run.ended_at = datetime.utcnow()
                        db.session.commit()
                    break
                
                # ğŸ”’ DB LOCK: Get next queued job with SELECT FOR UPDATE SKIP LOCKED
                # This prevents multiple workers from picking the same job
                # SKIP LOCKED means if another worker has locked a row, we skip it and get the next one
                next_job_row = db.session.execute(text("""
                    SELECT id
                    FROM outbound_call_jobs
                    WHERE run_id = :run_id 
                        AND status = 'queued'
                        AND business_id = :business_id
                    ORDER BY id
                    LIMIT 1
                    FOR UPDATE SKIP LOCKED
                """), {"run_id": run_id, "business_id": run.business_id}).first()
                
                if next_job_row:
                    job_id = next_job_row[0]
                    # ğŸ”¥ FIX: Get ORM object within same transaction to maintain lock
                    next_job = OutboundCallJob.query.filter_by(id=job_id).with_for_update().first()
                else:
                    next_job = None
                
                if next_job:
                    # ğŸ”¥ SEMAPHORE: Try to acquire slot before starting call
                    # This enforces hard limit of 3 concurrent calls per business
                    acquired, status = try_acquire_slot(run.business_id, next_job.id)
                    
                    if not acquired:
                        if status == "queued":
                            # Redis queue is full, wait for slot to free up
                            log.debug(f"[BulkCall] Waiting for slot to free up, sleeping 1s")
                            time.sleep(1)
                            continue
                        elif status == "already_queued":
                            # ğŸ”¥ FIX: Job is already in Redis queue waiting for a slot
                            # Don't skip - wait for it to be processed by release_slot
                            log.debug(f"[BulkCall] Job {next_job.id} already in Redis queue, waiting for slot to free up")
                            time.sleep(1)
                            continue
                        elif status == "inflight":
                            # This job is already being processed by another worker
                            log.warning(f"[BulkCall] Job {next_job.id} already inflight, skipping")
                            continue
                        # else: error_fallback or no_redis - proceed anyway
                    
                    # Start this call with finally block to ensure slot release
                    try:
                        lead = Lead.query.get(next_job.lead_id)
                        if not lead or not lead.phone_e164:
                            next_job.status = "failed"
                            next_job.error_message = "××™×Ÿ ××¡×¤×¨ ×˜×œ×¤×•×Ÿ ×œ×œ×™×“"
                            next_job.completed_at = datetime.utcnow()
                            run.failed_count += 1
                            run.queued_count -= 1
                            db.session.commit()
                            continue
                        
                        # ğŸ”’ ATOMIC LOCKING: Generate unique lock token for this call
                        lock_token = str(uuid.uuid4())
                        
                        # Atomic UPDATE: Only proceed if status='queued' AND twilio_call_sid IS NULL AND dial_lock_token IS NULL
                        result = db.session.execute(text("""
                            UPDATE outbound_call_jobs 
                            SET status='dialing', 
                                dial_started_at=NOW(), 
                                dial_lock_token=:lock_token
                            WHERE id=:job_id 
                                AND status='queued' 
                                AND twilio_call_sid IS NULL
                                AND dial_lock_token IS NULL
                        """), {"job_id": next_job.id, "lock_token": lock_token})
                        
                        db.session.commit()
                        
                        # Check if we acquired the lock
                        if result.rowcount == 0:
                            # Someone else already started this call or it's no longer queued
                            log.warning(f"[BulkCall] Job {next_job.id} already being processed, skipping")
                            continue
                        
                        # Successfully acquired lock, proceed with call
                        run.in_progress_count += 1
                        run.queued_count -= 1
                        db.session.commit()
                        
                        # Normalize phone
                        normalized_phone = normalize_israeli_phone(lead.phone_e164)
                        
                        # Create call log
                        call_log = CallLog()
                        call_log.business_id = run.business_id
                        call_log.lead_id = lead.id
                        call_log.from_number = from_phone
                        call_log.to_number = normalized_phone
                        call_log.direction = "outbound"
                        call_log.status = "initiated"
                        call_log.call_status = "initiated"
                        # ğŸ”¥ NAME SSOT: Store lead name for NAME_ANCHOR system
                        call_log.customer_name = _get_lead_display_name(lead)
                        # ğŸ¯ PROJECT TRACKING: Associate call with project if job has project_id
                        if next_job.project_id:
                            call_log.project_id = next_job.project_id
                        db.session.add(call_log)
                        db.session.flush()
                        
                        next_job.call_log_id = call_log.id
                        db.session.commit()
                        
                        # Initiate Twilio call
                        # ğŸ”¥ SSOT: Use centralized outbound call service
                        from server.services.twilio_outbound_service import create_outbound_call
                        
                        # ğŸ”¥ NAME SSOT: Extract lead name
                        lead_name = _get_lead_display_name(lead)
                        
                        try:
                            result = create_outbound_call(
                                to_phone=normalized_phone,
                                from_phone=from_phone,
                                business_id=run.business_id,
                                host=host,
                                lead_id=lead.id,
                                job_id=next_job.id,
                                business_name=business_name,
                                lead_name=lead_name
                            )
                            
                            call_sid = result["call_sid"]
                            is_duplicate = result.get("is_duplicate", False)
                            
                            if is_duplicate:
                                log.warning("=" * 70)
                                log.warning(f"[WORKER] DEDUP_CONFLICT detected")
                                log.warning(f"  â†’ WORKER_ID: {worker_id}")
                                log.warning(f"  â†’ run_id: {run_id}")
                                log.warning(f"  â†’ job_id: {next_job.id}")
                                log.warning(f"  â†’ lead_id: {lead.id}")
                                log.warning(f"  â†’ dedup_conflict: true")
                                log.warning(f"  â†’ action: skipping_duplicate")
                                log.warning("=" * 70)
                                continue
                            
                            # ğŸ”’ ATOMIC UPDATE: Update with Twilio call SID only if lock token matches
                            update_result = db.session.execute(text("""
                                UPDATE outbound_call_jobs 
                                SET twilio_call_sid=:twilio_sid, 
                                    call_sid=:twilio_sid,
                                    status='calling',
                                    started_at=NOW()
                                WHERE id=:job_id 
                                    AND dial_lock_token=:lock_token
                            """), {
                                "job_id": next_job.id, 
                                "twilio_sid": call_sid, 
                                "lock_token": lock_token
                            })
                            
                            # ğŸ”¥ FIX: Only log error if lock token actually mismatched (rowcount == 0)
                            if update_result.rowcount == 0:
                                log.error(f"[BulkCall] Lock token mismatch for job {next_job.id}, call may be duplicate")
                                # Call was created but we lost the lock - log warning but continue
                                # Twilio will handle the duplicate via their idempotency
                            
                            call_log.call_sid = call_sid
                            db.session.commit()
                            
                            log.info(f"[BulkCall] Started call for lead {lead.id}, job {next_job.id}, call_sid={call_sid}")
                            
                            # ğŸ”’ FIX: Add small delay after starting each call to prevent race conditions
                            # This ensures database status updates propagate before checking active count again
                            time.sleep(0.5)  # 500ms delay to prevent starting too many calls at once
                            
                        except Exception as twilio_error:
                            # ğŸ”’ DEDUPLICATION: Handle Twilio timeout/exception
                            # Don't retry if we already have a twilio_call_sid (call may have succeeded)
                            log.error(f"[BulkCall] Twilio error for job {next_job.id}: {twilio_error}")
                            
                            # Check if call was actually created despite the error
                            db.session.refresh(next_job)
                            if next_job.twilio_call_sid:
                                # Call was created successfully despite error
                                log.info(f"[BulkCall] Call created despite error for job {next_job.id}")
                                raise twilio_error
                            else:
                                # Call was not created - reset to queued for retry (but only if no call SID)
                                result = db.session.execute(text("""
                                    UPDATE outbound_call_jobs 
                                    SET status='queued',
                                        dial_lock_token=NULL,
                                        dial_started_at=NULL
                                    WHERE id=:job_id 
                                        AND twilio_call_sid IS NULL
                                        AND dial_lock_token=:lock_token
                                """), {"job_id": next_job.id, "lock_token": lock_token})
                                
                                run.in_progress_count -= 1
                                run.queued_count += 1
                                db.session.commit()
                                
                                log.warning(f"[BulkCall] Reset job {next_job.id} to queued for retry")
                                raise twilio_error
                        
                    except Exception as e:
                        log.error(f"[BulkCall] Error starting call for job {next_job.id}: {e}")
                        # Only mark as failed if we don't have a call SID (meaning call didn't start)
                        db.session.refresh(next_job)
                        if not next_job.twilio_call_sid:
                            next_job.status = "failed"
                            next_job.error_message = str(e)
                            next_job.completed_at = datetime.utcnow()
                            run.in_progress_count = max(0, run.in_progress_count - 1)
                            run.failed_count += 1
                            run.last_error = str(e)[:500]
                            db.session.commit()
                    finally:
                        # ğŸ”¥ CRITICAL: Release semaphore slot if call never started
                        # If call started successfully, webhook will release the slot when call ends
                        # Only release here if we failed before call actually started (no twilio_call_sid)
                        try:
                            db.session.refresh(next_job)
                            if not next_job.twilio_call_sid:
                                # Call never started - release slot now
                                from server.services.outbound_semaphore import release_slot
                                next_job_id = release_slot(run.business_id, next_job.id)
                                if next_job_id:
                                    log.info(f"[SEMAPHORE] Released slot for failed job {next_job.id}, next job {next_job_id} can proceed")
                                else:
                                    log.info(f"[SEMAPHORE] Released slot for failed job {next_job.id}, no jobs waiting")
                            # else: Call started, webhook will release the slot when it ends
                        except Exception as e:
                            log.error(f"[SEMAPHORE] Error releasing slot: {e}")
                            # Don't raise - we're in cleanup
                else:
                    # No more queued jobs - check if we're done
                    active_jobs_count = OutboundCallJob.query.filter(
                        OutboundCallJob.run_id == run_id,
                        OutboundCallJob.status.in_(["dialing", "calling"])
                    ).count()
                    
                    if active_jobs_count == 0:
                        # ğŸ”’ STATE MACHINE: All done - mark as completed
                        run.status = "completed"
                        run.ended_at = datetime.utcnow()
                        run.completed_at = datetime.utcnow()  # Legacy field
                        run.cursor_position = run.total_leads  # Processed all
                        db.session.commit()
                        log.info(f"[BulkCall] Run {run_id} completed: {run.completed_count} succeeded, {run.failed_count} failed")
                        break
                    else:
                        # Wait for active calls to complete
                        time.sleep(2)
                
                # ğŸ”’ CURSOR: Update cursor position after processing
                completed_jobs = OutboundCallJob.query.filter(
                    OutboundCallJob.run_id == run_id,
                    OutboundCallJob.status.in_(["completed", "failed", "cancelled"])
                ).count()
                run.cursor_position = completed_jobs
                db.session.commit()  # Persist cursor position
                
                # Refresh run to get latest counts
                db.session.refresh(run)
            
        except Exception as e:
            log.error(f"[BulkCall] Error in run {run_id}: {e}")
            import traceback
            traceback.print_exc()
            
            # ğŸ”’ STATE MACHINE: Mark run as failed on unexpected error
            try:
                run = OutboundCallRun.query.get(run_id)
                if run:
                    run.status = "failed"
                    run.ended_at = datetime.utcnow()
                    run.last_error = str(e)[:500]
                    db.session.commit()
            except:
                pass


def cleanup_stuck_dialing_jobs():
    """
    Cleanup jobs stuck in 'dialing' or 'calling' status
    
    This handles edge cases where:
    - calls.create succeeded but UPDATE with call_sid failed
    - Process crashed between acquiring lock and creating call
    - System restart left jobs in active state
    
    ğŸ”’ CRITICAL: Reset ALL stuck jobs on startup to prevent blocking new calls
    
    ğŸ”¥ FIX: Also cleanup stale call_log records with call_sid IS NULL
    - Records with call_sid=NULL and status IN ('initiated', 'ringing', 'in-progress') older than 60 seconds
    - These block new calls via dedup check
    
    NOTE: This function assumes it's called from within an app context
    (either during app startup or from a request handler)
    """
    from server.models_sql import OutboundCallJob
    
    # ğŸ”¥ FIX: Don't call get_process_app() - assume we're already in app context
    # This prevents circular dependency when called from create_app()
    try:
        # ğŸ”¥ FIX: On startup, reset ALL jobs stuck in 'dialing' state
        # Don't wait 5 minutes - clean immediately to prevent blocking
        result_dialing = db.session.execute(text("""
            UPDATE outbound_call_jobs 
            SET status='failed',
                error_message='System restart - job was stuck in dialing state',
                completed_at=NOW()
            WHERE status='dialing'
                AND twilio_call_sid IS NULL
        """))
        
        # ğŸ”¥ FIX: Also reset jobs in 'calling' state that are old (>10 minutes)
        # These are likely calls that ended but webhook never arrived
        cutoff_time = datetime.utcnow() - timedelta(minutes=10)
        result_calling = db.session.execute(text("""
            UPDATE outbound_call_jobs 
            SET status='failed',
                error_message='Call timeout - no status update received',
                completed_at=NOW()
            WHERE status='calling'
                AND started_at < :cutoff_time
        """), {"cutoff_time": cutoff_time})
        
        # ğŸ”¥ FIX: Cleanup stale call_log records with call_sid IS NULL
        # These are records that never got a SID from Twilio (failed before SID was assigned)
        # They block new calls via the dedup check
        stale_threshold = datetime.utcnow() - timedelta(seconds=60)
        result_stale_calls = db.session.execute(text("""
            UPDATE call_log
            SET status='failed',
                error_message='Stale record - no call_sid received from Twilio'
            WHERE call_sid IS NULL
                AND status IN ('initiated', 'ringing', 'in-progress')
                AND created_at < :stale_threshold
        """), {"stale_threshold": stale_threshold})
        
        db.session.commit()
        
        total_cleaned = result_dialing.rowcount + result_calling.rowcount + result_stale_calls.rowcount
        if total_cleaned > 0:
            log.info(f"[CLEANUP] âœ… Reset {result_dialing.rowcount} stuck 'dialing' jobs, {result_calling.rowcount} stuck 'calling' jobs, and {result_stale_calls.rowcount} stale call_log records")
        
        return total_cleaned
        
    except Exception as e:
        log.error(f"[CLEANUP] Error cleaning up stuck jobs: {e}")
        db.session.rollback()
        return 0


def cleanup_stuck_runs(on_startup: bool = False):
    """
    ğŸ”¥ CRITICAL: Cleanup runs stuck in 'running' status with TTL-based heartbeat check
    
    This prevents "ghost active queue" bug where old runs from before a crash/restart
    continue showing as active even though no actual processing is happening.
    
    A run is considered stuck if:
    - status = 'running' 
    - lock_ts > TTL_MINUTES ago (no heartbeat from worker)
    - OR queued_count = 0 AND in_progress_count = 0 (nothing actually running)
    - OR on_startup=True (server restart = all workers are dead)
    
    ğŸ”’ TTL-BASED RECLAIM: Uses lock_ts (heartbeat) instead of updated_at
    - Workers update lock_ts every iteration (heartbeat)
    - If lock_ts is stale (> 5 minutes), worker is considered dead
    - Run is marked as 'failed' with proper error message
    
    ğŸ”¥ STARTUP MODE: When on_startup=True, mark ALL 'running' runs as failed
    - On server restart, NO workers can be active
    - This prevents ghost queues from showing after immediate restart
    
    NOTE: This function assumes it's called from within an app context
    (typically during app startup or from a periodic cleanup task)
    """
    from server.models_sql import OutboundCallRun
    
    # ğŸ”’ TTL for worker heartbeat: 5 minutes (only used when not on startup)
    TTL_MINUTES = 5
    
    try:
        if on_startup:
            # ğŸ”¥ STARTUP MODE: Mark ALL 'running' runs as failed immediately
            # On server restart, all workers are dead by definition
            result = db.session.execute(text("""
                UPDATE outbound_call_runs 
                SET status='failed',
                    ended_at=NOW(),
                    completed_at=NOW(),
                    last_error=CONCAT('Server restarted - worker ', COALESCE(locked_by_worker, 'unknown'), ' terminated')
                WHERE status='running'
            """))
            
            db.session.commit()
            
            cleaned_count = result.rowcount
            if cleaned_count > 0:
                log.warning(f"[CLEANUP] âš ï¸  Marked {cleaned_count} running queues as failed on server startup")
            
            return cleaned_count
        else:
            # ğŸ”’ PERIODIC MODE: Use heartbeat TTL to detect dead workers
            # Mark runs as failed if heartbeat is stale (worker died)
            heartbeat_cutoff = datetime.utcnow() - timedelta(minutes=TTL_MINUTES)
            
            # Also check updated_at as fallback (for runs started before this fix)
            updated_cutoff = datetime.utcnow() - timedelta(minutes=30)
            
            result = db.session.execute(text("""
                UPDATE outbound_call_runs 
                SET status='failed',
                    ended_at=NOW(),
                    completed_at=NOW(),
                    last_error=CONCAT('Worker timeout - no heartbeat from ', locked_by_worker, ' since ', COALESCE(last_heartbeat_at, lock_ts))
                WHERE status='running'
                    AND (
                        -- ğŸ”’ PRIMARY: Check heartbeat (last_heartbeat_at)
                        (last_heartbeat_at IS NOT NULL AND last_heartbeat_at < :heartbeat_cutoff)
                        -- Fallback: Check lock_ts if last_heartbeat_at is NULL
                        OR (last_heartbeat_at IS NULL AND lock_ts IS NOT NULL AND lock_ts < :heartbeat_cutoff)
                        -- Legacy fallback: Old updated_at check
                        OR (last_heartbeat_at IS NULL AND lock_ts IS NULL AND updated_at < :updated_cutoff)
                        -- Empty queue (nothing to process)
                        OR (queued_count = 0 AND in_progress_count = 0)
                    )
            """), {
                "heartbeat_cutoff": heartbeat_cutoff,
                "updated_cutoff": updated_cutoff
            })
            
            db.session.commit()
            
            cleaned_count = result.rowcount
            if cleaned_count > 0:
                log.warning(f"[CLEANUP] âš ï¸  Reclaimed {cleaned_count} stuck runs (TTL-based heartbeat check, TTL={TTL_MINUTES}min)")
            
            return cleaned_count
        
    except Exception as e:
        log.error(f"[CLEANUP] Error cleaning up stuck runs: {e}")
        db.session.rollback()
        return 0


@outbound_bp.route("/api/outbound/cleanup-stuck-jobs", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def cleanup_stuck_jobs_endpoint():
    """
    Manually trigger cleanup of jobs stuck in 'dialing' status
    
    This is useful for recovering from crashes or network issues.
    Jobs stuck in 'dialing' for >5 minutes without call_sid are reset to 'queued'.
    
    Rate limited to 1 request per minute per business to prevent abuse.
    Note: Rate limit is per-worker/process. In multi-worker setups, each worker
    enforces its own limit. This is acceptable for this use case.
    """
    from flask import session
    from datetime import datetime, timedelta
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can clean up across all tenants
            pass
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    # Rate limiting: Check if cleanup was called in the last minute
    # This is per-worker, which is acceptable for this admin endpoint
    cache_key = f"cleanup_last_call_{tenant_id or 'all'}"
    from server.stream_state import stream_registry
    last_call = stream_registry.get_metadata('global', cache_key)
    
    if last_call:
        last_call_time = datetime.fromisoformat(last_call)
        if datetime.utcnow() - last_call_time < timedelta(minutes=1):
            seconds_remaining = 60 - (datetime.utcnow() - last_call_time).seconds
            return jsonify({
                "error": f"×™×© ×œ×”××ª×™×Ÿ {seconds_remaining} ×©× ×™×•×ª ×œ×¤× ×™ × ×™×§×•×™ × ×•×¡×£",
                "retry_after": seconds_remaining
            }), 429
    
    try:
        count = cleanup_stuck_dialing_jobs()
        
        # Update last call time
        stream_registry.set_metadata('global', cache_key, datetime.utcnow().isoformat())
        
        log.info(f"âœ… Cleanup completed: {count} jobs reset to queued")
        
        return jsonify({
            "success": True,
            "cleaned_count": count,
            "message": f"× ×•×§×• {count} ××©×™××•×ª ×ª×§×•×¢×•×ª"
        })
        
    except Exception as e:
        log.error(f"Error in cleanup endpoint: {e}")
        return jsonify({"error": "×©×’×™××” ×‘× ×™×§×•×™ ××©×™××•×ª"}), 500


@outbound_bp.route("/api/outbound/runs/reconcile", methods=["POST"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def reconcile_stuck_runs():
    """
    ğŸ”§ Reconcile stuck runs - fix runs showing as active when they're not
    
    This endpoint:
    1. Finds runs with status=running/pending but no active jobs
    2. Marks them as completed or failed based on their actual state
    3. Clears worker locks
    4. Fixes job counts
    
    This is useful when:
    - UI shows progress but no calls are actually happening
    - Worker died and didn't clean up properly
    - Database got into inconsistent state
    
    Rate limited to 1 request per minute per business to prevent abuse.
    
    Returns:
    {
        "success": true,
        "reconciled_count": 3,
        "runs": [
            {"run_id": 123, "status": "completed", "reason": "No active jobs"},
            ...
        ]
    }
    """
    from flask import session
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            # System admin can reconcile across all tenants
            tenant_id = None
        else:
            return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    # ğŸ”’ RATE LIMITING: Check if reconcile was called in the last minute
    # This is per-worker, which is acceptable for this admin endpoint
    cache_key = f"reconcile_last_call_{tenant_id or 'all'}"
    from server.stream_state import stream_registry
    last_call = stream_registry.get_metadata('global', cache_key)
    
    if last_call:
        last_call_time = datetime.fromisoformat(last_call)
        if datetime.utcnow() - last_call_time < timedelta(minutes=1):
            seconds_remaining = 60 - (datetime.utcnow() - last_call_time).seconds
            return jsonify({
                "error": f"×™×© ×œ×”××ª×™×Ÿ {seconds_remaining} ×©× ×™×•×ª ×œ×¤× ×™ ×ª×™×§×•×Ÿ × ×•×¡×£",
                "retry_after": seconds_remaining
            }), 429
    
    try:
        reconciled = []
        
        # Find runs that are marked as running/pending but have no active jobs
        query = OutboundCallRun.query.filter(
            OutboundCallRun.status.in_(['running', 'pending'])
        )
        
        if tenant_id:
            query = query.filter(OutboundCallRun.business_id == tenant_id)
        
        stuck_runs = query.all()
        
        if not stuck_runs:
            # Update last call time even if no runs to reconcile
            stream_registry.set_metadata('global', cache_key, datetime.utcnow().isoformat())
            return jsonify({
                "success": True,
                "reconciled_count": 0,
                "runs": [],
                "message": "××™×Ÿ ×¨×™×¦×•×ª ×ª×§×•×¢×•×ª ×œ×ª×™×§×•×Ÿ"
            })
        
        # ğŸ”¥ OPTIMIZATION: Get job counts for all runs in one query to avoid N+1 problem
        run_ids = [run.id for run in stuck_runs]
        
        # Query to get active job counts per run
        active_counts_query = db.session.query(
            OutboundCallJob.run_id,
            func.count(OutboundCallJob.id).label('count')
        ).filter(
            OutboundCallJob.run_id.in_(run_ids),
            OutboundCallJob.status.in_(['queued', 'dialing', 'calling'])
        ).group_by(OutboundCallJob.run_id).all()
        
        active_counts = {run_id: count for run_id, count in active_counts_query}
        
        # Query to get completed job counts per run
        completed_counts_query = db.session.query(
            OutboundCallJob.run_id,
            func.count(OutboundCallJob.id).label('count')
        ).filter(
            OutboundCallJob.run_id.in_(run_ids),
            OutboundCallJob.status.in_(['completed', 'failed'])
        ).group_by(OutboundCallJob.run_id).all()
        
        completed_counts = {run_id: count for run_id, count in completed_counts_query}
        
        for run in stuck_runs:
            # Get counts from preloaded data
            active_jobs = active_counts.get(run.id, 0)
            completed_jobs = completed_counts.get(run.id, 0)
            
            # If no active jobs, reconcile the run
            if active_jobs == 0:
                old_status = run.status
                
                # Determine new status based on completed jobs
                if completed_jobs >= run.total_leads:
                    # All jobs are done
                    run.status = 'completed'
                    reason = 'All jobs completed'
                elif completed_jobs > 0:
                    # Some jobs completed, some failed or never started
                    run.status = 'completed'
                    reason = f'{completed_jobs}/{run.total_leads} jobs completed'
                else:
                    # No jobs completed at all
                    run.status = 'failed'
                    run.last_error = 'No jobs were processed'
                    reason = 'No jobs processed'
                
                # Clear worker lock
                run.locked_by_worker = None
                run.lock_ts = None
                run.last_heartbeat_at = None
                
                # Set ended_at if not set
                if not run.ended_at:
                    run.ended_at = datetime.utcnow()
                    run.completed_at = datetime.utcnow()
                
                # Update counts to match reality
                run.queued_count = 0
                run.in_progress_count = 0
                
                reconciled.append({
                    'run_id': run.id,
                    'old_status': old_status,
                    'new_status': run.status,
                    'reason': reason,
                    'completed_jobs': completed_jobs,
                    'total_leads': run.total_leads
                })
                
                log.info(f"[RECONCILE] Run {run.id}: {old_status} â†’ {run.status} ({reason})")
        
        # ğŸ”¥ FIX: Commit all changes at once to ensure consistency
        db.session.commit()
        
        # Update last call time after successful reconcile
        stream_registry.set_metadata('global', cache_key, datetime.utcnow().isoformat())
        
        return jsonify({
            "success": True,
            "reconciled_count": len(reconciled),
            "runs": reconciled,
            "message": f"×ª×•×§× ×• {len(reconciled)} ×¨×™×¦×•×ª ×ª×§×•×¢×•×ª"
        })
        
    except Exception as e:
        log.error(f"Error in reconcile endpoint: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()
        return jsonify({"error": "×©×’×™××” ×‘×ª×™×§×•×Ÿ ×¨×™×¦×•×ª ×ª×§×•×¢×•×ª"}), 500


@outbound_bp.route("/api/outbound/leads/export", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin'])
@require_page_access('calls_outbound')
def export_leads_by_status():
    """
    Export leads filtered by status to CSV format
    
    Query params:
    - status_id: Status name/ID to filter by (required)
    - format: Export format (csv only for now, default: csv)
    
    Returns:
    CSV file with leads data including status information
    
    Filename format: outbound_leads_status_<statusId>_<statusName>_<YYYY-MM-DD>.csv
    """
    from flask import session, Response
    import csv
    import io
    from server.models_sql import LeadStatus
    
    tenant_id = g.get('tenant')
    
    if not tenant_id:
        user = session.get('user', {})
        if user.get('role') == 'system_admin':
            return jsonify({"error": "×™×© ×œ×‘×—×•×¨ ×¢×¡×§ ×œ×¤× ×™ ×™×™×¦×•×"}), 400
        return jsonify({"error": "××™×Ÿ ×’×™×©×” ×œ×¢×¡×§"}), 403
    
    # Get parameters
    status_name = request.args.get('status_id', '').strip()
    export_format = request.args.get('format', 'csv').lower()
    
    if not status_name:
        return jsonify({"error": "×—×¡×¨ ×¤×¨××˜×¨ status_id"}), 400
    
    if export_format != 'csv':
        return jsonify({"error": "× ×ª××š ×¨×§ ×¤×•×¨××˜ CSV ×›×¨×’×¢"}), 400
    
    # Validate status name (security)
    if not STATUS_FILTER_PATTERN.match(status_name) or len(status_name) > 64:
        return jsonify({"error": "×©× ×¡×˜×˜×•×¡ ×œ× ×ª×§×™×Ÿ"}), 400
    
    try:
        # Get status info for proper labeling
        status_obj = LeadStatus.query.filter_by(
            tenant_id=tenant_id,
            name=status_name.lower()
        ).first()
        
        status_label = status_obj.label if status_obj else status_name
        
        # Query leads with this status for this tenant
        leads = Lead.query.filter_by(
            tenant_id=tenant_id,
            status=status_name.lower()
        ).order_by(Lead.created_at.desc()).all()
        
        # Get last call information for each lead
        lead_call_info = {}
        if leads:
            lead_ids = [lead.id for lead in leads]
            # Get most recent call for each lead
            recent_calls = db.session.query(
                CallLog.lead_id,
                func.max(CallLog.created_at).label('last_call_at')
            ).filter(
                CallLog.lead_id.in_(lead_ids),
                CallLog.business_id == tenant_id
            ).group_by(CallLog.lead_id).all()
            
            for lead_id, last_call_at in recent_calls:
                # Get the actual call record to get status
                call = CallLog.query.filter_by(
                    lead_id=lead_id,
                    business_id=tenant_id
                ).order_by(CallLog.created_at.desc()).first()
                
                lead_call_info[lead_id] = {
                    'last_call_at': last_call_at,
                    'last_call_status': call.call_status if call else None
                }
        
        # Create CSV in memory
        output = io.StringIO()
        
        # Use UTF-8 with BOM for Excel Hebrew compatibility
        writer = csv.writer(output)
        
        # Write header
        writer.writerow([
            'status_id',
            'status_name',
            'lead_id',
            'full_name',
            'phone',
            'email',
            'created_at',
            'last_call_at',
            'last_call_status',
            'source',
            'notes'
        ])
        
        # Write lead rows
        for lead in leads:
            call_info = lead_call_info.get(lead.id, {})
            
            writer.writerow([
                status_name.lower(),
                status_label,
                lead.id,
                lead.full_name or '',
                lead.phone_e164 or '',
                lead.email or '',
                lead.created_at.isoformat() if lead.created_at else '',
                call_info.get('last_call_at').isoformat() if call_info.get('last_call_at') else '',
                call_info.get('last_call_status') or '',
                lead.source or '',
                lead.notes or ''
            ])
        
        # Get CSV content with UTF-8 BOM
        csv_content = '\ufeff' + output.getvalue()
        output.close()
        
        # Generate filename
        today = datetime.now().strftime('%Y-%m-%d')
        # Sanitize status name for filename (remove special chars)
        safe_status_name = re.sub(r'[^a-zA-Z0-9_-]', '', status_name)
        filename = f"outbound_leads_status_{safe_status_name}_{today}.csv"
        
        log.info(f"ğŸ“Š Exporting {len(leads)} leads with status '{status_name}' for tenant {tenant_id}")
        
        # Return CSV file
        return Response(
            csv_content,
            mimetype='text/csv; charset=utf-8',
            headers={
                'Content-Disposition': f'attachment; filename="{filename}"',
                'Content-Type': 'text/csv; charset=utf-8'
            }
        )
        
    except Exception as e:
        log.error(f"Error exporting leads by status: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"×©×’×™××” ×‘×™×™×¦×•×: {str(e)}"}), 500


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API COMPATIBILITY LAYER - Aliases for Frontend
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ PURPOSE: Frontend expects specific endpoint names after refactor to Runs
# These aliases map old endpoint names to new implementations

@outbound_bp.route("/api/inbound/recent", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_inbound')
def get_inbound_recent_alias():
    """
    âœ… COMPATIBILITY ALIAS: Maps /api/inbound/recent â†’ /api/inbound/recent-calls
    
    Frontend expects this endpoint after refactor to CallLog/RecordingRun.
    This ensures no 404 errors in UI.
    """
    return get_recent_inbound_calls()


@outbound_bp.route("/api/inbound/calls/counts", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_inbound')
def get_inbound_counts_alias():
    """
    âœ… COMPATIBILITY ALIAS: Maps /api/inbound/calls/counts â†’ /api/inbound_calls/counts
    
    Frontend expects this endpoint for call counters.
    """
    return get_inbound_call_counts_endpoint()


@outbound_bp.route("/api/outbound/recent", methods=["GET"])
@require_api_auth(['system_admin', 'owner', 'admin', 'agent'])
@require_page_access('calls_outbound')
def get_outbound_recent_alias():
    """
    âœ… COMPATIBILITY ALIAS: Maps /api/outbound/recent â†’ /api/outbound/recent-calls
    
    Frontend expects this endpoint after refactor to CallLog/OutboundCallRun.
    """
    return get_recent_calls()


# Note: /api/outbound/bulk/active already exists at line 2035 (no alias needed)
