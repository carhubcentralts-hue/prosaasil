"""
Smart Prompt Generator v2 - Structured System Prompt Builder
Creates professional SYSTEM PROMPTS using rigid templates and AI as architect

ğŸ¯ Goal: Generate production-ready prompts with consistent structure
ğŸ“‹ Based on questionnaire â†’ structured template â†’ quality validation

Key Principles:
- âŒ Does NOT generate free-form text
- âœ… Builds prompts using rigid template structure
- âœ… LLM serves as conversation architect, not copywriter
- âœ… Every prompt includes: identity, goal, rules, flow, stop conditions, limitations
"""
from flask import Blueprint, request, jsonify, session
from server.routes_admin import require_api_auth
from server.extensions import csrf
from server.models_sql import Business, BusinessSettings, PromptRevisions, db
import logging
import os
import json
from datetime import datetime
import re
import time

logger = logging.getLogger(__name__)

smart_prompt_bp = Blueprint('smart_prompt', __name__)

# Maximum input lengths for security
MAX_FIELD_LENGTH = 500
MAX_TOTAL_INPUT = 4000

# OpenAI generation settings
MAX_RETRIES = 2  # 1 initial attempt + 1 retry
RETRY_DELAY = 0.5  # seconds
OPENAI_TIMEOUT = 12.0  # seconds

# Output template sections - MUST appear in this exact order
REQUIRED_SECTIONS = [
    "×–×”×•×ª ×”×¡×•×›×Ÿ",
    "××˜×¨×ª ×”×©×™×—×”",
    "×—×•×§×™ ×©×™×—×”",
    "××”×œ×š ×©×™×—×”",
    "×ª× ××™ ×¢×¦×™×¨×” / ×”×¢×‘×¨×”",
    "××’×‘×œ×•×ª ×•××™×¡×•×¨×™×"
]

# Internal System Prompt - This IS the smart generator itself
# This meta-prompt is THE CORE - without it, output will be poor
GENERATOR_SYSTEM_PROMPT = """××ª×” ××—×•×œ×œ SYSTEM PROMPTS ×œ×¡×•×›× ×™ AI ×§×•×œ×™× ×•×›×ª×•×‘×™×.

×”××˜×¨×” ×©×œ×š:
×œ×™×™×¦×¨ ×¤×¨×•××¤×˜ ××§×¦×•×¢×™ ×œ×¡×•×›×Ÿ ×©×™×¨×•×ª / ××›×™×¨×”,
×©×™×¢×‘×•×“ ×‘×©×™×—×” ×—×™×” ×¢× ×œ×§×•×—×•×ª ×××™×ª×™×™×.

×—×•×§×™×:
- ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“
- ××œ ×ª×›×ª×•×‘ ×˜×§×¡×˜ ×©×™×•×•×§×™
- ××œ ×ª×›×ª×•×‘ ×¤×¡×§××•×ª
- ××œ ×ª×¡×‘×™×¨ ×“×‘×¨×™× ×œ×œ×§×•×—
- ×›×ª×•×‘ ×¨×§ ×”×•×¨××•×ª ×œ×¡×•×›×Ÿ
- ×”×©×ª××© ×‘×›×•×ª×¨×•×ª ×‘×¨×•×¨×•×ª
- ×”×©×ª××© ×‘×¨×©×™××•×ª
- ×©××œ×•×ª â€“ ××—×ª ×‘×›×œ ×¤×¢×
- ×¡×•×›×Ÿ ×—×™×™×‘ ×œ×“×¢×ª ××ª×™ ×œ×¢×¦×•×¨

×—×•×§×™× ×§×¨×™×˜×™×™× - ×—×•×‘×”:
- ××¡×•×¨ ×œ×š ×œ×‘×§×© ××™×“×¢ ×—×¡×¨ ××• ×œ×”×—×–×™×¨ ×”×•×“×¢×” ×©×—×¡×¨×™× ×¤×¨×˜×™×
- ××¡×•×¨ ×œ×›×ª×•×‘ "×—×¡×¨×•×ª ×©××œ×•×ª" ××• "×¦×¨×™×š ×¢×•×“ ×¤×¨×˜×™×" ××• ×“×•××”
- ×× ×—×¡×¨ ××™×“×¢ - ×ª×™×™×¦×¨ ×¤×¨×•××¤×˜ ××•×©×œ× ×œ×¤×™ ××” ×©×™×©
- ×”×©×ª××© ×‘-placeholders ×”×’×™×•× ×™×™× ×‘××§×•× ××™×“×¢ ×—×¡×¨ (×œ×“×•×’××”: {{BUSINESS_NAME}}, {{HOURS}})
- ×× ×©×¢×•×ª ×œ× ×™×“×•×¢×•×ª - ×›×ª×•×‘ "×©×¢×•×ª ×¤×¢×™×œ×•×ª: {{HOURS}} (××• '×œ× ×¦×•×™×Ÿ')"
- ×× ×©×™×¨×•×ª×™× ×œ× ×™×“×•×¢×™× - ×›×ª×•×‘ "×©×™×¨×•×ª×™×: {{SERVICES}}"
- ×ª××™×“ ×ª×™×™×¦×¨ ×¤×¨×•××¤×˜ ×©×œ× ×•×©××™×©, ×œ×œ× ×—×¨×™×’×™×

×”×¤×¨×•××¤×˜ ×—×™×™×‘ ×œ×›×œ×•×œ ××ª ×”×¡×¢×™×¤×™× ×”×‘××™× ×‘×œ×‘×“,
×•×‘×“×™×•×§ ×‘×¡×“×¨ ×”×–×”:

1. ×–×”×•×ª ×”×¡×•×›×Ÿ
2. ××˜×¨×ª ×”×©×™×—×”
3. ×—×•×§×™ ×©×™×—×”
4. ××”×œ×š ×©×™×—×” (×©×œ×‘×™×)
5. ×ª× ××™ ×¢×¦×™×¨×” / ×”×¢×‘×¨×” ×œ× ×¦×™×’ ×× ×•×©×™
6. ××’×‘×œ×•×ª ×•××™×¡×•×¨×™×

×× ×—×¡×¨ ××™×“×¢ â€“ ×”×©×œ× ×‘×¦×•×¨×” ×¡×‘×™×¨×” ×¢× placeholders,
××‘×œ ××œ ×ª××¦×™× ×”×‘×˜×—×•×ª, ××—×™×¨×™× ××• ×”×ª×—×™×™×‘×•×™×•×ª.

×”×—×–×¨ ××ª ×”×ª×©×•×‘×” ×‘×¤×•×¨××˜ ×”×‘× ×‘×“×™×•×§:

========================
×–×”×•×ª ×”×¡×•×›×Ÿ
========================
[×ª×•×›×Ÿ]

========================
××˜×¨×ª ×”×©×™×—×”
========================
[×ª×•×›×Ÿ]

========================
×—×•×§×™ ×©×™×—×”
========================
- [×—×•×§ 1]
- [×—×•×§ 2]
...

========================
××”×œ×š ×©×™×—×”
========================
1. [×©×œ×‘ 1]
2. [×©×œ×‘ 2]
...

========================
×ª× ××™ ×¢×¦×™×¨×” / ×”×¢×‘×¨×”
========================
- [×ª× ××™ 1]
- [×ª× ××™ 2]
...

========================
××’×‘×œ×•×ª ×•××™×¡×•×¨×™×
========================
- [××’×‘×œ×” 1]
- [××’×‘×œ×” 2]
..."""


def _get_business_id():
    """Get current business ID from session"""
    from flask import g
    
    user_session = session.get('user') or {}
    tenant_id = g.get('tenant') or session.get('impersonated_tenant_id')
    
    if not tenant_id:
        tenant_id = user_session.get('business_id') if isinstance(user_session, dict) else None
    
    if not tenant_id:
        user = session.get('al_user') or {}
        tenant_id = user.get('business_id') if isinstance(user, dict) else None
    
    return tenant_id


def _sanitize_input(text: str, max_length: int = MAX_FIELD_LENGTH) -> str:
    """Sanitize and truncate input text"""
    if not text:
        return ''
    
    sanitized = text.strip()
    
    # Truncate to max length
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length]
    
    return sanitized


def _validate_prompt_structure(prompt_text: str) -> tuple[bool, str]:
    """
    Quality Gate: Validate generated prompt has correct structure
    
    Returns: (is_valid, error_message)
    """
    # Check for required sections
    missing_sections = []
    for section in REQUIRED_SECTIONS:
        if section not in prompt_text:
            missing_sections.append(section)
    
    if missing_sections:
        return False, f"×—×¡×¨×™× ×¡×¢×™×¤×™× ×—×•×‘×”: {', '.join(missing_sections)}"
    
    # Check for "×©××œ×” ××—×ª ×‘×›×œ ×¤×¢×" rule
    if "×©××œ×” ××—×ª" not in prompt_text and "×©××œ×” 1" not in prompt_text:
        return False, "×—×¡×¨ ×›×œ×œ '×©××œ×” ××—×ª ×‘×›×œ ×¤×¢×'"
    
    # Check for clear goal in "××˜×¨×ª ×”×©×™×—×”" section
    goal_section = prompt_text.split("××˜×¨×ª ×”×©×™×—×”")[1].split("===")[1] if "××˜×¨×ª ×”×©×™×—×”" in prompt_text else ""
    if len(goal_section.strip()) < 20:
        return False, "××˜×¨×ª ×”×©×™×—×” ×œ× ××¤×•×¨×˜×ª ××¡×¤×™×§"
    
    # Check for stop conditions
    stop_section = prompt_text.split("×ª× ××™ ×¢×¦×™×¨×”")[1] if "×ª× ××™ ×¢×¦×™×¨×”" in prompt_text else ""
    if len(stop_section.strip()) < 20:
        return False, "×ª× ××™ ×¢×¦×™×¨×” ×œ× ××¤×•×¨×˜×™×"
    
    # Check for long paragraphs (more than 300 chars without newline is suspicious)
    lines = prompt_text.split('\n')
    for line in lines:
        # Skip section headers
        if '===' in line:
            continue
        if len(line.strip()) > 300 and not line.startswith('-') and not line[0].isdigit():
            return False, "× ××¦××” ×¤×¡×§×” ××¨×•×›×” ××“×™ - ×™×© ×œ×”×©×ª××© ×‘×¨×©×™××•×ª"
    
    # Check for marketing language (basic detection)
    marketing_phrases = [
        "×”×˜×•×‘ ×‘×™×•×ª×¨",
        "×”×›×™ ×˜×•×‘",
        "××•××—×™× ××•×‘×™×œ×™×",
        "×©×™×¨×•×ª ×œ×œ× ×ª×—×¨×•×ª",
        "×”××•××—×™× ×©×œ× ×•"
    ]
    lower_prompt = prompt_text.lower()
    found_marketing = [phrase for phrase in marketing_phrases if phrase in lower_prompt]
    if found_marketing:
        return False, f"× ××¦× × ×™×¡×•×— ×©×™×•×•×§×™: {found_marketing[0]}"
    
    return True, ""


def _generate_with_openai(questionnaire: dict, provider_config: dict) -> dict:
    """Generate prompt using OpenAI with timeout and retry"""
    from openai import OpenAI
    
    api_key = provider_config.get('api_key') or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not configured")
    
    model = provider_config.get('model', 'gpt-4o-mini')
    
    client = OpenAI(api_key=api_key, timeout=OPENAI_TIMEOUT)
    
    # Build user prompt from questionnaire
    user_prompt = f"""×¦×•×¨ SYSTEM PROMPT ×œ×¡×•×›×Ÿ AI ×¢×œ ×‘×¡×™×¡ ×”××™×“×¢ ×”×‘×:

×©× ×”×¢×¡×§: {questionnaire.get('business_name', '×œ× ×¦×•×™×Ÿ')}
×¡×•×’ ×”×¢×¡×§: {questionnaire.get('business_type', '×œ× ×¦×•×™×Ÿ')}
×§×”×œ ×™×¢×“: {questionnaire.get('target_audience', '×œ×§×•×—×•×ª ×›×œ×œ×™×™×')}
××˜×¨×” ×¢×™×§×¨×™×ª: {questionnaire.get('main_goal', '×©×™×¨×•×ª ×œ×§×•×—×•×ª')}
××” ×–×” ×œ×™×“ ××™×›×•×ª×™: {questionnaire.get('what_is_quality_lead', '×œ×§×•×— ×©××‘×™×¢ ×¢× ×™×™×Ÿ')}
×©×™×¨×•×ª×™×: {', '.join(questionnaire.get('services', [])) or '×œ× ×¦×•×™×Ÿ'}
×©×¢×•×ª ×¤×¢×™×œ×•×ª: {questionnaire.get('working_hours', '09:00-18:00')}
×¡×’× ×•×Ÿ ×©×™×—×”: {questionnaire.get('conversation_style', '××§×¦×•×¢×™')}
×¤×¢×•×œ×•×ª ××¡×•×¨×•×ª: {', '.join(questionnaire.get('forbidden_actions', [])) or '×œ× ×¦×•×™×Ÿ'}
×—×•×§×™ ×”×¢×‘×¨×”: {questionnaire.get('handoff_rules', '×œ× ×¦×•×™×Ÿ')}
××™× ×˜×’×¨×¦×™×•×ª: {', '.join(questionnaire.get('integrations', [])) or '××™×Ÿ'}
"""
    
    # Retry logic
    for attempt in range(MAX_RETRIES):
        try:
            start_time = time.time()
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": GENERATOR_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2500,
                temperature=0.5
            )
            duration = time.time() - start_time
            logger.info(f"OpenAI prompt generation completed in {duration:.2f}s (attempt {attempt + 1})")
            
            return {
                "prompt_text": response.choices[0].message.content.strip(),
                "provider": "openai",
                "model": model
            }
        except Exception as e:
            is_last_attempt = (attempt == MAX_RETRIES - 1)
            if is_last_attempt:
                logger.error(f"OpenAI call failed after {MAX_RETRIES} attempts: {str(e)}")
                raise
            else:
                logger.warning(f"OpenAI call failed (attempt {attempt + 1}), retrying: {str(e)}")
                time.sleep(RETRY_DELAY)


def _generate_with_gemini(questionnaire: dict, provider_config: dict) -> dict:
    """Generate prompt using Google Gemini"""
    import google.generativeai as genai
    
    api_key = provider_config.get('api_key') or os.getenv("GEMINI_API_KEY")
    model_name = provider_config.get('model', 'gemini-pro')
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    
    # Build user prompt from questionnaire
    user_prompt = f"""×¦×•×¨ SYSTEM PROMPT ×œ×¡×•×›×Ÿ AI ×¢×œ ×‘×¡×™×¡ ×”××™×“×¢ ×”×‘×:

×©× ×”×¢×¡×§: {questionnaire.get('business_name', '×œ× ×¦×•×™×Ÿ')}
×¡×•×’ ×”×¢×¡×§: {questionnaire.get('business_type', '×œ× ×¦×•×™×Ÿ')}
×§×”×œ ×™×¢×“: {questionnaire.get('target_audience', '×œ×§×•×—×•×ª ×›×œ×œ×™×™×')}
××˜×¨×” ×¢×™×§×¨×™×ª: {questionnaire.get('main_goal', '×©×™×¨×•×ª ×œ×§×•×—×•×ª')}
××” ×–×” ×œ×™×“ ××™×›×•×ª×™: {questionnaire.get('what_is_quality_lead', '×œ×§×•×— ×©××‘×™×¢ ×¢× ×™×™×Ÿ')}
×©×™×¨×•×ª×™×: {', '.join(questionnaire.get('services', [])) or '×œ× ×¦×•×™×Ÿ'}
×©×¢×•×ª ×¤×¢×™×œ×•×ª: {questionnaire.get('working_hours', '09:00-18:00')}
×¡×’× ×•×Ÿ ×©×™×—×”: {questionnaire.get('conversation_style', '××§×¦×•×¢×™')}
×¤×¢×•×œ×•×ª ××¡×•×¨×•×ª: {', '.join(questionnaire.get('forbidden_actions', [])) or '×œ× ×¦×•×™×Ÿ'}
×—×•×§×™ ×”×¢×‘×¨×”: {questionnaire.get('handoff_rules', '×œ× ×¦×•×™×Ÿ')}
××™× ×˜×’×¨×¦×™×•×ª: {', '.join(questionnaire.get('integrations', [])) or '××™×Ÿ'}
"""
    
    # Combine system prompt and user prompt for Gemini
    full_prompt = f"{GENERATOR_SYSTEM_PROMPT}\n\n{user_prompt}"
    
    response = model.generate_content(full_prompt)
    
    return {
        "prompt_text": response.text.strip(),
        "provider": "gemini",
        "model": model_name
    }


@smart_prompt_bp.route('/api/ai/smart_prompt_generator/schema', methods=['GET'])
@csrf.exempt
@require_api_auth(['system_admin', 'owner', 'admin'])
def get_input_schema():
    """
    Get the structured input schema for smart prompt generation
    Returns questionnaire structure with field definitions
    """
    schema = {
        "fields": [
            {
                "id": "business_name",
                "label": "×©× ×”×¢×¡×§",
                "type": "text",
                "required": True,
                "placeholder": "×œ×“×•×’××”: ×§×œ×™× ×™×§×ª ××¡×ª×˜×™×§×” '×™×•×¤×™ ×˜×‘×¢×™'",
                "maxLength": 100
            },
            {
                "id": "business_type",
                "label": "×¡×•×’ ×”×¢×¡×§ / ×ª×—×•×",
                "type": "text",
                "required": True,
                "placeholder": "×œ×“×•×’××”: ×§×œ×™× ×™×§×ª ××¡×ª×˜×™×§×”, ××©×¨×“ ×¢×•\"×“, ××•×¡×š ×¨×›×‘",
                "maxLength": 100
            },
            {
                "id": "target_audience",
                "label": "×§×”×œ ×™×¢×“",
                "type": "text",
                "required": False,
                "placeholder": "×œ×“×•×’××”: × ×©×™× ×‘×’×™×œ××™ 25-50, ×‘×¢×œ×™ ×¨×›×‘×™ ×™×•×§×¨×”",
                "maxLength": 200
            },
            {
                "id": "main_goal",
                "label": "××˜×¨×” ×¢×™×§×¨×™×ª ×©×œ ×”×©×™×—×”",
                "type": "select",
                "required": True,
                "options": [
                    {"value": "×ª×™××•× ×¤×’×™×©×”", "label": "×ª×™××•× ×¤×’×™×©×” / ×§×‘×™×¢×ª ×ª×•×¨"},
                    {"value": "××›×™×¨×”", "label": "××›×™×¨×” / ×¡×’×™×¨×ª ×¢×¡×§×”"},
                    {"value": "××™×“×¢", "label": "××ª×Ÿ ××™×“×¢ / ×©×™×¨×•×ª ×œ×§×•×—×•×ª"},
                    {"value": "×¡×™× ×•×Ÿ ×œ×™×“×™×", "label": "×¡×™× ×•×Ÿ ×•×¡×™×•×•×’ ×œ×™×“×™×"},
                    {"value": "×ª××™×›×”", "label": "×ª××™×›×” ×˜×›× ×™×ª / ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª"}
                ]
            },
            {
                "id": "what_is_quality_lead",
                "label": "××” × ×—×©×‘ ×œ×™×“ ××™×›×•×ª×™?",
                "type": "textarea",
                "required": False,
                "placeholder": "×œ×“×•×’××”: ×œ×§×•×— ×©×¨×•×¦×” ×œ×”×–××™×Ÿ ×˜×™×¤×•×œ ×‘×©×‘×•×¢×™×™× ×”×§×¨×•×‘×™×, ×‘×ª×§×¦×™×‘ ×©×œ ××¢×œ 1000 ×©\"×—",
                "maxLength": 300
            },
            {
                "id": "services",
                "label": "×©×™×¨×•×ª×™× / ××•×¦×¨×™× ×¢×™×§×¨×™×™×",
                "type": "tags",
                "required": False,
                "placeholder": "×”×§×œ×“ ×©×™×¨×•×ª ×•×”×§×© Enter",
                "maxItems": 10
            },
            {
                "id": "working_hours",
                "label": "×©×¢×•×ª ×¤×¢×™×œ×•×ª",
                "type": "text",
                "required": False,
                "placeholder": "×œ×“×•×’××”: ×-×” 09:00-18:00, ×• 09:00-13:00",
                "maxLength": 100
            },
            {
                "id": "conversation_style",
                "label": "×¡×’× ×•×Ÿ ×©×™×—×”",
                "type": "select",
                "required": True,
                "options": [
                    {"value": "×¨×’×•×¢", "label": "×¨×’×•×¢ ×•××“×™×‘"},
                    {"value": "××§×¦×•×¢×™", "label": "××§×¦×•×¢×™ ×•×™×©×™×¨"},
                    {"value": "××›×™×¨×ª×™", "label": "×—× ×•××›×™×¨×ª×™"},
                    {"value": "×¤×•×¨××œ×™", "label": "×¤×•×¨××œ×™ ×•×¨×¦×™× ×™"}
                ]
            },
            {
                "id": "forbidden_actions",
                "label": "×¤×¢×•×œ×•×ª ××¡×•×¨×•×ª",
                "type": "tags",
                "required": False,
                "placeholder": "×œ×“×•×’××”: ×”×‘×˜×—×ª ××—×™×¨×™×, ×”×ª×—×™×™×‘×•×ª ×œ×–×× ×™×",
                "maxItems": 10
            },
            {
                "id": "handoff_rules",
                "label": "××ª×™ ×œ×”×¢×‘×™×¨ ×œ× ×¦×™×’ ×× ×•×©×™?",
                "type": "textarea",
                "required": False,
                "placeholder": "×œ×“×•×’××”: ×ª×œ×•× ×•×ª ×¨×¦×™× ×™×•×ª, ×‘×§×©×•×ª ××•×¨×›×‘×•×ª, ×œ×§×•×— ×¢×¦×‘× ×™",
                "maxLength": 300
            },
            {
                "id": "integrations",
                "label": "××™× ×˜×’×¨×¦×™×•×ª / ××¢×¨×›×•×ª ×§×™×™××•×ª",
                "type": "tags",
                "required": False,
                "placeholder": "×œ×“×•×’××”: Google Calendar, CRM, ××¢×¨×›×ª ×ª×•×¨×™×",
                "maxItems": 5
            }
        ]
    }
    
    return jsonify(schema)


@smart_prompt_bp.route('/api/ai/smart_prompt_generator/generate', methods=['POST'])
@csrf.exempt
@require_api_auth(['system_admin', 'owner', 'admin'])
def generate_smart_prompt():
    """
    Generate a structured system prompt from questionnaire
    
    âœ… ALWAYS uses OpenAI (not related to business ai_provider)
    âœ… ALWAYS returns a prompt (best-effort) - never fails on quality
    
    Input: Structured questionnaire object
    Output: Structured prompt following rigid template + quality info (not blocking)
    """
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "× ×“×¨×©×™× × ×ª×•× ×™×"}), 400
        
        questionnaire = data.get('questionnaire', {})
        # REMOVED: provider selection - always use OpenAI
        # REMOVED: provider_config - not needed, use env var
        
        # Validate required fields
        if not questionnaire.get('business_name'):
            return jsonify({"error": "×©× ×”×¢×¡×§ ×”×•× ×©×“×” ×—×•×‘×”"}), 400
        if not questionnaire.get('business_type'):
            return jsonify({"error": "×¡×•×’ ×”×¢×¡×§ ×”×•× ×©×“×” ×—×•×‘×”"}), 400
        if not questionnaire.get('main_goal'):
            return jsonify({"error": "××˜×¨×” ×¢×™×§×¨×™×ª ×”×™× ×©×“×” ×—×•×‘×”"}), 400
        if not questionnaire.get('conversation_style'):
            return jsonify({"error": "×¡×’× ×•×Ÿ ×©×™×—×” ×”×•× ×©×“×” ×—×•×‘×”"}), 400
        
        # Check if OpenAI API key is available
        if not os.getenv("OPENAI_API_KEY"):
            logger.error("OPENAI_API_KEY not configured for smart prompt generator")
            return jsonify({
                "error": "××—×•×œ×œ ×”×¤×¨×•××¤×˜×™× ×”×–××™×Ÿ ×“×•×¨×© ×”×’×“×¨×ª OpenAI API Key",
                "details": "OPENAI_API_KEY environment variable is not set"
            }), 503
        
        # Sanitize all text inputs
        sanitized = {}
        for key, value in questionnaire.items():
            if isinstance(value, str):
                sanitized[key] = _sanitize_input(value)
            elif isinstance(value, list):
                # For tags/arrays, sanitize each item
                sanitized[key] = [_sanitize_input(str(item), 100) for item in value]
            else:
                sanitized[key] = value
        
        # Check total input size
        total_size = sum(len(str(v)) for v in sanitized.values())
        if total_size > MAX_TOTAL_INPUT:
            return jsonify({"error": f"×¡×š ×”×§×œ×˜ ××¨×•×š ××“×™ (××§×¡×™××•× {MAX_TOTAL_INPUT} ×ª×•×•×™×)"}), 400
        
        # Generate prompt - ALWAYS with OpenAI
        logger.info(f"Generating smart prompt with openai for business: {sanitized.get('business_name')}")
        
        try:
            # ALWAYS use OpenAI - no provider selection
            result = _generate_with_openai(sanitized, {})
            
            prompt_text = result['prompt_text']
            
            # Quality Check - BUT DON'T FAIL, only warn
            is_valid, validation_error = _validate_prompt_structure(prompt_text)
            
            if not is_valid:
                # Log as warning, not error - still return the prompt
                logger.warning(f"Generated prompt has quality issues (returning anyway): {validation_error}")
            
            logger.info(f"Smart prompt generated successfully ({len(prompt_text)} chars) using {result['provider']}")
            
            # ALWAYS return 200 with the prompt
            response_data = {
                "success": True,
                "prompt_text": prompt_text,
                "provider": result['provider'],
                "model": result['model'],
                "length": len(prompt_text),
                "validation": {
                    "passed": is_valid,
                    "sections_found": REQUIRED_SECTIONS
                }
            }
            
            # Add quality warning if validation failed (but still return prompt)
            if not is_valid:
                response_data["quality_warning"] = validation_error
                response_data["note"] = "×”×¤×¨×•××¤×˜ × ×•×¦×¨ ×‘×”×¦×œ×—×” - ×™×™×ª×›× ×• ×©×™×¤×•×¨×™× ××¤×©×¨×™×™×"
            
            return jsonify(response_data), 200
            
        except ValueError as ve:
            # This catches the "OPENAI_API_KEY not configured" error
            logger.exception(f"Configuration error in smart prompt generator")
            return jsonify({
                "error": "×©×’×™××ª ×”×’×“×¨×”",
                "details": str(ve)
            }), 503
        except Exception as gen_error:
            logger.exception(f"Error generating prompt with OpenAI")
            return jsonify({"error": f"×©×’×™××” ×‘×™×¦×™×¨×ª ×”×¤×¨×•××¤×˜"}), 500
        
    except Exception as e:
        logger.exception("Error in smart prompt generator")
        return jsonify({"error": "×©×’×™××” ×›×œ×œ×™×ª ×‘×™×¦×™×¨×ª ×”×¤×¨×•××¤×˜"}), 500


@smart_prompt_bp.route('/api/ai/smart_prompt_generator/save', methods=['POST'])
@csrf.exempt
@require_api_auth(['system_admin', 'owner', 'admin'])
def save_smart_prompt():
    """
    Save a generated smart prompt to business
    Stores as new prompt version with metadata
    """
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "× ×“×¨×©×™× × ×ª×•× ×™×"}), 400
        
        prompt_text = data.get('prompt_text', '').strip()
        channel = data.get('channel', 'calls')  # 'calls' or 'whatsapp'
        metadata = data.get('metadata', {})
        
        if not prompt_text:
            return jsonify({"error": "×˜×§×¡×˜ ×”×¤×¨×•××¤×˜ ×—×¡×¨"}), 400
        
        # Final validation before save
        is_valid, validation_error = _validate_prompt_structure(prompt_text)
        if not is_valid:
            return jsonify({
                "error": "×”×¤×¨×•××¤×˜ ×œ× ×ª×§×™×Ÿ",
                "validation_error": validation_error
            }), 400
        
        # Get business ID
        business_id = _get_business_id()
        if not business_id:
            return jsonify({"error": "×œ× × ××¦× ×¢×¡×§"}), 400
        
        business = Business.query.filter_by(id=business_id).first()
        if not business:
            return jsonify({"error": "×¢×¡×§ ×œ× × ××¦×"}), 404
        
        # Get current user
        current_user = session.get('user', {})
        user_id = current_user.get('email', 'smart_generator')
        
        # Get or create business settings
        settings = BusinessSettings.query.filter_by(tenant_id=business_id).first()
        
        # Build prompt object
        current_prompts = {}
        if settings and settings.ai_prompt:
            try:
                if settings.ai_prompt.startswith('{'):
                    current_prompts = json.loads(settings.ai_prompt)
                else:
                    current_prompts = {
                        'calls': settings.ai_prompt,
                        'whatsapp': settings.ai_prompt
                    }
            except (json.JSONDecodeError, TypeError, ValueError):
                current_prompts = {}
        
        # Update the specified channel
        current_prompts[channel] = prompt_text
        
        # Store as JSON
        new_prompt_data = json.dumps(current_prompts, ensure_ascii=False)
        
        if not settings:
            settings = BusinessSettings()
            settings.tenant_id = business_id
            settings.ai_prompt = new_prompt_data
            settings.updated_by = user_id
            db.session.add(settings)
        else:
            settings.ai_prompt = new_prompt_data
            settings.updated_by = user_id
            settings.updated_at = datetime.utcnow()
        
        # Get next version
        latest_revision = PromptRevisions.query.filter_by(
            tenant_id=business_id
        ).order_by(PromptRevisions.version.desc()).first()
        
        next_version = (latest_revision.version + 1) if latest_revision else 1
        
        # Create revision with metadata
        revision = PromptRevisions()
        revision.tenant_id = business_id
        revision.version = next_version
        revision.prompt = new_prompt_data
        revision.changed_by = f"{user_id} (Smart Generator v2)"
        revision.changed_at = datetime.utcnow()
        db.session.add(revision)
        
        db.session.commit()
        
        # Invalidate cache
        try:
            from server.services.ai_service import invalidate_business_cache
            invalidate_business_cache(business_id)
            logger.info(f"Cache invalidated for business {business_id} after smart prompt save")
        except Exception as e:
            logger.warning(f"Could not invalidate cache: {e}")
        
        logger.info(f"Smart prompt saved: business={business_id}, channel={channel}, version={next_version}, provider={metadata.get('provider', 'unknown')}")
        
        return jsonify({
            "success": True,
            "version": next_version,
            "channel": channel,
            "message": f"×¤×¨×•××¤×˜ ×—×›× × ×©××¨ ×‘×”×¦×œ×—×” (×’×¨×¡×” {next_version})",
            "metadata": {
                "generator": "smart_v2",
                "provider": metadata.get('provider'),
                "saved_at": datetime.utcnow().isoformat()
            }
        })
        
    except Exception as e:
        db.session.rollback()
        logger.exception("Error saving smart prompt")
        return jsonify({"error": "×©×’×™××” ×‘×©××™×¨×ª ×”×¤×¨×•××¤×˜"}), 500


@smart_prompt_bp.route('/api/ai/smart_prompt_generator/providers', methods=['GET'])
@csrf.exempt
@require_api_auth(['system_admin', 'owner', 'admin'])
def get_available_providers():
    """
    Get list of available AI providers for prompt generation
    
    âœ… NOTE: Smart Prompt Generator ALWAYS uses OpenAI only
    This endpoint is kept for UI compatibility but returns only OpenAI
    """
    # Only return OpenAI - Gemini is not used for smart prompt generation
    providers = [
        {
            "id": "openai",
            "name": "OpenAI",
            "default": True,
            "models": ["gpt-4o-mini", "gpt-4o"],
            "description": "××¢×¨×›×ª ×‘×¨×™×¨×ª ×”××—×“×œ - ×××™× ×” ×•××”×™×¨×”",
            "available": bool(os.getenv("OPENAI_API_KEY")),
            "note": "××—×•×œ×œ ×”×¤×¨×•××¤×˜×™× ×”×—×›× ××©×ª××© ×¨×§ ×‘-OpenAI"
        }
    ]
    
    return jsonify({
        "providers": providers,
        "default_provider": "openai",
        "note": "Smart Prompt Generator uses OpenAI exclusively"
    })

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ NEW: Status Change Prompt Management
# Allows businesses to customize how AI changes lead statuses
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@smart_prompt_bp.route('/api/ai/status_change_prompt/get', methods=['GET'])
@csrf.exempt
@require_api_auth(['system_admin', 'owner', 'admin'])
def get_status_change_prompt():
    """
    Get current status change prompt for business
    Returns custom prompt or default template with consistent structure
    
    âœ… FIX: Always returns stable default (never 404/null)
    âœ… FIX: Consistent JSON format for all responses
    """
    try:
        business_id = _get_business_id()
        if not business_id:
            return jsonify({
                "ok": False,
                "error": "BUSINESS_CONTEXT_REQUIRED",
                "details": "×œ× × ××¦× ×¢×¡×§"
            }), 400
        
        logger.info(f"[GET_STATUS_PROMPT] business_id={business_id}")
        
        # Get latest revision
        latest_revision = PromptRevisions.query.filter_by(
            tenant_id=business_id
        ).order_by(PromptRevisions.version.desc()).first()
        
        if latest_revision and latest_revision.status_change_prompt:
            # Return custom prompt
            result = {
                "ok": True,
                "business_id": business_id,
                "prompt": latest_revision.status_change_prompt,
                "version": latest_revision.version,
                "exists": True,
                "has_custom_prompt": True,
                "updated_at": latest_revision.changed_at.isoformat() if latest_revision.changed_at else None
            }
            logger.info(f"[GET_STATUS_PROMPT] Returning custom prompt, version={latest_revision.version}")
            return jsonify(result)
        
        # Return default template - STABLE DEFAULT
        default_prompt = """ğŸ¯ ×”× ×—×™×•×ª ×œ×©×™× ×•×™ ×¡×˜×˜×•×¡ ××•×˜×•××˜×™ ×©×œ ×œ×™×“×™×
==========================================

**×¢×§×¨×•× ×•×ª ×›×œ×œ×™×™×:**
- ×©× ×” ×¡×˜×˜×•×¡ ×¨×§ ×›××©×¨ ×™×© ××™× ×“×™×§×¦×™×” ×‘×¨×•×¨×” ××”×œ×§×•×—
- ×ª×¢×“×›×Ÿ ××ª ×”×¡×˜×˜×•×¡ ×‘×–××Ÿ ×××ª ×‘××”×œ×š ×”×©×™×—×”/×¦'××˜
- ×ª××™×“ ×¡×¤×§ ×¡×™×‘×” ×‘×¨×•×¨×” ×œ×©×™× ×•×™ ×”×¡×˜×˜×•×¡

**××ª×™ ×œ×¢×“×›×Ÿ ×¡×˜×˜×•×¡×™× (×“×•×’×××•×ª ×œ×¤×™ ×¡×˜×˜×•×¡×™× ×¨×œ×•×•× ×˜×™×™×):**

ğŸ“Œ ××¢×•× ×™×™×Ÿ (interested):
- ×œ×§×•×— ×©×•××œ ×©××œ×•×ª ×¢×œ ×”×©×™×¨×•×ª/××•×¦×¨
- ×œ×§×•×— ××‘×§×© ×¤×¨×˜×™× × ×•×¡×¤×™×
- ×œ×§×•×— ××¨××” ×¢× ×™×™×Ÿ ××§×˜×™×‘×™
×“×•×’××”: "×œ×§×•×— ×©××œ ×¢×œ ××—×™×¨×™× ×•×©×™×¨×•×ª×™× - ××¨××” ×¢× ×™×™×Ÿ ××§×˜×™×‘×™"

ğŸ“Œ × ×§×‘×¢×” ×¤×’×™×©×” (appointment_scheduled):
- ×œ×§×•×— ××™×©×¨ ×¤×’×™×©×” ×‘×ª××¨×™×š ×•×©×¢×” ×¡×¤×¦×™×¤×™×™×
- × ×§×‘×¢×” ×¤×’×™×©×” ×“×¨×š ×”×¡×•×›×Ÿ
×“×•×’××”: "× ×§×‘×¢×” ×¤×’×™×©×” ×œ×™×•× ×¨××©×•×Ÿ 10:00"

ğŸ“Œ ××—×›×” ×œ×—×–×¨×” (callback_requested):
- ×œ×§×•×— ×‘×™×§×© ×©× ×—×–×•×¨ ××œ×™×• ×‘××•×¢×“ ××¡×•×™×
- ×œ×§×•×— ×¢×¡×•×§ ×›×¨×’×¢ ×•××‘×§×© ×œ×™×¦×•×¨ ×§×©×¨ ×××•×—×¨ ×™×•×ª×¨
×“×•×’××”: "×œ×§×•×— ×‘×™×§×© ×©× ×—×–×•×¨ ××œ×™×• ××—×¨ ××—×”×´×¦"

ğŸ“Œ × ×©×œ×—×” ×”×¦×¢×” (proposal_sent):
- × ×©×œ×—×” ×”×¦×¢×ª ××—×™×¨ ×œ×œ×§×•×—
- ×œ×§×•×— ×‘×™×§×© ×”×¦×¢×ª ××—×™×¨ ×‘×›×ª×‘
×“×•×’××”: "× ×©×œ×—×” ×”×¦×¢×ª ××—×™×¨ ×œ××™×™×œ ×”×œ×§×•×—"

ğŸ“Œ ×œ× ×¨×œ×•×•× ×˜×™ (not_relevant):
- ×œ×§×•×— ×××¨ ×‘××¤×•×¨×© ×©×”×•× ×œ× ××¢×•× ×™×™×Ÿ
- ×˜×¢×™× ×• ×‘××¡×¤×¨ / ×œ×§×•×— ×œ× ×‘×§×‘×•×¦×ª ×”×™×¢×“
×“×•×’××”: "×œ×§×•×— ×××¨ ×©×˜×¢×™× ×• ×‘××¡×¤×¨ ×•××™× ×• ××¢×•× ×™×™×Ÿ"

**××’×‘×œ×•×ª ×—×©×•×‘×•×ª:**
âŒ ××œ ×ª×©× ×” ×¡×˜×˜×•×¡ ××:
- ×”×œ×§×•×— ×¨×§ ×¢× ×” ×œ×©×™×—×” (×–×” ×œ× ×¡×™×‘×” ×œ×©×™× ×•×™)
- ×©××œ ×©××œ×” ×›×œ×œ×™×ª ×××•×“
- ×××¨ "×× ×™ ××—×©×•×‘ ×¢×œ ×–×”" (×–×” ×œ× ×”×—×œ×˜×”)
- ×œ× ×‘×¨×•×¨ ××” ×”×›×•×•× ×” ×©×œ×•

**×¨××ª ×‘×™×˜×—×•×Ÿ (confidence):**
- 1.0 = ×œ×§×•×— ×××¨ ××©×”×• ××¤×•×¨×© ("×›×Ÿ ×× ×™ ××ª×¢× ×™×™×Ÿ", "×§×‘×¢ ×œ×™ ×¤×’×™×©×”")
- 0.8-0.9 = ×‘×¨×•×¨ ××”×”×§×©×¨ ××‘×œ ×œ× ××¤×•×¨×© ("× ×©××¢ ×˜×•×‘", "×›××” ×–×” ×¢×•×œ×”?")
- 0.7 = ×™×© ×¨××– ××‘×œ ×œ× ×‘×˜×•×—
- ×¤×—×•×ª ×-0.7 = ××œ ×ª×¢×“×›×Ÿ!

ğŸ’¡ **×”×¢×™×§×¨×•×Ÿ: ×ª×”×™×” ×©××¨×Ÿ! ×¢×“×›×Ÿ ×¨×§ ×›×©×‘×˜×•×— ×©×¦×¨×™×š!**"""
        
        result = {
            "ok": True,
            "business_id": business_id,
            "prompt": default_prompt,
            "version": 0,
            "exists": False,
            "has_custom_prompt": False,
            "updated_at": None,
            "note": "×–×”×• ×ª×‘× ×™×ª ×‘×¨×™×¨×ª ××—×“×œ. × ×™×ª×Ÿ ×œ×”×ª××™× ××™×©×™×ª ×œ×¤×™ ×¦×¨×›×™ ×”×¢×¡×§."
        }
        logger.info(f"[GET_STATUS_PROMPT] Returning default prompt")
        return jsonify(result)
        
    except Exception as e:
        logger.exception(f"[GET_STATUS_PROMPT] Error: {e}")
        return jsonify({
            "ok": False,
            "error": "PROMPT_LOAD_FAILED",
            "details": str(e)
        }), 500


@smart_prompt_bp.route('/api/ai/status_change_prompt/save', methods=['POST'])
@csrf.exempt
@require_api_auth(['system_admin', 'owner', 'admin'])
def save_status_change_prompt():
    """
    Save custom status change prompt for business
    Creates new prompt revision with status_change_prompt
    
    âœ… FIX: Returns full updated prompt object (not just {ok:true})
    âœ… FIX: Optimistic locking with version conflict handling
    âœ… FIX: Read-through cache pattern (set after invalidate)
    """
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                "ok": False,
                "error": "MISSING_DATA",
                "details": "× ×“×¨×©×™× × ×ª×•× ×™×"
            }), 400
        
        prompt_text = data.get('prompt_text', '').strip()
        client_version = data.get('version')  # Optional: for optimistic locking
        
        if not prompt_text:
            return jsonify({
                "ok": False,
                "error": "EMPTY_PROMPT",
                "details": "×˜×§×¡×˜ ×”×¤×¨×•××¤×˜ ×—×¡×¨"
            }), 400
        
        if len(prompt_text) > 5000:
            return jsonify({
                "ok": False,
                "error": "PROMPT_TOO_LONG",
                "details": "×”×¤×¨×•××¤×˜ ××¨×•×š ××“×™ (××§×¡×™××•× 5000 ×ª×•×•×™×)"
            }), 400
        
        # Get business ID
        business_id = _get_business_id()
        if not business_id:
            return jsonify({
                "ok": False,
                "error": "BUSINESS_CONTEXT_REQUIRED",
                "details": "×œ× × ××¦× ×¢×¡×§"
            }), 400
        
        business = Business.query.filter_by(id=business_id).first()
        if not business:
            return jsonify({
                "ok": False,
                "error": "BUSINESS_NOT_FOUND",
                "details": "×¢×¡×§ ×œ× × ××¦×"
            }), 404
        
        logger.info(f"[SAVE_STATUS_PROMPT] business_id={business_id}, client_version={client_version}, prompt_length={len(prompt_text)}")
        
        # Get current user
        current_user = session.get('user', {})
        user_id = current_user.get('email', 'system')
        
        # Get latest revision to preserve other fields
        latest_revision = PromptRevisions.query.filter_by(
            tenant_id=business_id
        ).order_by(PromptRevisions.version.desc()).first()
        
        current_version = latest_revision.version if latest_revision else 0
        
        # âœ… OPTIMISTIC LOCKING: Check version conflict
        if client_version is not None and current_version != client_version:
            logger.warning(f"[SAVE_STATUS_PROMPT] Version conflict: client={client_version}, server={current_version}")
            # Return 409 Conflict with latest data
            return jsonify({
                "ok": False,
                "error": "VERSION_CONFLICT",
                "details": "××™×©×”×• ×©××¨ ×©×™× ×•×™×™× ×œ×¤× ×™ ×©× ×™×™×”. ×”×¤×¨×•××¤×˜ ×¢×•×“×›×Ÿ ×œ×’×¨×¡×” ×”×—×“×©×”.",
                "latest_version": current_version,
                "latest_prompt": latest_revision.status_change_prompt if latest_revision else "",
                "updated_at": latest_revision.changed_at.isoformat() if latest_revision and latest_revision.changed_at else None
            }), 409
        
        next_version = current_version + 1
        
        # Create new revision
        revision = PromptRevisions()
        revision.tenant_id = business_id
        revision.version = next_version
        revision.status_change_prompt = prompt_text
        
        # Preserve existing prompts from latest revision
        if latest_revision:
            revision.prompt = latest_revision.prompt
            revision.whatsapp_system_prompt = latest_revision.whatsapp_system_prompt
        
        revision.changed_by = f"{user_id} (Status Prompt Editor)"
        revision.changed_at = datetime.utcnow()
        
        # Commit to database
        db.session.add(revision)
        db.session.commit()
        
        logger.info(f"[SAVE_STATUS_PROMPT] Committed version={next_version}")
        
        # âœ… SELECT the saved record to ensure consistency
        saved_revision = PromptRevisions.query.filter_by(
            tenant_id=business_id,
            version=next_version
        ).first()
        
        if not saved_revision:
            logger.error(f"[SAVE_STATUS_PROMPT] Failed to retrieve saved revision!")
            return jsonify({
                "ok": False,
                "error": "SAVE_VERIFICATION_FAILED",
                "details": "×”×©××™×¨×” × ×›×©×œ×” - ×œ× × ×™×ª×Ÿ ×œ×××ª ××ª ×”×©×™× ×•×™×™×"
            }), 500
        
        # âœ… CACHE: Invalidate + Set (read-through pattern)
        try:
            from server.services.ai_service import invalidate_business_cache
            from server.services.prompt_cache import get_prompt_cache
            
            # Invalidate old cache
            invalidate_business_cache(business_id)
            
            # Note: PromptCache is for conversation prompts, not status prompts
            # Status prompts are loaded directly from DB by agent_factory
            # But we still invalidate to ensure fresh load
            
            logger.info(f"[SAVE_STATUS_PROMPT] Cache invalidated for business {business_id}")
        except Exception as e:
            logger.warning(f"[SAVE_STATUS_PROMPT] Could not invalidate cache: {e}")
        
        # âœ… RETURN FULL UPDATED OBJECT (not just {ok:true})
        response = {
            "ok": True,
            "business_id": business_id,
            "version": saved_revision.version,
            "prompt": saved_revision.status_change_prompt,
            "updated_at": saved_revision.changed_at.isoformat() if saved_revision.changed_at else None,
            "message": f"×¤×¨×•××¤×˜ ×¡×˜×˜×•×¡×™× × ×©××¨ ×‘×”×¦×œ×—×” (×’×¨×¡×” {saved_revision.version})"
        }
        
        logger.info(f"[SAVE_STATUS_PROMPT] SUCCESS: version={saved_revision.version}")
        return jsonify(response), 200
        
    except Exception as e:
        db.session.rollback()
        logger.exception(f"[SAVE_STATUS_PROMPT] Error: {e}")
        return jsonify({
            "ok": False,
            "error": "SAVE_FAILED",
            "details": "×©×’×™××” ×‘×©××™×¨×ª ×”×¤×¨×•××¤×˜"
        }), 500