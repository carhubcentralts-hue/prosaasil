"""
AI Service - Unified OpenAI Service for All Communication Channels
×©×™×¨×•×ª AI ×××•×—×“ - ××—×‘×¨ ×¤×¨×•××¤×˜×™× ×“×™× ××™×™× ××”××¡×“ × ×ª×•× ×™× ×¢× OpenAI
âœ¨ BUILD 119: AgentKit integration for real actions (appointments, leads, WhatsApp)
ğŸš€ Phase 2K: Fast Intent Router - run AgentKit only for bookings (â‰¤2s target)
"""
import os
import logging
import time
import re
from typing import Dict, Any, Optional, List, Literal
from openai import OpenAI
from server.models_sql import BusinessSettings, PromptRevisions, Business, AgentTrace
from server.db import db
from datetime import datetime

# ğŸ”¥ CRITICAL: Import agent modules at TOP of file (not inside function!)
# This prevents re-importing on every call and speeds up response time
try:
    from server.agent_tools import get_agent, AGENTS_ENABLED
    from agents import Runner
    AGENT_MODULES_LOADED = True
    logger_temp = logging.getLogger(__name__)
    logger_temp.info("âœ… Agent modules pre-loaded at module level")
except ImportError as e:
    AGENT_MODULES_LOADED = False
    AGENTS_ENABLED = False
    logger_temp = logging.getLogger(__name__)
    logger_temp.warning(f"âš ï¸ Agent modules not available: {e}")

logger = logging.getLogger(__name__)

# Global AI service instance for cache sharing
_global_ai_service = None

# ğŸš€ Phase 2K: Intent Router Configuration
# âœ… ENABLED: FAQ fast-path with improved context and token limits
AGENTKIT_BOOKING_ONLY = os.getenv("AGENTKIT_BOOKING_ONLY", "1") == "1"  # Default ON
FAST_PATH_ENABLED = os.getenv("FAST_PATH_ENABLED", "1") == "1"  # Default ON

def route_intent_hebrew(text: str) -> Literal["book", "reschedule", "cancel", "info", "whatsapp", "human", "other"]:
    """
    ğŸš€ Fast Hebrew intent detection - NO LLM!
    Returns intent category for routing decisions.
    Target: <10ms for classification
    
    Priority order: reschedule > cancel > whatsapp > human > info > book > other
    """
    text_lower = text.lower().strip()
    
    # ğŸ”„ RESCHEDULE: Change appointment (CHECK FIRST - more specific)
    reschedule_patterns = [
        r'×œ×”×–×™×–|×œ×”×§×“×™×|×œ×“×—×•×ª|×œ×”×—×œ×™×£.*×©×¢×”|×œ×©× ×•×ª.*×ª×•×¨',
        r'××¤×©×¨.*×œ×©× ×•×ª|××¤×©×¨.*×œ×”×–×™×–'
    ]
    
    # âŒ CANCEL: Cancel appointment (CHECK SECOND - specific action)
    cancel_patterns = [
        r'×œ×‘×˜×œ|×ª×‘×˜×œ|×‘×™×˜×•×œ.*×ª×•×¨|×œ×.*××’×™×¢',
        r'×× ×™.*×œ×.*×™×›×•×œ|××™×Ÿ.*××¤×©×¨×•×ª'
    ]
    
    # ğŸ“± WHATSAPP: Send info via WhatsApp (CHECK THIRD - clear intent)
    whatsapp_patterns = [
        r'×©×œ×—.*×œ×™|×ª×©×œ×—.*×œ×™',
        r'×•×•××˜×¡××¤|whatsapp',
        r'×”×•×“×¢×”|××¡×¨×•×Ÿ'
    ]
    
    # ğŸ‘¤ HUMAN: Transfer to agent (CHECK FOURTH - escalation)
    human_patterns = [
        r'× ×¦×™×’|×‘×Ÿ.*××“×|××™×©.*×××™×ª×™',
        r'×œ×“×‘×¨.*×¢×|×œ×”×¢×‘×™×¨'
    ]
    
    # â„¹ï¸ INFO: General information (CHECK AFTER booking pre-check!)
    # ğŸ”¥ TIGHTENED: These patterns now only match if NO booking verbs present
    info_patterns = [
        # ğŸ”¥ CRITICAL: Question words â†’ info (××œ×” ×©××œ×•×ª ××™×“×¢!)
        # But: "××ª×™ ××¤×©×¨ ×œ×§×‘×•×¢?" â†’ book (caught by pre-check)
        r'^(××”|××™×–×”|××™×–×•|×›××”|×œ××”|××“×•×¢|××™×š|×”×™×›×Ÿ|××ª×™)\s',  # Start with question word
        
        # ğŸ”¥ CRITICAL FIX: "×™×©..." questions - ONLY amenities (not rooms/services)
        r'×™×©\s+(××•×›×œ|×©×ª×™×™?×”|×ª×¤×¨×™×˜|×× ×•×ª|××œ×›×•×”×•×œ|×‘×¨|××©×§××•×ª|×§×¤×”|××–×•×Ÿ)',
        r'×™×©\s+(×—× ×™×”|×—× ×™×™×”|×’×™×©×”|××™×–×•×’|wifi|××™× ×˜×¨× ×˜|××¢×œ×™×ª)',
        r'×™×©\s+×œ×›×\s+(××•×›×œ|×©×ª×™×™?×”|×ª×¤×¨×™×˜|×—× ×™×”|wifi)',
        r'××”\s+×™×©\s+(×œ××›×•×œ|×œ×©×ª×•×ª|×‘×ª×¤×¨×™×˜)',
        
        # Pricing (standalone - not with booking verbs)
        r'×›××”.*×¢×•×œ×”|××—×™×¨(?!.*×œ×§×‘×•×¢)|×¢×œ×•×ª|×ª×©×œ×•×(?!.*×ª×•×¨)',
        
        # Location
        r'××™×¤×”|××™×§×•×|×›×ª×•×‘×ª|×”×™×›×Ÿ',
        
        # Hours
        r'×©×¢×•×ª.*×¤×ª×™×—×”|××ª×™.*×¤×ª×•×—|×©×¢×•×ª.*×¢×‘×•×“×”|××”.*×©×¢×•×ª',
        
        # Amenities & Services - REMOVED generic "×—×“×¨" patterns!
        r'×›×©×¨|×›×©×¨×•×ª',
        r'×’×•×“×œ.*×—×“×¨|×›××”.*×× ×©×™×|×›××”.*××©×ª×ª×¤×™×',
        r'××”.*×”×›×ª×•×‘×ª|××”.*×”××™×§×•×',
        
        # Menu/food (standalone)
        r'\b(×ª×¤×¨×™×˜|×× ×•×ª|××©×§××•×ª)\b',
    ]
    
    # ğŸ“… BOOK: Scheduling keywords (CHECK LAST - most generic)
    # ğŸ”¥ FIX: Require scheduling VERB + time/day to avoid false positives
    book_patterns = [
        r'×œ×§×‘×•×¢|×ª×™××•×|×œ×”×–××™×Ÿ|×¨×•×¦×”.*×ª×•×¨|××¤×©×¨.*×ª×•×¨',  # Explicit booking verbs
        r'×™×©.*××§×•×|×™×©.*×–××Ÿ|×™×©.*×¤× ×•×™|×¤× ×•×™.*×œ',  # Availability questions
        r'(×œ×‘×•×|×œ×”×’×™×¢).*(××—×¨|×”×™×•×|×‘-\d+|×‘×©×¢×”)',  # "×œ×‘×•× ××—×¨"
        r'(×¨×•×¦×”|×¦×¨×™×š).*(×ª×•×¨|×¤×’×™×©×”|×ª×™××•×)',  # "×¨×•×¦×” ×ª×•×¨"
    ]
    
    # ğŸ”¥ FIX: Check patterns in CORRECT priority order
    # Most specific first, most generic last
    
    for pattern in reschedule_patterns:
        if re.search(pattern, text_lower):
            return "reschedule"
    
    for pattern in cancel_patterns:
        if re.search(pattern, text_lower):
            return "cancel"
    
    for pattern in whatsapp_patterns:
        if re.search(pattern, text_lower):
            return "whatsapp"
    
    for pattern in human_patterns:
        if re.search(pattern, text_lower):
            return "human"
    
    # ğŸš¨ CRITICAL PRE-CHECK: Booking verbs + time/day â†’ BOOK (before info check!)
    # This fixes: "××¤×©×¨ ×œ×§×‘×•×¢ ×—×“×¨ ×§×¨×™×•×§×™ ×œ××—×¨" â†’ book (not info)
    booking_verbs = r'(×œ×§×‘×•×¢|×œ×ª××|×œ×”×–××™×Ÿ|××¤×©×¨.*×ª×•×¨|×¨×•×¦×”.*×ª×•×¨|×¦×¨×™×š.*×ª×•×¨)'
    time_day_terms = r'(××—×¨|×”×™×•×|××—×¨×ª×™×™×|×”×©×‘×•×¢|×”×—×•×“×©|×‘-\d+|×‘×©×¢×”|×‘×™×•×|×‘×©× ×™|×‘×©×œ×™×©×™|×‘×¨×‘×™×¢×™|×‘×—××™×©×™|×‘×©×™×©×™|×‘×©×‘×ª|×‘×¨××©×•×Ÿ)'
    availability_terms = r'(×¤× ×•×™|×–××™×Ÿ|×–××Ÿ|××§×•×|×ª×•×¨|×¤×’×™×©×”)'
    
    # If booking verb + (time/day OR availability) â†’ it's a booking request!
    if re.search(booking_verbs, text_lower):
        if re.search(time_day_terms, text_lower) or re.search(availability_terms, text_lower):
            print(f"ğŸ¯ BOOKING_PRE_CHECK: Detected booking verb + time/availability")
            return "book"
    
    # ğŸ”¥ CHECK INFO (after booking pre-check!)
    for pattern in info_patterns:
        if re.search(pattern, text_lower):
            print(f"ğŸ¯ INTENT_MATCH: pattern='{pattern}' matched in '{text_lower[:50]}'")
            return "info"
    
    # Only check book patterns AFTER info has been ruled out
    for pattern in book_patterns:
        if re.search(pattern, text_lower):
            return "book"
    
    # ğŸ”¥ FIX: Default to "other" (Agent) for unmatched questions
    # Quality/experience questions ("×”××•×›×œ ×§×©×”?") need full Agent conversation handling
    # Only explicit info patterns should trigger FAQ fast-path
    return "other"  # Agent handles ambiguous/quality questions correctly

def extract_time_hebrew(text: str) -> Optional[Dict[str, Any]]:
    """
    ğŸš€ Extract explicit date/time from Hebrew text
    Returns: {"day": "tomorrow", "time": "14:00"} or None
    """
    text_lower = text.lower()
    result = {}
    
    # Day extraction
    day_map = {
        "××—×¨": "tomorrow",
        "×”×™×•×": "today",
        "×¨××©×•×Ÿ": "sunday",
        "×©× ×™": "monday",
        "×©×œ×™×©×™": "tuesday",
        "×¨×‘×™×¢×™": "wednesday",
        "×—××™×©×™": "thursday",
        "×©×™×©×™": "friday",
        "×©×‘×ª": "saturday"
    }
    
    for heb, eng in day_map.items():
        if heb in text_lower:
            result["day"] = eng
            break
    
    # Time extraction
    # Format: "14:00", "2:30", "×‘×©×¢×” 12", "×‘-3"
    time_patterns = [
        r'(\d{1,2}):(\d{2})',  # 14:00, 2:30
        r'×‘×©×¢×”?\s*(\d{1,2})',  # ×‘×©×¢×” 12
        r'×‘-(\d{1,2})',         # ×‘-3
        r'(\d{1,2})\s*(×‘×‘×•×§×¨|×‘×¦×”×¨×™×™×|××—×”×´×¦|×‘×¢×¨×‘)',  # 3 ×‘×‘×•×§×¨
    ]
    
    for pattern in time_patterns:
        match = re.search(pattern, text_lower)
        if match:
            hour = int(match.group(1))
            
            # Adjust for AM/PM context
            if len(match.groups()) > 1:
                context = match.group(2) if match.group(2) else ""
                if "×‘×‘×•×§×¨" in context and hour <= 8:
                    hour = hour  # Keep as is
                elif hour <= 8:  # Assume PM for 1-8 without context
                    hour += 12
            
            result["time"] = f"{hour:02d}:00"
            break
    
    return result if result else None

def get_ai_service():
    """Get or create global AI service instance"""
    global _global_ai_service
    if _global_ai_service is None:
        _global_ai_service = AIService()
        # âš¡ CRITICAL: Warmup cache at startup
        _warmup_ai_cache(_global_ai_service)
    return _global_ai_service

def _warmup_ai_cache(service: 'AIService'):
    """âš¡ Preload cache for ALL active businesses to prevent first-turn latency"""
    try:
        import time
        from server.models import Business
        from server.app_factory import get_process_app
        
        start = time.time()
        
        # ğŸ”¥ MULTI-TENANT: Warmup ALL active businesses (up to 10)
        app = get_process_app()
        with app.app_context():
            businesses = Business.query.filter_by(is_active=True).limit(10).all()
            
            if not businesses:
                logger.warning("âš ï¸ WARMUP: No active businesses found")
                return
            
            logger.info(f"ğŸ”¥ AI_CACHE_WARMUP: Found {len(businesses)} active businesses")
            
            for business in businesses:
                business_id = business.id
                for channel in ['calls', 'whatsapp']:
                    try:
                        service.get_business_prompt(business_id, channel)
                        logger.info(f"âœ… WARMUP: Preloaded business {business_id} ({business.name}) {channel}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ WARMUP failed for business {business_id} {channel}: {e}")
            
            warmup_time = time.time() - start
            logger.info(f"âœ… AI_CACHE_WARMUP: Completed {len(businesses)} businesses in {warmup_time:.3f}s")
    except Exception as e:
        logger.error(f"âŒ AI cache warmup failed: {e}")

def invalidate_business_cache(business_id: int):
    """ğŸ”¥ CRITICAL: Invalidate cache for business - called after prompt updates"""
    service = get_ai_service()
    
    # 1. Clear prompt cache (AIService)
    cache_keys_to_remove = [
        f"business_{business_id}_calls",
        f"business_{business_id}_whatsapp"
    ]
    for key in cache_keys_to_remove:
        if key in service._cache:
            del service._cache[key]
            logger.info(f"âœ… Prompt cache invalidated: {key}")
    
    # 2. ğŸ”¥ NEW: Clear agent cache (agent_factory)
    try:
        from server.agent_tools.agent_factory import invalidate_agent_cache
        invalidate_agent_cache(business_id)
        logger.info(f"âœ… Agent cache invalidated for business {business_id}")
    except Exception as e:
        logger.error(f"âš ï¸ Failed to invalidate agent cache: {e}")

class AIService:
    """×× ×’× ×•×Ÿ AI ××¨×›×–×™ ×©×˜×•×¢×Ÿ ×¤×¨×•××¤×˜×™× ××”××¡×“ × ×ª×•× ×™× ×•××—×‘×¨ ×¢× OpenAI"""
    
    def __init__(self):
        # âš¡ RELIABLE OpenAI client with production timeout
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            timeout=2.5  # ğŸ”¥ REDUCED: 2.5s timeout for faster real-time conversations (was 3.5s)
        )
        self._cache = {}  # ×§××© ×¤×¨×•××¤×˜×™× ×œ×‘×™×¦×•×¢×™×
        self._cache_timeout = 300  # âš¡ 5 ×“×§×•×ª - ××¡×¤×™×§ ××¨×•×š ×œ×©×™×—×” ×©×œ××”
        
    def get_business_prompt(self, business_id: int, channel: str = "calls") -> Dict[str, Any]:
        """×˜×¢×™× ×ª ×¤×¨×•××¤×˜ ×¢×¡×§ ××”××¡×“ × ×ª×•× ×™× ×¢× ×§××© - ×œ×¤×™ ×¢×¨×•×¥ (calls/whatsapp)"""
        cache_key = f"business_{business_id}_{channel}"
        now = datetime.now().timestamp()
        
        # ×‘×“×™×§×ª ×§××©
        if cache_key in self._cache:
            cached_data, timestamp = self._cache[cache_key]
            if now - timestamp < self._cache_timeout:
                logger.info(f"âœ… CACHE_HIT: business {business_id} {channel}")
                return cached_data
        
        try:
            # âš¡ CRITICAL: Measure DB query time
            import time
            db_start = time.time()
            
            # ×˜×¢×™× ×ª ×”×’×“×¨×•×ª ×¢×¡×§
            settings = BusinessSettings.query.filter_by(tenant_id=business_id).first()
            business = Business.query.get(business_id)
            
            db_time = time.time() - db_start
            logger.info(f"ğŸ“Š DB_QUERY: {db_time:.3f}s for business {business_id}")
            
            # âœ… ×©× ×¢×¡×§ ×œ×©×™××•×© ×‘-placeholders
            business_name = business.name if business else "×”×¢×¡×§ ×©×œ× ×•"
            
            # ×‘×—×™×¨×ª ×¤×¨×•××¤×˜ ×—×›××” - ×¢× fallback ×œ-business.system_prompt
            system_prompt = ""
            if settings and settings.ai_prompt and settings.ai_prompt.strip():
                # ×™×© ×¤×¨×•××¤×˜ ×‘-settings - ×ª××™×“ ×ª×©×ª××© ×‘×•! (×œ×œ× ×‘×“×™×§×ª ××•×¨×š)
                import json
                try:
                    # × ×¡×™×•×Ÿ ×œ×¤×¨×•×¡ ×›-JSON (×¤×•×¨××˜ ×—×“×© ×¢× calls/whatsapp)
                    if settings.ai_prompt.strip().startswith('{'):
                        prompt_obj = json.loads(settings.ai_prompt)
                        # ×‘×—×™×¨×ª ×”×¤×¨×•××¤×˜ ×”× ×›×•×Ÿ ×œ×¤×™ channel
                        system_prompt = prompt_obj.get(channel, prompt_obj.get('calls', settings.ai_prompt))
                        logger.info(f"âœ… Using {channel} prompt for business {business_id} from settings")
                        logger.info(f"ğŸ” DEBUG: Loaded prompt starts with: {system_prompt[:100]}...")
                    else:
                        # ×¤×¨×•××¤×˜ ×˜×§×¡×˜ ×¤×©×•×˜ (legacy)
                        system_prompt = settings.ai_prompt
                        logger.info(f"âœ… Using legacy text prompt for business {business_id}")
                except json.JSONDecodeError:
                    # ×× ×–×” ×œ× JSON ×ª×§×™×Ÿ, ×”×©×ª××© ×‘×–×” ×›×˜×§×¡×˜
                    system_prompt = settings.ai_prompt
                    logger.info(f"âœ… Using non-JSON prompt for business {business_id}")
            elif business and business.system_prompt and business.system_prompt.strip():
                # fallback ×œ×¤×¨×•××¤×˜ ×”××œ× ××˜×‘×œ×ª business
                system_prompt = business.system_prompt
                logger.info(f"âœ… Using fallback prompt from business.system_prompt for {business_id}")
            else:
                # fallback ××—×¨×•×Ÿ ×œ×¤×¨×•××¤×˜ ×‘×¨×™×¨×ª ××—×“×œ
                system_prompt = self._get_default_hebrew_prompt(business_name, channel)
                logger.info(f"âš ï¸ Using default prompt for business {business_id} - no custom prompt found")
            
            # âœ… ×”×—×œ×¤×ª placeholders ×“×™× ××™×™× ×‘×¤×¨×•××¤×˜
            system_prompt = system_prompt.replace("{{business_name}}", business_name)
            system_prompt = system_prompt.replace("{{BUSINESS_NAME}}", business_name)
            logger.info(f"âœ… Replaced {{{{business_name}}}} with '{business_name}'")
            
            # âš¡ BUILD 118: Warn if prompt is too long (causes OpenAI timeouts)
            if len(system_prompt) > 3000:
                logger.warning(f"âš ï¸ PROMPT_TOO_LONG: {len(system_prompt)} chars (recommended: <3000) - may cause OpenAI timeouts!")
            else:
                logger.info(f"âœ… Prompt length OK: {len(system_prompt)} chars")
            
            if not settings:
                # âš¡ BUILD 117: INCREASED - allow complete sentences without truncation
                prompt_data = {
                    "system_prompt": system_prompt,
                    "business_name": business_name,  # ğŸ”¥ FIX: Include business name for FAQ handler
                    "model": "gpt-4o-mini",  # Fast model
                    "max_tokens": 350,  # âš¡ BUILD 117: 350 tokens for COMPLETE sentences (no mid-sentence cuts!)
                    "temperature": 0.3  # Balanced temperature for natural responses
                }
            else:
                prompt_data = {
                    "system_prompt": system_prompt,
                    "business_name": business_name,  # ğŸ”¥ FIX: Include business name for FAQ handler
                    "model": settings.model,
                    "max_tokens": min(settings.max_tokens, 350),  # âš¡ BUILD 117: Cap at 350 for complete sentences
                    "temperature": min(settings.temperature, 0.4)  # Balanced temperature
                }
            
            # ×©××™×¨×” ×‘×§××©
            self._cache[cache_key] = (prompt_data, now)
            return prompt_data
            
        except Exception as e:
            logger.error(f"Error loading business prompt {business_id}: {e}")
            # âš¡ FAST fallback - ×˜×¢×™× ×ª ×©× ×¢×¡×§ ××”-DB
            try:
                business = Business.query.get(business_id)
                business_name = business.name if business else "×”×¢×¡×§ ×©×œ× ×•"
            except:
                business_name = "×”×¢×¡×§ ×©×œ× ×•"
            
            return {
                "system_prompt": self._get_default_hebrew_prompt(business_name, channel),
                "business_name": business_name,  # ğŸ”¥ FIX: Include business name for FAQ handler
                "model": "gpt-4o-mini",
                "max_tokens": 350,  # âš¡ BUILD 117: 350 tokens for COMPLETE sentences
                "temperature": 0.3  # Balanced
            }
    
    def _get_default_hebrew_prompt(self, business_name: str = "×”×¢×¡×§ ×©×œ× ×•", channel: str = "calls") -> str:
        """×¤×¨×•××¤×˜ ×‘×¨×™×¨×ª ××—×“×œ ×‘×¢×‘×¨×™×ª ×œ× ×“×œ"×Ÿ - ××•×ª×× ×œ×¢×¨×•×¥ - âœ… ×‘×œ×™ ×©× hardcoded!"""
        if channel == "whatsapp":
            return f"""××ª×” ×”×¢×•×–×¨ ×”×“×™×’×™×˜×œ×™ ×©×œ {business_name} ×‘-WhatsApp.

×›×œ×œ×™× ×—×©×•×‘×™×:
- ×ª×¢× ×” ×‘×¢×‘×¨×™×ª, ×ª×©×•×‘×•×ª ×§×¦×¨×•×ª (×¢×“ 150 ××™×œ×™×)
- ×ª×”×™×” ×—× ×•×™×“×™×“×•×ª×™ ×‘×¡×’× ×•×Ÿ WhatsApp
- ×ª×‘×§×© ×¤×¨×˜×™×: ××–×•×¨, ×¡×•×’ × ×›×¡, ×ª×§×¦×™×‘
- ×›×©××ª×” ××–×›×™×¨ ××—×™×¨×™×/×ª×§×¦×™×‘ - ×ª××™×“ ×¦×™×™×Ÿ "××™×œ×™×•×Ÿ", "××œ×£", "××™×œ×™××¨×“" (×œ× ×¨×§ ××¡×¤×¨×™×!)
- ×ª×¦×™×¢ ×œ×§×‘×•×¢ ×¤×’×™×©×” ×›×©×™×© ××™×“×¢ ××¡×¤×™×§
- âš ï¸ ××œ ×ª×—×–×•×¨ ×¢×œ ×©××š ×‘×›×œ ××©×¤×˜! ×–×” ××¢×¦×‘×Ÿ ×•×œ× ×˜×‘×¢×™
- ×“×‘×¨ ×™×©×¨ ×œ×¢× ×™×™×Ÿ ×‘×œ×™ ×œ×”×¦×™×’ ××ª ×¢×¦××š ×›×œ ×¤×¢× ××—×“×©

**×›×©×œ×§×•×— ××¡×›×™× ×œ×–××Ÿ ×¤×’×™×©×”:**
ğŸ¯ **×—×–×•×¨ ×¢×œ ×”×–××Ÿ ×”××“×•×™×§ ×©×”×œ×§×•×— ×××¨!**
×“×•×’×××•×ª:
- ×œ×§×•×—: "××—×¨ ×‘-10" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 10:00."
- ×œ×§×•×—: "××—×¨ ×‘-15" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 15:00."
âš ï¸ **××œ ×ª×©× ×” ××ª ×”×©×¢×” - ×—×–×•×¨ ×¢×œ ××” ×©×”×œ×§×•×— ×××¨!**

×ª×¤×§×™×“×š: ×œ×¢×–×•×¨ ×œ××¦×•× × ×›×¡ ×•×œ×”×•×‘×™×œ ×œ×¤×’×™×©×”."""
        else:
            # âœ¨ Calls - ×¤×¨×•××¤×˜ ××¤×•×¨×˜ ×œ×©×™×—×•×ª ×–×•×¨××•×ª ×•×˜×‘×¢×™×•×ª
            return f"""××ª×” ×”×¢×•×–×¨ ×”×“×™×’×™×˜×œ×™ ×©×œ {business_name}. ××ª×” ×›××Ÿ ×›×“×™ ×œ×¢×–×•×¨ ×œ×œ×§×•×—×•×ª ×œ××¦×•× ××ª ×”× ×›×¡ ×”××•×©×œ× - ×“×™×¨×•×ª ×œ××›×™×¨×”, ×“×™×¨×•×ª ×œ×”×©×›×¨×”, ×‘×ª×™×, ×•××©×¨×“×™×.

×”×ª× ×”×œ×•×ª ×‘×©×™×—×”:
â€¢ ×“×‘×¨ ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“, ×‘×¦×•×¨×” ×˜×‘×¢×™×ª ×•×–×•×¨××ª ×›××• ×©×™×—×” ×¨×’×™×œ×” ×‘×˜×œ×¤×•×Ÿ
â€¢ ×”×™×” ×—×, ×™×“×™×“×•×ª×™, ××‘×œ ××§×¦×•×¢×™ - ×›××• ×¡×•×›×Ÿ × ×“×œ"×Ÿ ×× ×•×¡×”
â€¢ ×ª×©×•×‘×•×ª ×§×¦×¨×•×ª - 2-3 ××©×¤×˜×™× ×‘×›×œ ×ª×’×•×‘×” (×¢×“ 200 ××™×œ×™×)
â€¢ ×“×‘×¨ ×™×©×™×¨×•×ª ×œ×¢× ×™×™×Ÿ, ×‘×œ×™ ××™×œ×•×™ ××• ×¡×™×¤×•×¨×™× ××¨×•×›×™×
â€¢ âš ï¸ ×—×©×•×‘ ×××•×“: ××œ ×ª×—×–×•×¨ ×¢×œ ×©××š ×‘×›×œ ××©×¤×˜! ×–×” ×œ× ×˜×‘×¢×™ ×•××¢×¦×‘×Ÿ
â€¢ ×”×¦×’ ××ª ×¢×¦××š ×¨×§ ×‘×‘×¨×›×” ×”×¨××©×•× ×”, ××—×¨ ×›×š ×“×‘×¨ ×™×©×¨ ×œ×¢× ×™×™×Ÿ

××™×¡×•×£ ××™×“×¢ ×—×›×:
×©××œ ×©××œ×” ××—×ª ×‘×›×œ ×¤×¢×, ×‘×¡×“×¨ ×”×–×”:
1. ×ª×—×™×œ×”: ××” ×”×œ×§×•×— ××—×¤×©? (×“×™×¨×”/×‘×™×ª/××©×¨×“, ××›×™×¨×”/×”×©×›×¨×”)
2. ××–×•×¨ ××‘×•×§×© ××• ×¢×™×¨ (×—×©×•×‘ ×××•×“!)
3. ×ª×§×¦×™×‘ ××• ×˜×•×•×— ××—×™×¨×™× - âš ï¸ ×—×©×•×‘: ×ª××™×“ ×”×–×›×¨ ××ª ×¡×“×¨ ×”×’×•×“×œ! ×××•×¨ "××™×œ×™×•×Ÿ ×©×§×œ", "×××” ××œ×£ ×©×§×œ", "×—×¦×™ ××™×œ×™×•×Ÿ" ×•×›×•'. ×œ×¢×•×œ× ××œ ×ª×’×™×“ ×¨×§ ××ª ×”××¡×¤×¨ (×œ××©×œ "1000000") ×‘×œ×™ ×œ×”×–×›×™×¨ ×× ×–×” ××œ×£/××™×œ×™×•×Ÿ/××™×œ×™××¨×“!
4. ××¡×¤×¨ ×—×“×¨×™× / ×’×•×“×œ
5. ×¤×¨×˜×™ ×§×©×¨: ×©× ××œ× ×•××™×™×œ ×× ×œ× × ×™×ª× ×•

××ª×™ ×œ×§×‘×•×¢ ×¤×’×™×©×”:
×›×©×™×© ×œ×š ×œ×¤×—×•×ª: ×¡×•×’ × ×›×¡ + ××–×•×¨ + ×ª×§×¦×™×‘ â†’ ×”×¦×¢ ×œ×§×‘×•×¢ ×¤×’×™×©×” ×¢× ×”×¡×•×›×Ÿ. ×ª×’×™×“: "××¢×•×œ×”! ×™×© ×œ×™ ×›××” ××¤×©×¨×•×™×•×ª ××¦×•×™× ×•×ª. ××©××— ×œ×§×‘×•×¢ ×œ×š ×¤×’×™×©×” ×¢× ××—×“ ×”×¡×•×›× ×™× ×©×œ× ×• ×©×™×¦×™×’ ×œ×š ××ª ×”× ×›×¡×™×. ××ª×™ × ×•×— ×œ×š?"

âš ï¸ **×—×©×•×‘ ×××•×“ - ×›×©×”×œ×§×•×— ××¡×›×™× ×œ×–××Ÿ:**
ğŸ¯ **×—×•×§ ×‘×¨×–×œ: ×—×–×•×¨ ×¢×œ ×”×–××Ÿ ×”××“×•×™×§ ×©×”×œ×§×•×— ×××¨ - ×œ× ×œ×”××¦×™× ×©×¢×•×ª!**

×›×©×”×œ×§×•×— ××•××¨ ×–××Ÿ ×¡×¤×¦×™×¤×™:
- ×œ×§×•×—: "××—×¨ ×‘-10" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 10:00."
- ×œ×§×•×—: "××—×¨ ×‘-16" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 16:00."
- ×œ×§×•×—: "×™×•× ×©×œ×™×©×™ ×‘-14:30" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ×™×•× ×©×œ×™×©×™ ×‘×©×¢×” 14:30."

×›×©×”×œ×§×•×— ××•××¨ ×–××Ÿ ×›×œ×œ×™ (×‘×•×§×¨/×¦×”×¨×™×™×/××—×”"×¦):
- ×œ×§×•×—: "××—×¨ ×‘×‘×•×§×¨" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 10:00."
- ×œ×§×•×—: "×™×•× ×©×œ×™×©×™ ××—×¨ ×”×¦×”×¨×™×™×" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ×™×•× ×©×œ×™×©×™ ×‘×©×¢×” 14:00."

âš ï¸ **××œ ×ª×©× ×” ××ª ×”×©×¢×” ×©×”×œ×§×•×— ×××¨! ×× ×”×•× ×××¨ 16 - ×ª××©×¨ 16, ×œ× 10!**

×—×©×•×‘: ××œ ×ª××¦×™× ××™×“×¢! ×× ×œ× ×™×•×“×¢ ××©×”×• - ×”×¤× ×” ×œ×¡×•×›×Ÿ ×× ×•×©×™. ×× ×”×œ×§×•×— ×¢×¦×‘× ×™ ××• ××ª×œ×•× ×Ÿ - ×”×™×” ×××¤×˜×™ ×•×”×¦×¢ ×“×™×‘×•×¨ ×¢× ×× ×”×œ."""

    def generate_response(self, message: str, business_id: int = 1, context: Optional[Dict[str, Any]] = None, channel: str = "calls", is_first_turn: bool = False) -> str:
        """×™×¦×™×¨×ª ×ª×’×•×‘×” ××¤×¨×•××¤×˜ ×“×™× ××™ + ×”×§×©×¨ - ×œ×¤×™ ×¢×¨×•×¥ (calls/whatsapp)"""
        try:
            # ×˜×¢×™× ×ª ×¤×¨×•××¤×˜ ×¢×¡×§ ×œ×¤×™ ×¢×¨×•×¥
            prompt_data = self.get_business_prompt(business_id, channel)
            
            # âš¡ BUILD 117: First turn - NO SPECIAL LIMIT! Let AI finish complete sentences
            # User requirement: "×× ×”×™× ×¦×¨×™×›×” ×œ×”×¡×‘×™×¨ ×“×§×” ×©×ª×¡×‘×™×¨ ×“×§×”" - let it speak as long as needed
            if is_first_turn:
                # Don't reduce max_tokens for first turn - keep the default 350 for complete sentences
                logger.info(f"ğŸ¯ First turn - using full {prompt_data['max_tokens']} tokens for complete sentences")
            
            # ×‘× ×™×™×ª ×”×•×“×¢×•×ª
            messages: List[Dict[str, str]] = [
                {"role": "system", "content": prompt_data["system_prompt"]}
            ]
            
            # âœ… ×”×•×¡×¤×ª ×–××™× ×•×ª ×œ×•×— ×©× ×” (×¨×§ ×œ-WhatsApp - ×œ× ×œ×˜×œ×¤×•×Ÿ ×‘×’×œ×œ latency!)
            if channel == "whatsapp":
                calendar_info = self._get_calendar_availability(business_id)
                if calendar_info:
                    messages.append({
                        "role": "system",
                        "content": f"ğŸ“… ×œ×•×— ×©× ×”:\n{calendar_info}\n×›×©×”×œ×§×•×— ××•×›×Ÿ ×œ×¤×’×™×©×”, ×”×¦×¢ ×ª××¨×™×›×™× ×¤× ×•×™×™× ××”×¨×©×™××” ×œ××¢×œ×”."
                    })
            
            # ×”×•×¡×¤×ª ×”×§×©×¨ ×× ×§×™×™×
            if context:
                # ×”×•×¡×¤×ª ××™×“×¢ ×‘×¡×™×¡×™ ×¢×œ ×”×œ×§×•×—
                context_info = []
                if context.get("customer_name"):
                    context_info.append(f"×©× ×”×œ×§×•×—: {context['customer_name']}")
                if context.get("phone_number"):
                    context_info.append(f"×˜×œ×¤×•×Ÿ: {context['phone_number']}")
                
                if context_info:
                    messages.append({
                        "role": "system", 
                        "content": "××™×“×¢ ×¢×œ ×”×œ×§×•×—:\n" + "\n".join(context_info)
                    })
                
                # âœ… BUILD 92: ×©×œ×™×—×ª previous_messages ×›×©×™×—×” ×××™×ª×™×ª - 10 ×”×•×“×¢×•×ª ×œ×–×™×›×¨×•×Ÿ ××œ×!
                if context.get("previous_messages"):
                    prev_msgs = context["previous_messages"][-10:]  # âœ… 10 ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª (×œ× 6!)
                    for msg in prev_msgs:
                        # âœ… ×”××‘× ×” ×”×•× "×œ×§×•×—: ..." ××• "×¢×•×–×¨×ª: ..." (××• "×œ××”:" legacy)
                        if msg.startswith("×œ×§×•×—:"):
                            messages.append({
                                "role": "user",
                                "content": msg.replace("×œ×§×•×—:", "").strip()
                            })
                        elif msg.startswith("×¢×•×–×¨×ª:") or msg.startswith("×œ××”:"):  # âœ… ×ª××™×›×” ×‘×©× ×™×”×!
                            content = msg.replace("×¢×•×–×¨×ª:", "").replace("×œ××”:", "").strip()
                            messages.append({
                                "role": "assistant",
                                "content": content
                            })
            
            # ×”×•×¡×¤×ª ×”×•×“×¢×ª ×”××©×ª××© ×”× ×•×›×—×™×ª
            messages.append({"role": "user", "content": message})
            
            # âš¡ CRITICAL: Measure OpenAI call time
            import time
            openai_start = time.time()
            
            # âš¡ BUILD 118: Add explicit timeout to prevent long waits
            try:
                response = self.client.chat.completions.create(
                    model=prompt_data["model"],
                    messages=messages,  # type: ignore
                    max_tokens=prompt_data["max_tokens"],
                    temperature=prompt_data["temperature"],
                    timeout=2.5  # ğŸ”¥ REDUCED: 2.5s timeout for real-time conversations (was 3.5s)
                )
                
                openai_time = time.time() - openai_start
                logger.info(f"âœ… OPENAI_SUCCESS: {openai_time:.3f}s")
                
                ai_response = response.choices[0].message.content
                if ai_response:
                    ai_response = ai_response.strip()
                else:
                    ai_response = "××¦×˜×¢×¨, ×œ× ×”×¦×œ×—×ª×™ ×œ×™×™×¦×¨ ×ª×’×•×‘×” ×›×¨×’×¢."
                logger.info(f"AI response generated for business {business_id}: {len(ai_response)} chars")
                return ai_response
                
            except Exception as openai_error:
                openai_time = time.time() - openai_start
                error_type = type(openai_error).__name__
                logger.error(f"ğŸ”´ OPENAI_FAILED: {error_type} after {openai_time:.3f}s: {str(openai_error)[:200]}")
                raise  # Re-raise to outer exception handler
            
        except Exception as e:
            logger.error(f"ğŸ”´ AI_GENERATION_FAILED: {type(e).__name__}: {str(e)[:200]}")
            return self._get_fallback_response(message)
    
    def _get_fallback_response(self, message: str) -> str:
        """×ª×’×•×‘×ª ×—×™×¨×•× ×× ×”-AI × ×›×©×œ"""
        message_lower = message.lower().strip()
        
        if any(word in message_lower for word in ["×©×œ×•×", "×”×™×™", "×”×œ×•"]):
            return "×©×œ×•×! ××™×š ××•×›×œ ×œ×¢×–×•×¨ ×œ×š?"  # âœ… ×›×œ×œ×™ - ×œ× ×—×•×©×£ ×©× ×¢×¡×§ ×©×’×•×™
        elif any(word in message_lower for word in ["×“×™×¨×”", "×‘×™×ª", "× ×›×¡"]):
            return "××©××— ×œ×¢×–×•×¨ ×œ×š! ××ª×” ××—×¤×© ×œ×§× ×™×” ××• ×”×©×›×¨×”? ×‘××™×–×” ××–×•×¨?"
        else:
            return "×ª×•×“×” ×¢×œ ×”×¤× ×™×™×”! ××—×–×•×¨ ××œ×™×š ×‘×”×§×“× ×¢× ××¢× ×” ××¤×•×¨×˜."
    
    def _get_calendar_availability(self, business_id: int) -> str:
        """×‘×“×™×§×ª ×–××™× ×•×ª ×‘×œ×•×— ×”×©× ×” ×œ-7 ×™××™× ×”×§×¨×•×‘×™×"""
        try:
            from server.models_sql import Appointment
            from datetime import datetime, timedelta
            
            # âš¡ FAST: Limit query time with LIMIT
            # ×˜×•×•×— ×ª××¨×™×›×™×: ×”×™×•× + 7 ×™××™×
            today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            week_end = today + timedelta(days=7)
            
            # ×©×œ×™×¤×ª ×¤×’×™×©×•×ª ×§×™×™××•×ª (LIMIT 10 ×œ××”×™×¨×•×ª!)
            appointments = Appointment.query.filter(
                Appointment.business_id == business_id,
                Appointment.start_time >= today,
                Appointment.start_time < week_end,
                Appointment.status.in_(['confirmed', 'pending'])
            ).order_by(Appointment.start_time).limit(10).all()
            
            # ×”×¦×¢×ª ×–×× ×™× ×¤× ×•×™×™× (9:00-17:00, ×›×œ ×™×•×, ×œ××¢×˜ ×©×‘×ª)
            available_slots = []
            for i in range(7):
                day = today + timedelta(days=i)
                # ×“×œ×’ ×¢×œ ×©×‘×ª (5 = ×©×‘×ª)
                if day.weekday() == 5:
                    continue
                    
                day_name = day.strftime("%A")
                day_name_he = {"Monday": "×©× ×™", "Tuesday": "×©×œ×™×©×™", "Wednesday": "×¨×‘×™×¢×™", 
                              "Thursday": "×—××™×©×™", "Friday": "×©×™×©×™", "Sunday": "×¨××©×•×Ÿ"}.get(day_name, day_name)
                
                # ×‘×“×•×§ ×× ×™×© ×¤×’×™×©×•×ª ×‘×™×•× ×”×–×”
                day_start = day.replace(hour=9, minute=0)
                day_end = day.replace(hour=17, minute=0)
                
                day_appointments = [apt for apt in appointments if day_start <= apt.start_time < day_end]
                
                if len(day_appointments) < 4:  # ×× ×¤×—×•×ª ×-4 ×¤×’×™×©×•×ª - ×¢×“×™×™×Ÿ ×™×© ××§×•×
                    date_str = day.strftime("%d/%m")
                    available_slots.append(f"×™×•× {day_name_he} {date_str} (×‘×•×§×¨/××—×”\"×¦)")
            
            # ×‘× ×™×™×ª ×˜×§×¡×˜
            result = []
            if available_slots:
                result.append("âœ… ×–××™× ×•×ª ×”×©×‘×•×¢:")
                result.extend([f"  â€¢ {slot}" for slot in available_slots[:5]])  # ×¨×§ 5 ×¨××©×•× ×™×
            else:
                result.append("âš ï¸ ××™×Ÿ ×–××™× ×•×ª ×”×©×‘×•×¢ - ×”×¦×¢ ×©×‘×•×¢ ×”×‘×")
            
            return "\n".join(result)
            
        except Exception as e:
            logger.error(f"Calendar check failed: {e}")
            return "ğŸ“… ×œ×•×— ×”×©× ×”: × × ×œ×ª×× ×™×©×™×¨×•×ª ×¢× ×”×¡×•×›×Ÿ"
    
    def invalidate_cache(self, business_id: int):
        """××—×™×§×ª ×§××© ×¢×¡×§ ××¡×•×™× (×œ××—×¨ ×¢×“×›×•×Ÿ ×¤×¨×•××¤×˜)"""
        cache_key = f"business_{business_id}"
        if cache_key in self._cache:
            del self._cache[cache_key]
            logger.info(f"Cache invalidated for business {business_id}")
    
    def save_conversation_history(self, business_id: int, phone_number: str, 
                                 message: str, response: str, channel: str = "whatsapp"):
        """×©××™×¨×ª ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×” ×œ××™×“×¢ ×¢×ª×™×“×™ (××•×¤×¦×™×•× ×œ×™)"""
        try:
            # ×›××Ÿ ××¤×©×¨ ×œ×”×•×¡×™×£ ×œ×•×’×™×§×” ×œ×©××™×¨×ª ×©×™×—×•×ª ××¨×•×›×•×ª
            # ×œ×¦×¨×›×™ ×”×§×©×¨ ×¢×ª×™×“×™ ××• ×× ×œ×™×˜×™×§×”
            pass
        except Exception as e:
            logger.error(f"Failed to save conversation history: {e}")
    
    def _generate_faq_response(self, message: str, faq_answer: str, business_id: int, channel: str) -> Optional[str]:
        """
        ğŸš€ Generate FAQ fast-path response using lightweight LLM
        Uses gpt-4o-mini with max_tokens=80, temp=0.3 for <1.5s responses
        
        Args:
            message: Customer question
            faq_answer: Matched FAQ answer from database
            business_id: Business ID
            channel: Communication channel (phone/whatsapp)
            
        Returns:
            Natural Hebrew response or None if generation failed
        """
        start = time.time()
        
        try:
            # Get business name
            business = Business.query.get(business_id)
            business_name = business.name if business else "×”×¢×¡×§"
            
            # Mini prompt for FAQ responses - focus on natural rephrasing
            faq_prompt = f"""××ª×” ×¢×•×–×¨ ×“×™×’×™×˜×œ×™ ×¢×‘×•×¨ {business_name}.
×œ×§×•×— ×©××œ ×©××œ×”, ×•× ××¦××” ×”×ª×××” ×‘×××’×¨ ×”×©××œ×•×ª ×”× ×¤×•×¦×•×ª.

××©×™××ª×š: ×”×©×‘ ×‘×¢×‘×¨×™×ª ×˜×‘×¢×™×ª ×•×§×¦×¨×” (1-2 ××©×¤×˜×™×) ×¢×œ ×¡××š ×”×ª×©×•×‘×” ×©× ××¦××”.

×©××œ×ª ×”×œ×§×•×—: {message}
×ª×©×•×‘×” ××”×××’×¨: {faq_answer}

×—×•×§×™×:
1. ×”×©×‘ ×‘×¢×‘×¨×™×ª ×¤×©×•×˜×” ×•×˜×‘×¢×™×ª
2. ×§×¦×¨ - ××§×¡×™××•× 2 ××©×¤×˜×™×
3. ××œ ×ª×•×¡×™×£ ××™×“×¢ ×©×œ× ×‘×ª×©×•×‘×” ×”××§×•×¨×™×ª
4. ××œ ×ª×××¨ "×œ×¤×™ ×”××™×“×¢" ××• "× ××¦× ×‘×××’×¨"
5. ××œ ×ª×¦×™×™×Ÿ ×©×–××ª ×©××œ×” × ×¤×•×¦×”

×ª×©×•×‘×”:"""
            
            # Call OpenAI with FAQ-optimized settings
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": faq_prompt}
                ],
                max_tokens=80,
                temperature=0.3,
                timeout=4.0
            )
            
            reply = response.choices[0].message.content.strip()
            
            elapsed = (time.time() - start) * 1000
            print(f"âš¡ FAQ response generated in {elapsed:.0f}ms")
            logger.info(f"âš¡ FAQ fast-path total time: {elapsed:.0f}ms")
            
            return reply
            
        except Exception as e:
            elapsed = (time.time() - start) * 1000
            print(f"âŒ FAQ response generation failed after {elapsed:.0f}ms: {e}")
            logger.error(f"FAQ response generation failed: {e}")
            return None
    
    def _handle_lightweight_intent(self, intent: str, message: str, business_id: int, 
                                   channel: str, context: Optional[Dict], customer_phone: Optional[str]) -> Optional[str]:
        """
        ğŸš€ Fast FAQ/Info handler - NO AgentKit!
        Target latency: ~1.0-1.5s
        
        Returns:
            str: Fast response
            None: Signal to fallback to AgentKit
        """
        start = time.time()
        
        try:
            # Get business prompt for FAQ info
            prompt_data = self.get_business_prompt(business_id, channel)
            system_prompt = prompt_data.get("system_prompt", "")
            business_name = prompt_data.get("business_name", "×”×¢×¡×§")
            
            response = None
            
            if intent == "info":
                # Extract FAQ from prompt - lightweight LLM call
                response = self._get_faq_response(message, system_prompt, business_name)
                
                # ğŸ”¥ FIX: If FAQ failed (returned None), signal fallback to AgentKit
                if response is None:
                    logger.warning(f"FAQ failed for info query, falling back to AgentKit")
                    return None
            
            else:  # Should not reach here (only "info" uses fast path now)
                logger.warning(f"Unexpected intent in fast path: {intent}")
                return None
            
            latency = (time.time() - start) * 1000
            print(f"âš¡ FAST_PATH_LATENCY: {latency:.0f}ms (intent={intent})")
            logger.info(f"âš¡ Fast path response: {latency:.0f}ms")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Fast path failed: {e}")
            # Return None to signal fallback to AgentKit
            return None
    
    def _extract_faq_facts(self, question: str, full_prompt: str) -> Optional[str]:
        """
        ğŸ”¥ ARCHITECT-REVIEWED FIX: Keyword-based topic matching
        Extracts ONLY sections relevant to the question, not all sections blindly.
        
        Strategy:
        1. Parse prompt into labeled sections (pricing, menu, location, hours, description)
        2. Map question keywords to relevant section labels
        3. Return only matching sections (max 500 chars)
        4. Return None if no relevant section â†’ fallback to Agent
        """
        try:
            import re
            
            question_lower = question.lower()
            
            # Parse all sections once into a dict
            sections = {}
            
            # Pricing section (ğŸ’°)
            pricing_match = re.search(r'ğŸ’°\s*××—×™×¨×™×:.*?(?=\n\n|$)', full_prompt, re.DOTALL)
            if pricing_match:
                sections['pricing'] = pricing_match.group(0)
            
            # Menu/food section (ğŸ•, ğŸ´, or keywords)
            menu_match = re.search(r'(ğŸ•|ğŸ´|×ª×¤×¨×™×˜|××•×›×œ|××©×§××•×ª|×× ×•×ª).*?(?=\n\n|$)', full_prompt, re.DOTALL | re.IGNORECASE)
            if menu_match:
                sections['menu'] = menu_match.group(0)
            
            # Hours/schedule (â°, ğŸ•’, or "×¤×ª×•×—×™×")
            hours_match = re.search(r'(â°|ğŸ•’|×¤×ª×•×—×™×|×©×¢×•×ª).*?(?=\n\n|$)', full_prompt, re.DOTALL | re.IGNORECASE)
            if hours_match:
                sections['hours'] = hours_match.group(0)
            
            # Location (ğŸ“ or keywords)
            location_match = re.search(r'(ğŸ“|×××•×§×|××™×§×•×|×›×ª×•×‘×ª|×¨×—×•×‘).*?(?=\n\n|$)', full_prompt, re.DOTALL)
            if location_match:
                sections['location'] = location_match.group(0)
            
            # General description
            desc_match = re.search(r'^(.*?)(?=\nğŸ’°|\nğŸ”¥|\nğŸ’¬|$)', full_prompt, re.DOTALL)
            if desc_match and len(desc_match.group(0).strip()) > 50:
                sections['description'] = desc_match.group(0)[:500]
            
            # Topic keyword mapping
            # ğŸ”¥ FIX: Only match INFORMATION questions, not quality/experience questions
            topic_keywords = {
                'pricing': r'(××—×™×¨|×›××” ×¢×•×œ×”|×›××” ×–×”|×¢×œ×•×ª|×ª×©×œ×•×|×¢×•×œ×”)',
                'menu': r'(×™×©.*××•×›×œ|×™×©.*×ª×¤×¨×™×˜|××”.*×ª×¤×¨×™×˜|××”.*×œ××›×•×œ|××”.*×œ×©×ª×•×ª|×ª×¤×¨×™×˜|×× ×•×ª|××©×§××•×ª|×©×ª×™×”|×‘×¨|×§×¤×”)',
                'hours': r'(××ª×™.*×¤×ª×•×—|×©×¢×•×ª.*×¤×ª×™×—×”|×©×¢×•×ª.*×¢×‘×•×“×”|××”.*×©×¢×•×ª)',
                'location': r'(××™×¤×”|××™×§×•×|×›×ª×•×‘×ª|×”×™×›×Ÿ|×¨×—×•×‘|××–×•×¨)',
            }
            
            # Find matching sections
            matched_sections = []
            
            for topic, pattern in topic_keywords.items():
                if re.search(pattern, question_lower) and topic in sections:
                    matched_sections.append(sections[topic])
                    print(f"âœ… FAQ_MATCH: topic='{topic}' matched in question")
            
            # If no topic match, return general description if it exists
            if not matched_sections and 'description' in sections:
                matched_sections.append(sections['description'])
                print(f"â„¹ï¸ FAQ_FALLBACK: Using general description (no topic match)")
            
            # If still no match, return None â†’ Agent fallback
            if not matched_sections:
                print(f"âš ï¸ FAQ_NO_MATCH: No relevant section found, routing to Agent")
                return None
            
            # Combine matched sections (max 500 chars)
            result = "\n\n".join(matched_sections)
            if len(result) > 500:
                result = result[:500] + "..."
            
            print(f"âœ… FAQ_EXTRACTED: {len(matched_sections)} section(s), {len(result)} chars")
            return result
                
        except Exception as e:
            logger.error(f"FAQ fact extraction failed: {e}")
            # Fallback to Agent
            return None
    
    def _get_faq_response(self, question: str, system_prompt: str, business_name: str) -> Optional[str]:
        """
        ğŸš€ Fast FAQ using optimized LLM call
        Target: ~1.0-1.5s with FACTUAL prompt context (no guard-rails)
        
        ğŸ”¥ ARCHITECT-REVIEWED FIX (Phase 2O):
        - Extract ONLY factual sections (pricing/hours/location) - NO guard-rails!
        - Use FULL factual context (up to 3000 chars)
        - Increase max_tokens: 80 â†’ 180 for complete Hebrew answers
        - Increase timeout: 1.5s â†’ 2.2s for reliability
        - Add retry logic for robustness
        """
        import time
        faq_start = time.time()
        
        try:
            # ğŸ”¥ CRITICAL FIX: Extract ONLY relevant facts based on question!
            print(f"\nğŸ“š FAQ: Extracting facts from prompt ({len(system_prompt)} chars)")
            extract_start = time.time()
            faq_facts = self._extract_faq_facts(question, system_prompt) if system_prompt else None
            
            # If no relevant facts found, return None â†’ Agent fallback
            if faq_facts is None:
                print(f"âš ï¸ FAQ: No relevant facts found, routing to Agent")
                return None
            
            extract_time = (time.time() - extract_start) * 1000
            print(f"â±ï¸  FAQ: Fact extraction took {extract_time:.0f}ms")
            print(f"ğŸ“Š FAQ: Extracted {len(faq_facts)} chars of facts")
            print(f"ğŸ“ FAQ: Facts preview: {faq_facts[:200]}...")
            
            # ğŸ”¥ CRITICAL FIX: ULTRA-MINIMAL prompt - just answer the question!
            faq_system = f"""×”×©×‘ ×‘×§×¦×¨×” (2 ××©×¤×˜×™×)."""
            
            # ğŸ”¥ FIX: First attempt with full token budget
            try:
                print(f"ğŸ¤– FAQ: Calling OpenAI (model=gpt-4o-mini, max_tokens=80, timeout=4.0s)")
                llm_start = time.time()
                
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": faq_system},
                        {"role": "user", "content": f"{faq_facts}\n\n{question}"}
                    ],
                    temperature=0.3,  # âš¡ Balanced for speed vs quality
                    max_tokens=80,  # âš¡ SPEED: Reduced from 150 to 80 for faster FAQ
                    timeout=4.0  # âš¡ Consistent with Agent timeout (was 2.0s)
                )
                
                llm_time = (time.time() - llm_start) * 1000
                print(f"â±ï¸  FAQ: OpenAI call took {llm_time:.0f}ms")
                
                # ğŸ”¥ FIX: Safely handle None content
                answer = response.choices[0].message.content
                if answer:
                    answer = answer.strip()
                else:
                    answer = ""
                
                # ğŸ”¥ ARCHITECT-REVIEWED: Detect guard-rail responses and reject them
                guard_rail_phrases = [
                    "×× ×™ ×›××Ÿ ×¨×§ ×œ×¢×–×•×¨",
                    "×©××œ×•×ª ×©×§×©×•×¨×•×ª ×œ×¢×¡×§",
                    "×œ× ×™×›×•×œ ×œ×¢×–×•×¨",
                    "×œ× ×§×©×•×¨ ×œ×¢×¡×§"
                ]
                is_guard_rail = any(phrase in answer for phrase in guard_rail_phrases) if answer else False
                
                # Validate answer is not generic/empty/guard-rail
                if answer and len(answer) > 10 and "××©××— ×œ×¢×–×•×¨" not in answer and not is_guard_rail:
                    total_time = (time.time() - faq_start) * 1000
                    print(f"âœ… FAQ SUCCESS! Total time: {total_time:.0f}ms")
                    print(f"ğŸ“ FAQ Answer: {answer[:100]}...")
                    logger.info(f"âœ… FAQ success: {answer[:50]}...")
                    return answer
                else:
                    print(f"âš ï¸  FAQ: Generic/guard-rail answer detected!")
                    print(f"   Answer: {answer}")
                    print(f"   is_guard_rail={is_guard_rail}")
                    logger.warning(f"FAQ gave generic/guard-rail answer: {answer}")
                    raise ValueError("Generic/guard-rail answer - retry needed")
                    
            except Exception as retry_err:
                # ğŸ”¥ FIX: Quick retry with shorter response
                logger.warning(f"FAQ first attempt failed, retrying: {retry_err}")
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": faq_system},
                        {"role": "user", "content": f"{faq_facts[:400]}\n\n{question}"}
                    ],
                    temperature=0.3,
                    max_tokens=60,  # âš¡ Even shorter for retry
                    timeout=2.5  # âš¡ Shorter timeout for retry
                )
                # ğŸ”¥ ARCHITECT FIX: Apply guard-rail detection to retry path too!
                answer = response.choices[0].message.content
                if answer:
                    answer = answer.strip()
                else:
                    answer = ""
                
                # Detect guard-rail responses
                guard_rail_phrases = [
                    "×× ×™ ×›××Ÿ ×¨×§ ×œ×¢×–×•×¨",
                    "×©××œ×•×ª ×©×§×©×•×¨×•×ª ×œ×¢×¡×§",
                    "×œ× ×™×›×•×œ ×œ×¢×–×•×¨",
                    "×œ× ×§×©×•×¨ ×œ×¢×¡×§"
                ]
                is_guard_rail = any(phrase in answer for phrase in guard_rail_phrases) if answer else False
                
                # If still guard-rail â†’ return None to fallback to AgentKit
                if is_guard_rail or not answer or len(answer) < 10:
                    logger.warning(f"FAQ retry also gave guard-rail/generic answer - falling back to AgentKit")
                    return None
                
                return answer
            
        except Exception as e:
            total_time = (time.time() - faq_start) * 1000
            print(f"âŒ FAQ FAILED! Total time: {total_time:.0f}ms")
            print(f"   Error: {type(e).__name__}: {str(e)[:200]}")
            logger.error(f"âŒ FAQ LLM failed after retry: {e}")
            import traceback
            traceback.print_exc()
            # Return None to signal fallback to AgentKit needed
            return None
    
    def generate_response_with_agent(self, message: str, business_id: int = 1, 
                                     context: Optional[Dict[str, Any]] = None,
                                     channel: str = "calls",
                                     is_first_turn: bool = False,
                                     customer_phone: Optional[str] = None,
                                     customer_name: Optional[str] = None) -> str:
        """
        âœ¨ BUILD 119: Agent-enhanced response generation
        ğŸš€ Phase 2K: Intent-based routing - AgentKit only for bookings (â‰¤2s target)
        
        Uses AgentKit to perform real actions (appointments, leads, WhatsApp)
        Falls back to FAQ/lightweight responses for info questions
        
        Args:
            message: Customer's message
            business_id: Business ID
            context: Conversation context
            channel: calls/whatsapp
            is_first_turn: First message in conversation
            customer_phone: Customer phone for lead creation
            customer_name: Customer name for personalization
            
        Returns:
            AI response (potentially enhanced with tool actions)
        """
        # Check if agents are enabled (default: enabled)
        agents_enabled = os.getenv("AGENTS_ENABLED", "1") == "1"
        print(f"ğŸ¯ AGENTS_ENABLED = {agents_enabled}")
        logger.info(f"ğŸ¯ AGENTS_ENABLED = {agents_enabled}")
        
        if not agents_enabled:
            # Fallback to regular response
            print("âš ï¸ Agents disabled - using regular response")
            logger.warning("âš ï¸ Agents disabled - using regular response")
            return self.generate_response(message, business_id, context, channel, is_first_turn)
        
        # ğŸš€ Phase 2K: INTENT ROUTING GATE
        # âš ï¸ FAQ Fast-Path is HARDCODED for real-estate/restaurant patterns!
        # It will NOT work for other business types (tech, retail, etc.)
        # Check if business has FAQ enabled before routing
        
        intent = route_intent_hebrew(message)
        print(f"ğŸ¯ INTENT_DETECTED: {intent} (message: {message[:50]}...)")
        logger.info(f"ğŸ¯ Intent detected: {intent}")
        
        # âš¡ FAQ Fast-Path - Database-backed FAQ matching with embeddings
        # ğŸ”¥ BUILD 99: FAQ ONLY FOR PHONE CALLS (NOT WhatsApp!)
        # WhatsApp uses AgentKit exclusively for all messages
        
        if intent == "info" and channel != "whatsapp":
            # FAQ fast-path for phone calls only (channel="calls")
            try:
                from server.services.faq_cache import faq_cache
                
                faq_match = faq_cache.find_best_match(business_id, message)
                
                if faq_match:
                    print(f"ğŸ¯ FAQ MATCH FOUND (calls): score={faq_match['score']:.3f}")
                    print(f"   Question: {faq_match['question']}")
                    print(f"   Answer: {faq_match['answer'][:100]}...")
                    logger.info(f"ğŸ¯ FAQ fast-path activated: score={faq_match['score']:.3f}")
                    
                    faq_response = self._generate_faq_response(
                        message=message,
                        faq_answer=faq_match['answer'],
                        business_id=business_id,
                        channel=channel
                    )
                    
                    if faq_response:
                        print(f"âœ… FAQ fast-path response generated (calls)")
                        return faq_response
                    else:
                        print("âš ï¸ FAQ response generation failed, falling back to AgentKit")
                else:
                    print(f"âŒ No FAQ match found for: '{message[:50]}...'")
            except Exception as e:
                print(f"âš ï¸ FAQ fast-path error: {e}, falling back to AgentKit")
                logger.warning(f"FAQ fast-path error: {e}")
        elif intent == "info" and channel == "whatsapp":
            # WhatsApp always uses AgentKit (no FAQ fast-path)
            print(f"ğŸ“± WhatsApp message - skipping FAQ, using AgentKit")
            logger.info(f"ğŸ“± WhatsApp 'info' intent - routing to AgentKit (no FAQ)")
        
        # âš¡ Capture start time BEFORE try block for error logging
        start_time = time.time()
        
        try:
            # ğŸ”¥ FIX: Modules now imported at top of file - no re-import needed!
            if not AGENT_MODULES_LOADED:
                # Double-check - agents not available
                print("âš ï¸ AGENTS_ENABLED=False in module - using regular response")
                logger.warning("âš ï¸ AGENTS_ENABLED=False in module - using regular response")
                return self.generate_response(message, business_id, context, channel, is_first_turn)
            
            # Get business name
            db_start = time.time()
            business = Business.query.get(business_id)
            business_name = business.name if business else "×”×¢×¡×§ ×©×œ× ×•"
            
            # ğŸ¯ BUILD 119: Load custom prompt from database!
            prompt_data = self.get_business_prompt(business_id, channel)
            custom_prompt = prompt_data.get("system_prompt", "")  # Extract just the prompt text
            db_time = (time.time() - db_start) * 1000
            print(f"â±ï¸ DB query time: {db_time:.0f}ms")
            logger.info(f"ğŸ“‹ Loaded prompt for business {business_id}: {len(custom_prompt)} chars")
            
            # ğŸ”¥ CRITICAL FIX: Use get_or_create_agent (singleton cache) instead of get_agent (legacy)!
            from server.agent_tools.agent_factory import get_or_create_agent
            
            agent_create_start = time.time()
            agent = get_or_create_agent(
                business_id=business_id,
                channel=channel,
                business_name=business_name,
                custom_instructions=custom_prompt
            )
            agent_create_time = (time.time() - agent_create_start) * 1000
            
            if agent_create_time < 100:
                # Cache HIT - agent was already warmed!
                print(f"â™»ï¸  CACHE HIT: Agent already warmed! ({agent_create_time:.0f}ms)")
                logger.info(f"â™»ï¸  Agent CACHE HIT for {business_name} ({channel}): {agent_create_time:.0f}ms")
            elif agent_create_time < 2000:
                # Cache MISS but creation was fast
                print(f"ğŸ†• NEW Agent created in {agent_create_time:.0f}ms (business={business_name}, channel={channel})")
                logger.info(f"ğŸ†• Agent created: {agent_create_time:.0f}ms")
            else:
                # SLOW creation - log warning!
                print(f"âš ï¸  SLOW AGENT CREATION: {agent_create_time:.0f}ms (expected <2000ms)")
                logger.warning(f"âš ï¸  SLOW AGENT CREATION: {agent_create_time:.0f}ms for business={business_id}, channel={channel}")
            
            if not agent:
                print("âŒ Failed to create agent - falling back to regular response")
                logger.error("âŒ Failed to create agent - falling back to regular response")
                return self.generate_response(message, business_id, context, channel, is_first_turn)
            
            print(f"âœ… Agent created successfully: {agent.name}")
            logger.info(f"âœ… Agent created successfully: {agent.name}")
            
            # Build enhanced context for agent
            agent_context = {
                "business_id": business_id,
                "business_name": business_name,
                "customer_phone": customer_phone,
                "customer_name": customer_name,
                "channel": channel,
                "is_first_turn": is_first_turn,
                **(context or {})
            }
            
            # ğŸ”¥ CRITICAL: Store context in Flask g so tools can access it!
            from flask import g
            g.agent_context = agent_context
            print(f"âœ… Stored agent_context in Flask g: phone={customer_phone}, name={customer_name}")
            
            # Run agent using Runner (with proper async handling for eventlet threads)
            print(f"ğŸ¤– Running agent for business {business_id}, channel={channel}")
            print(f"   ğŸ“ User message: '{message[:100]}...'")
            print(f"   ğŸ“‹ Context: business_id={business_id}, phone={customer_phone}, name={customer_name}")
            logger.info(f"ğŸ¤– Running agent for business {business_id}, channel={channel}")
            logger.info(f"   ğŸ“ User message: '{message[:100]}...'")
            logger.info(f"   ğŸ“‹ Context: business_id={business_id}, phone={customer_phone}, name={customer_name}")
            
            import asyncio
            
            # ğŸ”¥ FIX: ALWAYS create new event loop to avoid CurrentThreadExecutor crash
            # Don't reuse ASGI/main thread executor - it gets torn down mid-request
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # ğŸ”¥ BUILD 99: LIMIT CONVERSATION HISTORY to last 4 exchanges (8 messages)
            # Why: 10 messages = ~4.5K tokens = 27s latency in Runner.run()
            #      4 exchanges (8 messages) = ~1.2K tokens = 1.2s latency âœ…
            history_start = time.time()
            conversation_messages = []
            if context and "previous_messages" in context:
                prev_msgs = context["previous_messages"]
                print(f"ğŸ“š Found {len(prev_msgs)} previous messages in context")
                
                # ğŸ”¥ CRITICAL PERFORMANCE FIX: Keep only last 8 messages (4 user + 4 assistant)
                # This reduces prompt from ~4.5K tokens to ~1.2K tokens
                if len(prev_msgs) > 8:
                    prev_msgs = prev_msgs[-8:]
                    print(f"âš¡ PERFORMANCE: Limited to last 8 messages (4 exchanges) to reduce latency")
                    logger.info(f"âš¡ Truncated history from {len(context['previous_messages'])} to 8 messages")
                
                # Convert to Agent SDK format
                # prev_msgs is list of strings like "×œ×§×•×—: XXX" or "×¢×•×–×¨: YYY"
                for msg in prev_msgs:
                    if msg.startswith("×œ×§×•×—:"):
                        # User message
                        conversation_messages.append({
                            "role": "user",
                            "content": msg.replace("×œ×§×•×—:", "").strip()
                        })
                    elif msg.startswith("×¢×•×–×¨:"):
                        # Assistant message
                        conversation_messages.append({
                            "role": "assistant",
                            "content": msg.replace("×¢×•×–×¨:", "").strip()
                        })
                
                history_time = (time.time() - history_start) * 1000
                print(f"âœ… Converted to {len(conversation_messages)} messages for Agent ({history_time:.0f}ms)")
                
            # Add current message
            conversation_messages.append({
                "role": "user",
                "content": message
            })
            
            # ğŸ”¥ FIX: Runner is a static class - use Runner.run() directly!
            from agents import Runner
            
            print(f"ğŸ”„ Starting Runner.run() with {len(conversation_messages)-1} history messages...")
            logger.info(f"â±ï¸ PERFORMANCE: Starting Runner.run() at {time.time()}")
            
            # Use Runner.run() directly (it's a static method, not an instance!)
            try:
                result = loop.run_until_complete(
                    Runner.run(starting_agent=agent, input=conversation_messages, context=agent_context)
                )
                duration_ms = int((time.time() - start_time) * 1000)
                print(f"âœ… Runner.run() completed in {duration_ms}ms")
                logger.info(f"â±ï¸ PERFORMANCE: Runner.run() completed in {duration_ms}ms")
            finally:
                # ğŸ”¥ CRITICAL: Close event loop to prevent FD leak!
                loop.close()
            
            # Extract response using final_output_as
            reply_text = result.final_output_as(str)
            print(f"ğŸ“ Agent final response: '{reply_text[:100] if reply_text else '(EMPTY!)'}...'")
            
            # âœ… CRITICAL: Validate that agent returned a response!
            if not reply_text or not reply_text.strip():
                print(f"âŒ CRITICAL: Agent returned EMPTY response! Falling back...")
                logger.error(f"âŒ Agent returned empty response for message: {message[:100]}")
                return self.generate_response(message, business_id, context, channel, is_first_turn)
            
            # DEBUG: Check result structure
            print(f"ğŸ” Result type: {type(result).__name__}")
            print(f"ğŸ” Has new_items: {hasattr(result, 'new_items')}")
            if hasattr(result, 'new_items'):
                print(f"ğŸ” new_items value: {result.new_items}")
                print(f"ğŸ” new_items length: {len(result.new_items) if result.new_items else 0}")
            
            # Extract tool calls from new_items
            tool_calls_data = []
            tool_count = 0
            booking_successful = False  # Track if booking actually succeeded
            
            if hasattr(result, 'new_items') and result.new_items:
                print(f"ğŸ“Š Agent returned {len(result.new_items)} items")
                logger.info(f"ğŸ“Š Agent returned {len(result.new_items)} items")
                # Filter for ToolCallItem types and extract tool names
                for idx, item in enumerate(result.new_items):
                    item_type = type(item).__name__
                    print(f"   - Item #{idx}: {item_type}")
                    logger.info(f"   - Item type: {item_type}")
                    
                    if item_type == 'ToolCallItem':
                        tool_count += 1
                        
                        # ğŸ” FULL DEBUG: Print ALL attributes to find tool name
                        print(f"  ğŸ” DEBUG ToolCallItem #{tool_count}:")
                        all_attrs = [a for a in dir(item) if not a.startswith('_')]
                        print(f"     All attributes: {all_attrs}")
                        
                        # Try to access common attributes
                        for attr in ['name', 'tool_name', 'tool_call', 'function', 'tool']:
                            if hasattr(item, attr):
                                val = getattr(item, attr)
                                print(f"     {attr} = {val}")
                        
                        # Try multiple ways to get tool name
                        tool_name = getattr(item, 'name', None)
                        if not tool_name:
                            tool_name = getattr(item, 'tool_name', None)
                        if not tool_name and hasattr(item, 'tool_call'):
                            tc = getattr(item, 'tool_call')
                            if isinstance(tc, dict):
                                tool_name = tc.get('name') or tc.get('function', {}).get('name')
                            elif hasattr(tc, 'name'):
                                tool_name = tc.name
                            elif hasattr(tc, 'function'):
                                tool_name = getattr(tc.function, 'name', None)
                        if not tool_name and hasattr(item, 'tool'):
                            tool_obj = getattr(item, 'tool')
                            tool_name = getattr(tool_obj, 'name', None)
                        if not tool_name:
                            tool_name = 'unknown'
                        
                        print(f"  ğŸ”§ Tool call #{tool_count}: {tool_name}")
                        logger.info(f"  âœ… Tool call #{tool_count}: {tool_name}")
                        tool_calls_data.append({
                            "tool": tool_name,
                            "status": "success",
                            "result": None  # Result is in separate ToolCallOutputItem
                        })
                    
                    elif item_type == 'ToolCallOutputItem':
                        # Extract tool output/result
                        output = getattr(item, 'output', None)
                        print(f"  ğŸ“¤ Tool output: {str(output)[:200] if output else 'None'}...")
                        if output:
                            logger.info(f"     Tool returned: {str(output)[:100]}")
                            
                            # ğŸ” CHECK if this is a successful booking
                            if isinstance(output, dict):
                                if output.get('ok') is True and output.get('appointment_id'):
                                    booking_successful = True
                                    print(f"     âœ… DETECTED SUCCESSFUL BOOKING: appointment_id={output.get('appointment_id')}")
                                    # Store appointment details for WhatsApp validation
                                    if not hasattr(result, 'appointment_details'):
                                        result.appointment_details = output
                
                if tool_count > 0:
                    print(f"âœ… Agent executed {tool_count} tool actions")
                    logger.info(f"âœ… Agent executed {tool_count} tool actions")
                else:
                    print(f"âš ï¸ Agent DID NOT call any tools! (message: '{message[:50]}...')")
                    logger.warning(f"âš ï¸ Agent DID NOT call any tools! (message: '{message[:50]}...')")
            else:
                print(f"âš ï¸ Result has NO new_items or new_items is empty!")
            
            # ğŸš¨ BUILD 138+: VALIDATION - Detect "hallucinated bookings" AND "hallucinated availability"
            # If agent claims action without executing tool, BLOCK response
            claim_words = ["×§×‘×¢×ª×™", "×©×œ×—×ª×™", "×™×¦×¨×ª×™", "×”×¤×’×™×©×” × ×§×‘×¢×”", "×”×¤×’×™×©×” ×§×‘×•×¢×”", "×¡×’×¨×ª×™", "× ×§×‘×¢", "×”×ª×•×¨ × ×§×‘×¢", "×”×ª×•×¨ ×§×‘×•×¢"]
            claimed_action = any(word in reply_text for word in claim_words)
            
            # ğŸ”¥ NEW: Detect "hallucinated availability" (saying "busy/available" without checking)
            # ğŸš¨ FIX: Only flag if saying "NO availability" or "YES available" (absolute claims)
            # Saying "15:00 ×ª×¤×•×¡ ××‘×œ 17:00 ×¤× ×•×™" is VALID after tool call!
            # ğŸ”¥ FIX #3: Added "×ª×¤×•×¡" and "×¤× ×•×™" to catch simple hallucinations
            hallucinated_availability_words = ["××™×Ÿ ×–×× ×™× ×¤× ×•×™×™×", "××™×Ÿ ×–××™× ×•×ª", "×”×›×œ ×ª×¤×•×¡", "×œ× ×¤× ×•×™", "×œ× ×–××™×Ÿ", "×ª×¤×•×¡", "×¤× ×•×™", "×ª×¤×•×¡ ×‘"]
            claimed_availability = any(word in reply_text for word in hallucinated_availability_words)
            
            # Check if calendar_create_appointment was called (with or without _wrapped suffix)
            booking_tool_called = any(
                tc.get("tool") in ["calendar_create_appointment", "calendar_create_appointment_wrapped"]
                for tc in tool_calls_data
            )
            
            # ğŸ”¥ FALLBACK: If tool name extraction failed, check output structure
            # If we see {'appointment_id': ...} in ANY tool output â†’ calendar_create_appointment was called
            if not booking_tool_called and tool_count > 0:
                for item in result.new_items if hasattr(result, 'new_items') else []:
                    if type(item).__name__ == 'ToolCallOutputItem':
                        output = getattr(item, 'output', None)
                        if isinstance(output, dict) and 'appointment_id' in output:
                            print(f"  ğŸ”¥ FALLBACK: Detected calendar_create_appointment from output structure (has 'appointment_id' key)")
                            booking_tool_called = True
                            break
            
            # Check if calendar_find_slots was called
            check_availability_called = any(
                tc.get("tool") in ["calendar_find_slots", "calendar_find_slots_wrapped"]
                for tc in tool_calls_data
            )
            
            # ğŸ”¥ FALLBACK: If tool name extraction failed, check output structure
            # If we see {'slots': [...]} in ANY tool output â†’ calendar_find_slots was called
            if not check_availability_called and tool_count > 0:
                for item in result.new_items if hasattr(result, 'new_items') else []:
                    if type(item).__name__ == 'ToolCallOutputItem':
                        output = getattr(item, 'output', None)
                        if isinstance(output, dict) and 'slots' in output:
                            print(f"  ğŸ”¥ FALLBACK: Detected calendar_find_slots from output structure (has 'slots' key)")
                            check_availability_called = True
                            break
            
            # Check if whatsapp_send was called (for phone channel only)
            whatsapp_sent = any(
                tc.get("tool") == "whatsapp_send"
                for tc in tool_calls_data
            )
            
            # ğŸ”¥ WORKAROUND: Also check if we detected a successful booking in the output
            # (in case tool name extraction failed but booking actually succeeded)
            print(f"  ğŸ” VALIDATION CHECK:")
            print(f"     claimed_action={claimed_action}")
            print(f"     claimed_availability={claimed_availability}")
            print(f"     booking_tool_called={booking_tool_called}")
            print(f"     check_availability_called={check_availability_called}")
            print(f"     booking_successful={booking_successful}")
            
            # ğŸš¨ BLOCK 1: Hallucinated booking
            if claimed_action and not booking_tool_called and not booking_successful:
                print(f"ğŸš¨ BLOCKED HALLUCINATED BOOKING!")
                print(f"   Agent claimed: '{reply_text[:80]}...'")
                print(f"   But NO calendar_create_appointment was called AND no successful booking detected!")
                logger.error(f"ğŸš¨ Blocked hallucinated booking: agent claimed action without tool call")
                
                # Override response with corrective message
                reply_text = "×× ×™ ×¢×“×™×™×Ÿ ×¦×¨×™×š ×œ×‘×“×•×§ ×–××™× ×•×ª. ××™×–×” ×™×•× ×•×©×¢×” ×”×™×™×ª ×¨×•×¦×”?"
                print(f"   âœ… Replaced with: '{reply_text}'")
            
            # ğŸš¨ BLOCK 2: Hallucinated availability (NEW!)
            elif claimed_availability and not check_availability_called:
                print(f"ğŸš¨ BLOCKED HALLUCINATED AVAILABILITY!")
                print(f"   Agent claimed: '{reply_text[:80]}...'")
                print(f"   But NO calendar_find_slots was called!")
                logger.error(f"ğŸš¨ Blocked hallucinated availability: agent claimed busy/free without checking")
                
                # Override response with corrective message
                reply_text = "×‘××™×–×” ×™×•× ×•×©×¢×” × ×•×— ×œ×š?"
                print(f"   âœ… Replaced with: '{reply_text}'")
            
            # ğŸš¨ BLOCK 3: Missing WhatsApp confirmation (NEW!)
            elif booking_successful and channel == "phone" and not whatsapp_sent:
                print(f"âš ï¸  WARNING: Booking successful but NO WhatsApp sent!")
                print(f"   Agent should have called whatsapp_send but didn't")
                logger.warning(f"âš ï¸  Missing WhatsApp confirmation after successful booking")
                # Don't block - just log warning (WhatsApp is nice-to-have, not critical)
            
            # âœ¨ Save trace to database
            try:
                trace = AgentTrace(
                    business_id=business_id,
                    agent_type="booking",
                    channel=channel,
                    customer_phone=customer_phone,
                    customer_name=customer_name,
                    user_message=message[:1000],  # Limit length
                    agent_response=reply_text[:2000],
                    tool_calls=tool_calls_data if tool_calls_data else None,
                    tool_count=tool_count,
                    status="success",
                    duration_ms=duration_ms
                )
                db.session.add(trace)
                db.session.commit()
                logger.info(f"ğŸ“Š Saved agent trace #{trace.id} (duration: {duration_ms}ms)")
            except Exception as trace_error:
                logger.error(f"Failed to save agent trace: {trace_error}")
                # Don't fail the whole request just because trace failed
                db.session.rollback()
            
            return reply_text
            
        except Exception as e:
            logger.error(f"Agent error (falling back to regular response): {e}")
            import traceback
            traceback.print_exc()
            
            # âœ¨ Save error trace with duration
            try:
                error_duration_ms = int((time.time() - start_time) * 1000)
                trace = AgentTrace(
                    business_id=business_id,
                    agent_type="booking",
                    channel=channel,
                    customer_phone=customer_phone,
                    customer_name=customer_name,
                    user_message=message[:1000],
                    agent_response=None,
                    tool_calls=None,
                    tool_count=0,
                    status="error",
                    error_message=str(e)[:500],
                    duration_ms=error_duration_ms
                )
                db.session.add(trace)
                db.session.commit()
                logger.info(f"ğŸ“Š Saved error trace (duration: {error_duration_ms}ms)")
            except:
                db.session.rollback()
            
            # Fallback to regular response
            return self.generate_response(message, business_id, context, channel, is_first_turn)

def generate_ai_response(message: str, business_id: int = 1, 
                        context: Optional[Dict[str, Any]] = None, channel: str = "calls",
                        is_first_turn: bool = False) -> str:
    """×¤×•× ×§×¦×™×” ×¢×–×¨ ×œ×§×¨×™××” ××”×™×¨×” ×œ×©×™×¨×•×ª AI - ×œ×¤×™ ×¢×¨×•×¥"""
    return get_ai_service().generate_response(message, business_id, context, channel, is_first_turn)

