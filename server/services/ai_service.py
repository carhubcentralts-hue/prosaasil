"""
AI Service - Unified OpenAI Service for All Communication Channels
×©×™×¨×•×ª AI ×××•×—×“ - ××—×‘×¨ ×¤×¨×•××¤×˜×™× ×“×™× ××™×™× ××”××¡×“ × ×ª×•× ×™× ×¢× OpenAI
âœ¨ BUILD 119: AgentKit integration for real actions (appointments, leads, WhatsApp)
ğŸš€ Phase 2K: Fast Intent Router - run AgentKit only for bookings (â‰¤2s target)
"""
import os
import logging
import time
import re
from typing import Dict, Any, Optional, List, Literal
from openai import OpenAI
from server.models_sql import BusinessSettings, PromptRevisions, Business, AgentTrace
from server.db import db
from datetime import datetime

# ğŸ”¥ CRITICAL: Import agent modules at TOP of file (not inside function!)
# This prevents re-importing on every call and speeds up response time
try:
    from server.agent_tools import get_agent, AGENTS_ENABLED
    from agents import Runner
    AGENT_MODULES_LOADED = True
    logger_temp = logging.getLogger(__name__)
    logger_temp.info("âœ… Agent modules pre-loaded at module level")
except ImportError as e:
    AGENT_MODULES_LOADED = False
    AGENTS_ENABLED = False
    logger_temp = logging.getLogger(__name__)
    logger_temp.warning(f"âš ï¸ Agent modules not available: {e}")

logger = logging.getLogger(__name__)

# Global AI service instance for cache sharing
_global_ai_service = None

# ğŸš€ Phase 2K: Intent Router Configuration
# âœ… ENABLED: FAQ fast-path with improved context and token limits
AGENTKIT_BOOKING_ONLY = os.getenv("AGENTKIT_BOOKING_ONLY", "1") == "1"  # Default ON
FAST_PATH_ENABLED = os.getenv("FAST_PATH_ENABLED", "1") == "1"  # Default ON

def route_intent_hebrew(text: str) -> Literal["book", "reschedule", "cancel", "info", "whatsapp", "human", "other"]:
    """
    ğŸš€ Fast Hebrew intent detection - NO LLM!
    Returns intent category for routing decisions.
    Target: <10ms for classification
    
    Priority order: reschedule > cancel > whatsapp > human > info > book > other
    """
    text_lower = text.lower().strip()
    
    # ğŸ”„ RESCHEDULE: Change appointment (CHECK FIRST - more specific)
    reschedule_patterns = [
        r'×œ×”×–×™×–|×œ×”×§×“×™×|×œ×“×—×•×ª|×œ×”×—×œ×™×£.*×©×¢×”|×œ×©× ×•×ª.*×ª×•×¨',
        r'××¤×©×¨.*×œ×©× ×•×ª|××¤×©×¨.*×œ×”×–×™×–'
    ]
    
    # âŒ CANCEL: Cancel appointment (CHECK SECOND - specific action)
    cancel_patterns = [
        r'×œ×‘×˜×œ|×ª×‘×˜×œ|×‘×™×˜×•×œ.*×ª×•×¨|×œ×.*××’×™×¢',
        r'×× ×™.*×œ×.*×™×›×•×œ|××™×Ÿ.*××¤×©×¨×•×ª'
    ]
    
    # ğŸ“± WHATSAPP: Send info via WhatsApp (CHECK THIRD - clear intent)
    whatsapp_patterns = [
        r'×©×œ×—.*×œ×™|×ª×©×œ×—.*×œ×™',
        r'×•×•××˜×¡××¤|whatsapp',
        r'×”×•×“×¢×”|××¡×¨×•×Ÿ'
    ]
    
    # ğŸ‘¤ HUMAN: Transfer to agent (CHECK FOURTH - escalation)
    human_patterns = [
        r'× ×¦×™×’|×‘×Ÿ.*××“×|××™×©.*×××™×ª×™',
        r'×œ×“×‘×¨.*×¢×|×œ×”×¢×‘×™×¨'
    ]
    
    # â„¹ï¸ INFO: General information (CHECK AFTER booking pre-check!)
    # ğŸ”¥ TIGHTENED: These patterns now only match if NO booking verbs present
    info_patterns = [
        # ğŸ”¥ CRITICAL: Question words â†’ info (××œ×” ×©××œ×•×ª ××™×“×¢!)
        # But: "××ª×™ ××¤×©×¨ ×œ×§×‘×•×¢?" â†’ book (caught by pre-check)
        r'^(××”|××™×–×”|××™×–×•|×›××”|×œ××”|××“×•×¢|××™×š|×”×™×›×Ÿ|××ª×™)\s',  # Start with question word
        
        # ğŸ”¥ CRITICAL FIX: "×™×©..." questions - ONLY amenities (not rooms/services)
        r'×™×©\s+(××•×›×œ|×©×ª×™×™?×”|×ª×¤×¨×™×˜|×× ×•×ª|××œ×›×•×”×•×œ|×‘×¨|××©×§××•×ª|×§×¤×”|××–×•×Ÿ)',
        r'×™×©\s+(×—× ×™×”|×—× ×™×™×”|×’×™×©×”|××™×–×•×’|wifi|××™× ×˜×¨× ×˜|××¢×œ×™×ª)',
        r'×™×©\s+×œ×›×\s+(××•×›×œ|×©×ª×™×™?×”|×ª×¤×¨×™×˜|×—× ×™×”|wifi)',
        r'××”\s+×™×©\s+(×œ××›×•×œ|×œ×©×ª×•×ª|×‘×ª×¤×¨×™×˜)',
        
        # Pricing (standalone - not with booking verbs)
        r'×›××”.*×¢×•×œ×”|××—×™×¨(?!.*×œ×§×‘×•×¢)|×¢×œ×•×ª|×ª×©×œ×•×(?!.*×ª×•×¨)',
        
        # Hours
        r'×©×¢×•×ª.*×¤×ª×™×—×”|××ª×™.*×¤×ª×•×—|×©×¢×•×ª.*×¢×‘×•×“×”|××”.*×©×¢×•×ª',
        
        # Amenities & Services - REMOVED generic "×—×“×¨" patterns!
        r'×›×©×¨|×›×©×¨×•×ª',
        r'×’×•×“×œ.*×—×“×¨|×›××”.*×× ×©×™×|×›××”.*××©×ª×ª×¤×™×',
        
        # Menu/food (standalone)
        r'\b(×ª×¤×¨×™×˜|×× ×•×ª|××©×§××•×ª)\b',
    ]
    
    # ğŸ“… BOOK: Scheduling keywords (CHECK LAST - most generic)
    # ğŸ”¥ FIX: Require scheduling VERB + time/day to avoid false positives
    book_patterns = [
        r'×œ×§×‘×•×¢|×ª×™××•×|×œ×”×–××™×Ÿ|×¨×•×¦×”.*×ª×•×¨|××¤×©×¨.*×ª×•×¨',  # Explicit booking verbs
        r'×™×©.*××§×•×|×™×©.*×–××Ÿ|×™×©.*×¤× ×•×™|×¤× ×•×™.*×œ',  # Availability questions
        r'(×œ×‘×•×|×œ×”×’×™×¢).*(××—×¨|×”×™×•×|×‘-\d+|×‘×©×¢×”)',  # "×œ×‘×•× ××—×¨"
        r'(×¨×•×¦×”|×¦×¨×™×š).*(×ª×•×¨|×¤×’×™×©×”|×ª×™××•×)',  # "×¨×•×¦×” ×ª×•×¨"
    ]
    
    # ğŸ”¥ FIX: Check patterns in CORRECT priority order
    # Most specific first, most generic last
    
    for pattern in reschedule_patterns:
        if re.search(pattern, text_lower):
            return "reschedule"
    
    for pattern in cancel_patterns:
        if re.search(pattern, text_lower):
            return "cancel"
    
    for pattern in whatsapp_patterns:
        if re.search(pattern, text_lower):
            return "whatsapp"
    
    for pattern in human_patterns:
        if re.search(pattern, text_lower):
            return "human"
    
    # ğŸš¨ CRITICAL PRE-CHECK: Booking verbs + time/day â†’ BOOK (before info check!)
    # This fixes: "××¤×©×¨ ×œ×§×‘×•×¢ ×—×“×¨ ×§×¨×™×•×§×™ ×œ××—×¨" â†’ book (not info)
    booking_verbs = r'(×œ×§×‘×•×¢|×œ×ª××|×œ×”×–××™×Ÿ|××¤×©×¨.*×ª×•×¨|×¨×•×¦×”.*×ª×•×¨|×¦×¨×™×š.*×ª×•×¨)'
    time_day_terms = r'(××—×¨|×”×™×•×|××—×¨×ª×™×™×|×”×©×‘×•×¢|×”×—×•×“×©|×‘-\d+|×‘×©×¢×”|×‘×™×•×|×‘×©× ×™|×‘×©×œ×™×©×™|×‘×¨×‘×™×¢×™|×‘×—××™×©×™|×‘×©×™×©×™|×‘×©×‘×ª|×‘×¨××©×•×Ÿ)'
    availability_terms = r'(×¤× ×•×™|×–××™×Ÿ|×–××Ÿ|××§×•×|×ª×•×¨|×¤×’×™×©×”)'
    
    # If booking verb + (time/day OR availability) â†’ it's a booking request!
    if re.search(booking_verbs, text_lower):
        if re.search(time_day_terms, text_lower) or re.search(availability_terms, text_lower):
            print(f"ğŸ¯ BOOKING_PRE_CHECK: Detected booking verb + time/availability")
            return "book"
    
    # ğŸ”¥ CHECK INFO (after booking pre-check!)
    for pattern in info_patterns:
        if re.search(pattern, text_lower):
            print(f"ğŸ¯ INTENT_MATCH: pattern='{pattern}' matched in '{text_lower[:50]}'")
            return "info"
    
    # Only check book patterns AFTER info has been ruled out
    for pattern in book_patterns:
        if re.search(pattern, text_lower):
            return "book"
    
    # ğŸ”¥ FIX: Default to "other" (Agent) for unmatched questions
    # Quality/experience questions ("×”××•×›×œ ×§×©×”?") need full Agent conversation handling
    # Only explicit info patterns should trigger FAQ fast-path
    return "other"  # Agent handles ambiguous/quality questions correctly

def extract_time_hebrew(text: str) -> Optional[Dict[str, Any]]:
    """
    ğŸš€ Extract explicit date/time from Hebrew text
    Returns: {"day": "tomorrow", "time": "14:00"} or None
    """
    text_lower = text.lower()
    result = {}
    
    # Day extraction
    day_map = {
        "××—×¨": "tomorrow",
        "×”×™×•×": "today",
        "×¨××©×•×Ÿ": "sunday",
        "×©× ×™": "monday",
        "×©×œ×™×©×™": "tuesday",
        "×¨×‘×™×¢×™": "wednesday",
        "×—××™×©×™": "thursday",
        "×©×™×©×™": "friday",
        "×©×‘×ª": "saturday"
    }
    
    for heb, eng in day_map.items():
        if heb in text_lower:
            result["day"] = eng
            break
    
    # Time extraction
    # Format: "14:00", "2:30", "×‘×©×¢×” 12", "×‘-3"
    time_patterns = [
        r'(\d{1,2}):(\d{2})',  # 14:00, 2:30
        r'×‘×©×¢×”?\s*(\d{1,2})',  # ×‘×©×¢×” 12
        r'×‘-(\d{1,2})',         # ×‘-3
        r'(\d{1,2})\s*(×‘×‘×•×§×¨|×‘×¦×”×¨×™×™×|××—×”×´×¦|×‘×¢×¨×‘)',  # 3 ×‘×‘×•×§×¨
    ]
    
    for pattern in time_patterns:
        match = re.search(pattern, text_lower)
        if match:
            hour = int(match.group(1))
            
            # Adjust for AM/PM context
            if len(match.groups()) > 1:
                context = match.group(2) if match.group(2) else ""
                if "×‘×‘×•×§×¨" in context and hour <= 8:
                    hour = hour  # Keep as is
                elif hour <= 8:  # Assume PM for 1-8 without context
                    hour += 12
            
            result["time"] = f"{hour:02d}:00"
            break
    
    return result if result else None

def get_ai_service():
    """Get or create global AI service instance"""
    global _global_ai_service
    if _global_ai_service is None:
        _global_ai_service = AIService()
        # âš¡ CRITICAL: Warmup cache at startup
        _warmup_ai_cache(_global_ai_service)
    return _global_ai_service

def _warmup_ai_cache(service: 'AIService'):
    """âš¡ Preload cache for ALL active businesses to prevent first-turn latency"""
    try:
        import time
        from server.models import Business
        from server.app_factory import get_process_app
        
        start = time.time()
        
        # ğŸ”¥ MULTI-TENANT: Warmup ALL active businesses (up to 10)
        app = get_process_app()
        with app.app_context():
            businesses = Business.query.filter_by(is_active=True).limit(10).all()
            
            if not businesses:
                logger.warning("âš ï¸ WARMUP: No active businesses found")
                return
            
            logger.info(f"ğŸ”¥ AI_CACHE_WARMUP: Found {len(businesses)} active businesses")
            
            for business in businesses:
                business_id = business.id
                for channel in ['calls', 'whatsapp']:
                    try:
                        service.get_business_prompt(business_id, channel)
                        logger.info(f"âœ… WARMUP: Preloaded business {business_id} ({business.name}) {channel}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ WARMUP failed for business {business_id} {channel}: {e}")
            
            warmup_time = time.time() - start
            logger.info(f"âœ… AI_CACHE_WARMUP: Completed {len(businesses)} businesses in {warmup_time:.3f}s")
    except Exception as e:
        logger.error(f"âŒ AI cache warmup failed: {e}")

def invalidate_business_cache(business_id: int):
    """ğŸ”¥ CRITICAL: Invalidate cache for business - called after prompt updates"""
    service = get_ai_service()
    
    # 1. Clear prompt cache (AIService)
    cache_keys_to_remove = [
        f"business_{business_id}_calls",
        f"business_{business_id}_whatsapp"
    ]
    for key in cache_keys_to_remove:
        if key in service._cache:
            del service._cache[key]
            logger.info(f"âœ… Prompt cache invalidated: {key}")
    
    # 2. ğŸ”¥ NEW: Clear agent cache (agent_factory)
    try:
        from server.agent_tools.agent_factory import invalidate_agent_cache
        invalidate_agent_cache(business_id)
        logger.info(f"âœ… Agent cache invalidated for business {business_id}")
    except Exception as e:
        logger.error(f"âš ï¸ Failed to invalidate agent cache: {e}")

class AIService:
    """×× ×’× ×•×Ÿ AI ××¨×›×–×™ ×©×˜×•×¢×Ÿ ×¤×¨×•××¤×˜×™× ××”××¡×“ × ×ª×•× ×™× ×•××—×‘×¨ ×¢× OpenAI"""
    
    def __init__(self):
        # âš¡ RELIABLE OpenAI client with production timeout
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            timeout=2.5  # ğŸ”¥ REDUCED: 2.5s timeout for faster real-time conversations (was 3.5s)
        )
        self._cache = {}  # ×§××© ×¤×¨×•××¤×˜×™× ×œ×‘×™×¦×•×¢×™×
        self._cache_timeout = 300  # âš¡ 5 ×“×§×•×ª - ××¡×¤×™×§ ××¨×•×š ×œ×©×™×—×” ×©×œ××”
        
    def get_business_prompt(self, business_id: int, channel: str = "calls") -> Dict[str, Any]:
        """×˜×¢×™× ×ª ×¤×¨×•××¤×˜ ×¢×¡×§ ××”××¡×“ × ×ª×•× ×™× ×¢× ×§××© - ×œ×¤×™ ×¢×¨×•×¥ (calls/whatsapp)"""
        cache_key = f"business_{business_id}_{channel}"
        now = datetime.now().timestamp()
        
        # ×‘×“×™×§×ª ×§××©
        if cache_key in self._cache:
            cached_data, timestamp = self._cache[cache_key]
            if now - timestamp < self._cache_timeout:
                logger.info(f"âœ… CACHE_HIT: business {business_id} {channel}")
                return cached_data
        
        try:
            # âš¡ CRITICAL: Measure DB query time
            import time
            db_start = time.time()
            
            # ğŸ”¥ BUILD 186 FIX: Handle missing columns gracefully
            settings = None
            try:
                settings = BusinessSettings.query.filter_by(tenant_id=business_id).first()
            except Exception as db_err:
                logger.warning(f"âš ï¸ Could not load BusinessSettings for {business_id} (DB schema issue): {db_err}")
            
            business = Business.query.get(business_id)
            
            db_time = time.time() - db_start
            logger.info(f"ğŸ“Š DB_QUERY: {db_time:.3f}s for business {business_id}")
            
            # âœ… ×©× ×¢×¡×§ ×œ×©×™××•×© ×‘-placeholders
            business_name = business.name if business else "×”×¢×¡×§ ×©×œ× ×•"
            
            # ×‘×—×™×¨×ª ×¤×¨×•××¤×˜ ×—×›××” - ×¢× fallback ×œ-business.system_prompt
            system_prompt = ""
            if settings and settings.ai_prompt and settings.ai_prompt.strip():
                # ×™×© ×¤×¨×•××¤×˜ ×‘-settings - ×ª××™×“ ×ª×©×ª××© ×‘×•! (×œ×œ× ×‘×“×™×§×ª ××•×¨×š)
                import json
                try:
                    # × ×¡×™×•×Ÿ ×œ×¤×¨×•×¡ ×›-JSON (×¤×•×¨××˜ ×—×“×© ×¢× calls/whatsapp)
                    if settings.ai_prompt.strip().startswith('{'):
                        prompt_obj = json.loads(settings.ai_prompt)
                        # ×‘×—×™×¨×ª ×”×¤×¨×•××¤×˜ ×”× ×›×•×Ÿ ×œ×¤×™ channel
                        # âœ… STRICT: Require channel-specific key
                        if channel in prompt_obj:
                            system_prompt = prompt_obj[channel]
                            logger.info(f"âœ… Using {channel} prompt for business {business_id} from settings")
                        else:
                            # âš ï¸ STRICT MODE: Missing channel key
                            logger.error(f"âŒ Missing '{channel}' key in ai_prompt JSON for business {business_id}. Available keys: {list(prompt_obj.keys())}")
                            # Use default prompt as fallback but log error
                            system_prompt = self._get_default_hebrew_prompt(business_name, channel)
                            logger.warning(f"âš ï¸ Using default prompt due to missing '{channel}' key")
                        
                        logger.info(f"ğŸ” DEBUG: Loaded prompt starts with: {system_prompt[:100]}...")
                    else:
                        # ×¤×¨×•××¤×˜ ×˜×§×¡×˜ ×¤×©×•×˜ (legacy)
                        system_prompt = settings.ai_prompt
                        logger.info(f"âœ… Using legacy text prompt for business {business_id}")
                except json.JSONDecodeError:
                    # ×× ×–×” ×œ× JSON ×ª×§×™×Ÿ, ×”×©×ª××© ×‘×–×” ×›×˜×§×¡×˜
                    system_prompt = settings.ai_prompt
                    logger.info(f"âœ… Using non-JSON prompt for business {business_id}")
            elif business and business.system_prompt and business.system_prompt.strip():
                # fallback ×œ×¤×¨×•××¤×˜ ×”××œ× ××˜×‘×œ×ª business
                system_prompt = business.system_prompt
                logger.info(f"âœ… Using fallback prompt from business.system_prompt for {business_id}")
            else:
                # fallback ××—×¨×•×Ÿ ×œ×¤×¨×•××¤×˜ ×‘×¨×™×¨×ª ××—×“×œ
                system_prompt = self._get_default_hebrew_prompt(business_name, channel)
                logger.info(f"âš ï¸ Using default prompt for business {business_id} - no custom prompt found")
            
            # âœ… ×”×—×œ×¤×ª placeholders ×“×™× ××™×™× ×‘×¤×¨×•××¤×˜
            system_prompt = system_prompt.replace("{{business_name}}", business_name)
            system_prompt = system_prompt.replace("{{BUSINESS_NAME}}", business_name)
            logger.info(f"âœ… Replaced {{{{business_name}}}} with '{business_name}'")
            
            # ğŸ”¥ ADD DYNAMIC POLICY TO PROMPT (hours, slots, etc.) 
            try:
                from server.policy.business_policy import get_business_policy
                policy = get_business_policy(business_id, prompt_text=None)  # Don't pass ai_prompt to avoid circular parsing
                
                # Build hours description
                from server.services.realtime_prompt_builder import _build_hours_description, _build_slot_description
                hours_desc = _build_hours_description(policy)
                slot_desc = _build_slot_description(policy.slot_size_min)
                
                # Build min notice description
                min_notice_desc = ""
                if policy.min_notice_min > 0:
                    min_notice_hours = policy.min_notice_min // 60
                    if min_notice_hours > 0:
                        min_notice_desc = f"\n- ×“×•×¨×©×™× ×”×–×× ×” ××¨××© ×©×œ ×œ×¤×—×•×ª {min_notice_hours} ×©×¢×•×ª."
                    else:
                        min_notice_desc = f"\n- ×“×•×¨×©×™× ×”×–×× ×” ××¨××© ×©×œ ×œ×¤×—×•×ª {policy.min_notice_min} ×“×§×•×ª."
                
                # Append policy info to system prompt
                policy_info = f"\n\nğŸ“… ×”×’×“×¨×•×ª ×ª×•×¨×™×:\n{hours_desc}\n- {slot_desc}{min_notice_desc}"
                system_prompt += policy_info
                
                logger.info(f"âœ… Added dynamic policy to {channel} prompt: {len(policy_info)} chars")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to add dynamic policy to prompt: {e}")
            
            # âš¡ BUILD 118: Warn if prompt is too long (causes OpenAI timeouts)
            if len(system_prompt) > 3000:
                logger.warning(f"âš ï¸ PROMPT_TOO_LONG: {len(system_prompt)} chars (recommended: <3000) - may cause OpenAI timeouts!")
            else:
                logger.info(f"âœ… Prompt length OK: {len(system_prompt)} chars")
            
            if not settings:
                # âš¡ BUILD 117: INCREASED - allow complete sentences without truncation
                prompt_data = {
                    "system_prompt": system_prompt,
                    "business_name": business_name,  # ğŸ”¥ FIX: Include business name for FAQ handler
                    "model": "gpt-4o-mini",  # Fast model
                    "max_tokens": 350,  # âš¡ BUILD 117: 350 tokens for COMPLETE sentences (no mid-sentence cuts!)
                    "temperature": 0.3  # Balanced temperature for natural responses
                }
            else:
                prompt_data = {
                    "system_prompt": system_prompt,
                    "business_name": business_name,  # ğŸ”¥ FIX: Include business name for FAQ handler
                    "model": settings.model,
                    "max_tokens": min(settings.max_tokens, 350),  # âš¡ BUILD 117: Cap at 350 for complete sentences
                    "temperature": min(settings.temperature, 0.4)  # Balanced temperature
                }
            
            # ×©××™×¨×” ×‘×§××©
            self._cache[cache_key] = (prompt_data, now)
            return prompt_data
            
        except Exception as e:
            logger.error(f"Error loading business prompt {business_id}: {e}")
            # âš¡ FAST fallback - ×˜×¢×™× ×ª ×©× ×¢×¡×§ ××”-DB
            try:
                business = Business.query.get(business_id)
                business_name = business.name if business else "×”×¢×¡×§ ×©×œ× ×•"
            except:
                business_name = "×”×¢×¡×§ ×©×œ× ×•"
            
            return {
                "system_prompt": self._get_default_hebrew_prompt(business_name, channel),
                "business_name": business_name,  # ğŸ”¥ FIX: Include business name for FAQ handler
                "model": "gpt-4o-mini",
                "max_tokens": 350,  # âš¡ BUILD 117: 350 tokens for COMPLETE sentences
                "temperature": 0.3  # Balanced
            }
    
    def _get_default_hebrew_prompt(self, business_name: str = "×”×¢×¡×§ ×©×œ× ×•", channel: str = "calls") -> str:
        """×¤×¨×•××¤×˜ ×‘×¨×™×¨×ª ××—×“×œ ×‘×¢×‘×¨×™×ª - ×›×œ×œ×™ ×œ×›×œ ×¡×•×’ ×¢×¡×§ - âœ… ×‘×œ×™ ×”× ×—×•×ª ×¢×œ ×ª×—×•× ×”×¢×™×¡×•×§!"""
        if channel == "whatsapp":
            return f"""××ª×” ×”×¢×•×–×¨ ×”×“×™×’×™×˜×œ×™ ×©×œ {business_name} ×‘-WhatsApp.

×›×œ×œ×™× ×—×©×•×‘×™×:
- ×ª×¢× ×” ×‘×¢×‘×¨×™×ª. ×›×©×”×œ×§×•×— ××‘×§×© ××™×“×¢ ××¤×•×¨×˜ - ×ª×¢× ×” ×‘×¦×•×¨×” ××§×™×¤×” ×•××œ××” ×œ×œ× ×§×™×¦×•×¨
- ×ª×”×™×” ×—×, ××“×™×‘ ×•×™×“×™×“×•×ª×™ ×‘×¡×’× ×•×Ÿ WhatsApp
- ×”×‘×Ÿ ××” ×”×œ×§×•×— ×¦×¨×™×š ×•×¢×–×•×¨ ×œ×• ×‘×”×ª××
- ×›×©××ª×” ××–×›×™×¨ ××—×™×¨×™×/×ª×§×¦×™×‘ - ×ª××™×“ ×¦×™×™×Ÿ "××™×œ×™×•×Ÿ", "××œ×£" ×•×›×•' (×œ× ×¨×§ ××¡×¤×¨×™×!)
- ×ª×¦×™×¢ ×œ×§×‘×•×¢ ×¤×’×™×©×” ××• ×©×™×—×” ×›×©××ª××™×
- âš ï¸ ××œ ×ª×—×–×•×¨ ×¢×œ ×©××š ×‘×›×œ ××©×¤×˜! ×–×” ××¢×¦×‘×Ÿ ×•×œ× ×˜×‘×¢×™
- ×“×‘×¨ ×™×©×¨ ×œ×¢× ×™×™×Ÿ ×‘×œ×™ ×œ×”×¦×™×’ ××ª ×¢×¦××š ×›×œ ×¤×¢× ××—×“×©
- âš ï¸ ×—×©×•×‘ ×××•×“: ×¡×™×™× ×›×œ ××©×¤×˜ ×©×”×ª×—×œ×ª! ×œ×¢×•×œ× ××œ ×ª×—×ª×•×š ×ª×©×•×‘×” ×‘×××¦×¢ ××©×¤×˜

**×›×©×œ×§×•×— ××¡×›×™× ×œ×–××Ÿ ×¤×’×™×©×”:**
ğŸ¯ **×—×–×•×¨ ×¢×œ ×”×–××Ÿ ×”××“×•×™×§ ×©×”×œ×§×•×— ×××¨!**
×“×•×’×××•×ª:
- ×œ×§×•×—: "××—×¨ ×‘-10" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 10:00."
- ×œ×§×•×—: "××—×¨ ×‘-15" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 15:00."
âš ï¸ **××œ ×ª×©× ×” ××ª ×”×©×¢×” - ×—×–×•×¨ ×¢×œ ××” ×©×”×œ×§×•×— ×××¨!**

×ª×¤×§×™×“×š: ×œ×¢×–×•×¨ ×œ×œ×§×•×— ×‘××” ×©×”×•× ×¦×¨×™×š ×‘×¦×•×¨×” ××§×¦×•×¢×™×ª ×•××“×™×‘×”."""
        else:
            # âœ¨ Calls - ×¤×¨×•××¤×˜ ××¤×•×¨×˜ ×œ×©×™×—×•×ª ×–×•×¨××•×ª ×•×˜×‘×¢×™×•×ª - ×›×œ×œ×™ ×œ×›×œ ×¡×•×’ ×¢×¡×§
            return f"""××ª×” ×”×¢×•×–×¨ ×”×“×™×’×™×˜×œ×™ ×©×œ {business_name}. ××ª×” ×›××Ÿ ×›×“×™ ×œ×¢×–×•×¨ ×œ×œ×§×•×—×•×ª ×‘×¦×•×¨×” ××§×¦×•×¢×™×ª ×•××“×™×‘×”.

×”×ª× ×”×œ×•×ª ×‘×©×™×—×”:
â€¢ ×“×‘×¨ ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“, ×‘×¦×•×¨×” ×˜×‘×¢×™×ª ×•×–×•×¨××ª ×›××• ×©×™×—×” ×¨×’×™×œ×” ×‘×˜×œ×¤×•×Ÿ
â€¢ ×”×™×” ×—×, ×™×“×™×“×•×ª×™ ×•××§×¦×•×¢×™ - ×›××• × ×¦×™×’ ×©×™×¨×•×ª ×× ×•×¡×”
â€¢ ×ª×©×•×‘×•×ª ×§×¦×¨×•×ª - 2-3 ××©×¤×˜×™× ×‘×›×œ ×ª×’×•×‘×” (×¢×“ 200 ××™×œ×™×)
â€¢ ×“×‘×¨ ×™×©×™×¨×•×ª ×œ×¢× ×™×™×Ÿ, ×‘×œ×™ ××™×œ×•×™ ××• ×¡×™×¤×•×¨×™× ××¨×•×›×™×
â€¢ âš ï¸ ×—×©×•×‘ ×××•×“: ××œ ×ª×—×–×•×¨ ×¢×œ ×©××š ×‘×›×œ ××©×¤×˜! ×–×” ×œ× ×˜×‘×¢×™ ×•××¢×¦×‘×Ÿ
â€¢ ×”×¦×’ ××ª ×¢×¦××š ×¨×§ ×‘×‘×¨×›×” ×”×¨××©×•× ×”, ××—×¨ ×›×š ×“×‘×¨ ×™×©×¨ ×œ×¢× ×™×™×Ÿ

××™×¡×•×£ ××™×“×¢ ×—×›×:
â€¢ ×”×§×©×‘ ×œ××” ×©×”×œ×§×•×— ×¦×¨×™×š ×•×©××œ ×©××œ×•×ª ×”×‘×”×¨×” ×œ×¤×™ ×”×¦×•×¨×š
â€¢ ×©××œ ×©××œ×” ××—×ª ×‘×›×œ ×¤×¢× - ×œ× ×œ×”×¦×™×£ ××ª ×”×œ×§×•×—
â€¢ ×›×©××ª××™× - ××¡×•×£ ×©× ×•×¤×¨×˜×™ ×§×©×¨ ×œ×—×–×¨×”
â€¢ ×›×©××ª×” ××–×›×™×¨ ××—×™×¨×™× - ×ª××™×“ ×¦×™×™×Ÿ ××ª ×¡×“×¨ ×”×’×•×“×œ (××œ×£/××™×œ×™×•×Ÿ)

××ª×™ ×œ×§×‘×•×¢ ×¤×’×™×©×”:
×›×©×™×© ×œ×š ××¡×¤×™×§ ××™×“×¢ â†’ ×”×¦×¢ ×œ×§×‘×•×¢ ×¤×’×™×©×” ××• ×©×™×—×ª ×”××©×š

âš ï¸ **×—×©×•×‘ ×××•×“ - ×›×©×”×œ×§×•×— ××¡×›×™× ×œ×–××Ÿ:**
ğŸ¯ **×—×•×§ ×‘×¨×–×œ: ×—×–×•×¨ ×¢×œ ×”×–××Ÿ ×”××“×•×™×§ ×©×”×œ×§×•×— ×××¨ - ×œ× ×œ×”××¦×™× ×©×¢×•×ª!**

×›×©×”×œ×§×•×— ××•××¨ ×–××Ÿ ×¡×¤×¦×™×¤×™:
- ×œ×§×•×—: "××—×¨ ×‘-10" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 10:00."
- ×œ×§×•×—: "××—×¨ ×‘-16" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 16:00."

×›×©×”×œ×§×•×— ××•××¨ ×–××Ÿ ×›×œ×œ×™ (×‘×•×§×¨/×¦×”×¨×™×™×/××—×”"×¦):
- ×œ×§×•×—: "××—×¨ ×‘×‘×•×§×¨" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ××—×¨ ×‘×©×¢×” 10:00."
- ×œ×§×•×—: "×™×•× ×©×œ×™×©×™ ××—×¨ ×”×¦×”×¨×™×™×" â†’ ××ª×”: "××¢×•×œ×”! × ×§×‘×¢ ×œ×š ×¤×’×™×©×” ×œ×™×•× ×©×œ×™×©×™ ×‘×©×¢×” 14:00."

âš ï¸ **××œ ×ª×©× ×” ××ª ×”×©×¢×” ×©×”×œ×§×•×— ×××¨! ×× ×”×•× ×××¨ 16 - ×ª××©×¨ 16, ×œ× 10!**

×—×©×•×‘: ××œ ×ª××¦×™× ××™×“×¢! ×× ×œ× ×™×•×“×¢ ××©×”×• - ×”×¤× ×” ×œ× ×¦×™×’ ×× ×•×©×™. ×× ×”×œ×§×•×— ×¢×¦×‘× ×™ ××• ××ª×œ×•× ×Ÿ - ×”×™×” ×××¤×˜×™ ×•×”×¦×¢ ×“×™×‘×•×¨ ×¢× ×× ×”×œ."""

    def generate_response(self, message: str, business_id: int = None, context: Optional[Dict[str, Any]] = None, channel: str = "calls", is_first_turn: bool = False) -> str:
        """×™×¦×™×¨×ª ×ª×’×•×‘×” ××¤×¨×•××¤×˜ ×“×™× ××™ + ×”×§×©×¨ - ×œ×¤×™ ×¢×¨×•×¥ (calls/whatsapp)"""
        try:
            # ×˜×¢×™× ×ª ×¤×¨×•××¤×˜ ×¢×¡×§ ×œ×¤×™ ×¢×¨×•×¥
            prompt_data = self.get_business_prompt(business_id, channel)
            
            # âš¡ BUILD 117: First turn - NO SPECIAL LIMIT! Let AI finish complete sentences
            # User requirement: "×× ×”×™× ×¦×¨×™×›×” ×œ×”×¡×‘×™×¨ ×“×§×” ×©×ª×¡×‘×™×¨ ×“×§×”" - let it speak as long as needed
            if is_first_turn:
                # Don't reduce max_tokens for first turn - keep the default 350 for complete sentences
                logger.info(f"ğŸ¯ First turn - using full {prompt_data['max_tokens']} tokens for complete sentences")
            
            # ×‘× ×™×™×ª ×”×•×“×¢×•×ª
            messages: List[Dict[str, str]] = [
                {"role": "system", "content": prompt_data["system_prompt"]}
            ]
            
            # âœ… ×”×•×¡×¤×ª ×–××™× ×•×ª ×œ×•×— ×©× ×” (×¨×§ ×œ-WhatsApp - ×œ× ×œ×˜×œ×¤×•×Ÿ ×‘×’×œ×œ latency!)
            if channel == "whatsapp":
                calendar_info = self._get_calendar_availability(business_id)
                if calendar_info:
                    messages.append({
                        "role": "system",
                        "content": f"ğŸ“… ×œ×•×— ×©× ×”:\n{calendar_info}\n×›×©×”×œ×§×•×— ××•×›×Ÿ ×œ×¤×’×™×©×”, ×”×¦×¢ ×ª××¨×™×›×™× ×¤× ×•×™×™× ××”×¨×©×™××” ×œ××¢×œ×”."
                    })
            
            # ×”×•×¡×¤×ª ×”×§×©×¨ ×× ×§×™×™×
            if context:
                # ×”×•×¡×¤×ª ××™×“×¢ ×‘×¡×™×¡×™ ×¢×œ ×”×œ×§×•×—
                context_info = []
                if context.get("customer_name"):
                    context_info.append(f"×©× ×”×œ×§×•×—: {context['customer_name']}")
                if context.get("phone_number"):
                    context_info.append(f"×˜×œ×¤×•×Ÿ: {context['phone_number']}")
                
                if context_info:
                    messages.append({
                        "role": "system", 
                        "content": "××™×“×¢ ×¢×œ ×”×œ×§×•×—:\n" + "\n".join(context_info)
                    })
                
                # âœ… BUILD 92: ×©×œ×™×—×ª previous_messages ×›×©×™×—×” ×××™×ª×™×ª - 10 ×”×•×“×¢×•×ª ×œ×–×™×›×¨×•×Ÿ ××œ×!
                if context.get("previous_messages"):
                    prev_msgs = context["previous_messages"][-10:]  # âœ… 10 ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª (×œ× 6!)
                    for msg in prev_msgs:
                        # âœ… ×”××‘× ×” ×”×•× "×œ×§×•×—: ..." ××• "×¢×•×–×¨×ª: ..." (××• "×œ××”:" legacy)
                        if msg.startswith("×œ×§×•×—:"):
                            messages.append({
                                "role": "user",
                                "content": msg.replace("×œ×§×•×—:", "").strip()
                            })
                        elif msg.startswith("×¢×•×–×¨×ª:") or msg.startswith("×œ××”:"):  # âœ… ×ª××™×›×” ×‘×©× ×™×”×!
                            content = msg.replace("×¢×•×–×¨×ª:", "").replace("×œ××”:", "").strip()
                            messages.append({
                                "role": "assistant",
                                "content": content
                            })
            
            # ×”×•×¡×¤×ª ×”×•×“×¢×ª ×”××©×ª××© ×”× ×•×›×—×™×ª
            messages.append({"role": "user", "content": message})
            
            # âš¡ CRITICAL: Measure OpenAI call time
            import time
            openai_start = time.time()
            
            # âš¡ BUILD 118: Add explicit timeout to prevent long waits
            try:
                response = self.client.chat.completions.create(
                    model=prompt_data["model"],
                    messages=messages,  # type: ignore
                    max_tokens=prompt_data["max_tokens"],
                    temperature=prompt_data["temperature"],
                    timeout=2.5  # ğŸ”¥ REDUCED: 2.5s timeout for real-time conversations (was 3.5s)
                )
                
                openai_time = time.time() - openai_start
                logger.info(f"âœ… OPENAI_SUCCESS: {openai_time:.3f}s")
                
                ai_response = response.choices[0].message.content
                if ai_response:
                    ai_response = ai_response.strip()
                else:
                    ai_response = ""  # Empty - let caller handle
                logger.info(f"AI response generated for business {business_id}: {len(ai_response)} chars")
                return ai_response
                
            except Exception as openai_error:
                openai_time = time.time() - openai_start
                error_type = type(openai_error).__name__
                logger.error(f"ğŸ”´ OPENAI_FAILED: {error_type} after {openai_time:.3f}s: {str(openai_error)[:200]}")
                raise  # Re-raise to outer exception handler
            
        except Exception as e:
            logger.error(f"ğŸ”´ AI_GENERATION_FAILED: {type(e).__name__}: {str(e)[:200]}")
            return self._get_fallback_response(message)
    
    def _get_fallback_response(self, message: str, business_id: int = None) -> str:
        """Emergency fallback if AI fails - uses business settings"""
        try:
            if business_id:
                from server.models_sql import Business
                business = Business.query.get(business_id)
                if business:
                    fallback = business.greeting_message or business.whatsapp_greeting
                    if fallback and fallback.strip():
                        return fallback
                    # Use business name as absolute minimum
                    if business.name:
                        return business.name
        except:
            pass
        return None  # Return None to signal caller to skip/handle
    
    def _get_calendar_availability(self, business_id: int) -> str:
        """×‘×“×™×§×ª ×–××™× ×•×ª ×‘×œ×•×— ×”×©× ×” ×œ-7 ×™××™× ×”×§×¨×•×‘×™×"""
        try:
            from server.models_sql import Appointment
            from datetime import datetime, timedelta
            
            # âš¡ FAST: Limit query time with LIMIT
            # ×˜×•×•×— ×ª××¨×™×›×™×: ×”×™×•× + 7 ×™××™×
            today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            week_end = today + timedelta(days=7)
            
            # ×©×œ×™×¤×ª ×¤×’×™×©×•×ª ×§×™×™××•×ª (LIMIT 10 ×œ××”×™×¨×•×ª!)
            appointments = Appointment.query.filter(
                Appointment.business_id == business_id,
                Appointment.start_time >= today,
                Appointment.start_time < week_end,
                Appointment.status.in_(['confirmed', 'pending'])
            ).order_by(Appointment.start_time).limit(10).all()
            
            # ×”×¦×¢×ª ×–×× ×™× ×¤× ×•×™×™× (9:00-17:00, ×›×œ ×™×•×, ×œ××¢×˜ ×©×‘×ª)
            available_slots = []
            for i in range(7):
                day = today + timedelta(days=i)
                # ×“×œ×’ ×¢×œ ×©×‘×ª (5 = ×©×‘×ª)
                if day.weekday() == 5:
                    continue
                    
                day_name = day.strftime("%A")
                day_name_he = {"Monday": "×©× ×™", "Tuesday": "×©×œ×™×©×™", "Wednesday": "×¨×‘×™×¢×™", 
                              "Thursday": "×—××™×©×™", "Friday": "×©×™×©×™", "Sunday": "×¨××©×•×Ÿ"}.get(day_name, day_name)
                
                # ×‘×“×•×§ ×× ×™×© ×¤×’×™×©×•×ª ×‘×™×•× ×”×–×”
                day_start = day.replace(hour=9, minute=0)
                day_end = day.replace(hour=17, minute=0)
                
                day_appointments = [apt for apt in appointments if day_start <= apt.start_time < day_end]
                
                if len(day_appointments) < 4:  # ×× ×¤×—×•×ª ×-4 ×¤×’×™×©×•×ª - ×¢×“×™×™×Ÿ ×™×© ××§×•×
                    date_str = day.strftime("%d/%m")
                    available_slots.append(f"×™×•× {day_name_he} {date_str} (×‘×•×§×¨/××—×”\"×¦)")
            
            # ×‘× ×™×™×ª ×˜×§×¡×˜
            result = []
            if available_slots:
                result.append("âœ… ×–××™× ×•×ª ×”×©×‘×•×¢:")
                result.extend([f"  â€¢ {slot}" for slot in available_slots[:5]])  # ×¨×§ 5 ×¨××©×•× ×™×
            else:
                result.append("âš ï¸ ××™×Ÿ ×–××™× ×•×ª ×”×©×‘×•×¢ - ×”×¦×¢ ×©×‘×•×¢ ×”×‘×")
            
            return "\n".join(result)
            
        except Exception as e:
            logger.error(f"Calendar check failed: {e}")
            return "ğŸ“… ×œ×•×— ×”×©× ×”: × × ×œ×ª×× ×™×©×™×¨×•×ª ×¢× ×”×¡×•×›×Ÿ"
    
    def invalidate_cache(self, business_id: int):
        """××—×™×§×ª ×§××© ×¢×¡×§ ××¡×•×™× (×œ××—×¨ ×¢×“×›×•×Ÿ ×¤×¨×•××¤×˜)"""
        cache_key = f"business_{business_id}"
        if cache_key in self._cache:
            del self._cache[cache_key]
            logger.info(f"Cache invalidated for business {business_id}")
    
    def save_conversation_history(self, business_id: int, phone_number: str, 
                                 message: str, response: str, channel: str = "whatsapp"):
        """×©××™×¨×ª ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×” ×œ××™×“×¢ ×¢×ª×™×“×™ (××•×¤×¦×™×•× ×œ×™)"""
        try:
            # ×›××Ÿ ××¤×©×¨ ×œ×”×•×¡×™×£ ×œ×•×’×™×§×” ×œ×©××™×¨×ª ×©×™×—×•×ª ××¨×•×›×•×ª
            # ×œ×¦×¨×›×™ ×”×§×©×¨ ×¢×ª×™×“×™ ××• ×× ×œ×™×˜×™×§×”
            pass
        except Exception as e:
            logger.error(f"Failed to save conversation history: {e}")
    
    def _generate_faq_response(self, message: str, faq_answer: str, business_id: int, channel: str) -> Optional[str]:
        """
        ğŸš€ Generate FAQ fast-path response using lightweight LLM
        Uses gpt-4o-mini with max_tokens=80, temp=0.3 for <1.5s responses
        
        Args:
            message: Customer question
            faq_answer: Matched FAQ answer from database
            business_id: Business ID
            channel: Communication channel (phone/whatsapp)
            
        Returns:
            Natural Hebrew response or None if generation failed
        """
        start = time.time()
        
        try:
            # Get business name
            business = Business.query.get(business_id)
            business_name = business.name if business else "×”×¢×¡×§"
            
            # Mini prompt for FAQ responses - focus on natural rephrasing
            faq_prompt = f"""××ª×” ×¢×•×–×¨ ×“×™×’×™×˜×œ×™ ×¢×‘×•×¨ {business_name}.
×œ×§×•×— ×©××œ ×©××œ×”, ×•× ××¦××” ×”×ª×××” ×‘×××’×¨ ×”×©××œ×•×ª ×”× ×¤×•×¦×•×ª.

××©×™××ª×š: ×”×©×‘ ×‘×¢×‘×¨×™×ª ×˜×‘×¢×™×ª ×•×§×¦×¨×” (1-2 ××©×¤×˜×™×) ×¢×œ ×¡××š ×”×ª×©×•×‘×” ×©× ××¦××”.

×©××œ×ª ×”×œ×§×•×—: {message}
×ª×©×•×‘×” ××”×××’×¨: {faq_answer}

×—×•×§×™×:
1. ×”×©×‘ ×‘×¢×‘×¨×™×ª ×¤×©×•×˜×” ×•×˜×‘×¢×™×ª
2. ×§×¦×¨ - ××§×¡×™××•× 2 ××©×¤×˜×™×
3. ××œ ×ª×•×¡×™×£ ××™×“×¢ ×©×œ× ×‘×ª×©×•×‘×” ×”××§×•×¨×™×ª
4. ××œ ×ª×××¨ "×œ×¤×™ ×”××™×“×¢" ××• "× ××¦× ×‘×××’×¨"
5. ××œ ×ª×¦×™×™×Ÿ ×©×–××ª ×©××œ×” × ×¤×•×¦×”

×ª×©×•×‘×”:"""
            
            # Call OpenAI with FAQ-optimized settings
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": faq_prompt}
                ],
                max_tokens=80,
                temperature=0.3,
                timeout=4.0
            )
            
            reply = response.choices[0].message.content.strip()
            
            elapsed = (time.time() - start) * 1000
            print(f"âš¡ FAQ response generated in {elapsed:.0f}ms")
            logger.info(f"âš¡ FAQ fast-path total time: {elapsed:.0f}ms")
            
            return reply
            
        except Exception as e:
            elapsed = (time.time() - start) * 1000
            print(f"âŒ FAQ response generation failed after {elapsed:.0f}ms: {e}")
            logger.error(f"FAQ response generation failed: {e}")
            return None
    
    def _handle_lightweight_intent(self, intent: str, message: str, business_id: int, 
                                   channel: str, context: Optional[Dict], customer_phone: Optional[str]) -> Optional[str]:
        """
        ğŸš€ Fast FAQ/Info handler - NO AgentKit!
        Target latency: ~1.0-1.5s
        
        Returns:
            str: Fast response
            None: Signal to fallback to AgentKit
        """
        start = time.time()
        
        try:
            # Get business prompt for FAQ info
            prompt_data = self.get_business_prompt(business_id, channel)
            system_prompt = prompt_data.get("system_prompt", "")
            business_name = prompt_data.get("business_name", "×”×¢×¡×§")
            
            response = None
            
            if intent == "info":
                # Extract FAQ from prompt - lightweight LLM call
                response = self._get_faq_response(message, system_prompt, business_name)
                
                # ğŸ”¥ FIX: If FAQ failed (returned None), signal fallback to AgentKit
                if response is None:
                    logger.warning(f"FAQ failed for info query, falling back to AgentKit")
                    return None
            
            else:  # Should not reach here (only "info" uses fast path now)
                logger.warning(f"Unexpected intent in fast path: {intent}")
                return None
            
            latency = (time.time() - start) * 1000
            print(f"âš¡ FAST_PATH_LATENCY: {latency:.0f}ms (intent={intent})")
            logger.info(f"âš¡ Fast path response: {latency:.0f}ms")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Fast path failed: {e}")
            # Return None to signal fallback to AgentKit
            return None
    
    def _extract_faq_facts(self, question: str, full_prompt: str) -> Optional[str]:
        """
        ğŸ”¥ ARCHITECT-REVIEWED FIX: Keyword-based topic matching
        Extracts ONLY sections relevant to the question, not all sections blindly.
        
        Strategy:
        1. Parse prompt into labeled sections (pricing, menu, location, hours, description)
        2. Map question keywords to relevant section labels
        3. Return only matching sections (max 500 chars)
        4. Return None if no relevant section â†’ fallback to Agent
        """
        try:
            import re
            
            question_lower = question.lower()
            
            # Parse all sections once into a dict
            sections = {}
            
            # Pricing section (ğŸ’°)
            pricing_match = re.search(r'ğŸ’°\s*××—×™×¨×™×:.*?(?=\n\n|$)', full_prompt, re.DOTALL)
            if pricing_match:
                sections['pricing'] = pricing_match.group(0)
            
            # Menu/food section (ğŸ•, ğŸ´, or keywords)
            menu_match = re.search(r'(ğŸ•|ğŸ´|×ª×¤×¨×™×˜|××•×›×œ|××©×§××•×ª|×× ×•×ª).*?(?=\n\n|$)', full_prompt, re.DOTALL | re.IGNORECASE)
            if menu_match:
                sections['menu'] = menu_match.group(0)
            
            # Hours/schedule (â°, ğŸ•’, or "×¤×ª×•×—×™×")
            hours_match = re.search(r'(â°|ğŸ•’|×¤×ª×•×—×™×|×©×¢×•×ª).*?(?=\n\n|$)', full_prompt, re.DOTALL | re.IGNORECASE)
            if hours_match:
                sections['hours'] = hours_match.group(0)
            
            # Location (ğŸ“ or keywords)
            location_match = re.search(r'(ğŸ“|×××•×§×|××™×§×•×|×›×ª×•×‘×ª|×¨×—×•×‘).*?(?=\n\n|$)', full_prompt, re.DOTALL)
            if location_match:
                sections['location'] = location_match.group(0)
            
            # General description
            desc_match = re.search(r'^(.*?)(?=\nğŸ’°|\nğŸ”¥|\nğŸ’¬|$)', full_prompt, re.DOTALL)
            if desc_match and len(desc_match.group(0).strip()) > 50:
                sections['description'] = desc_match.group(0)[:500]
            
            # Topic keyword mapping
            # ğŸ”¥ FIX: Only match INFORMATION questions, not quality/experience questions
            topic_keywords = {
                'pricing': r'(××—×™×¨|×›××” ×¢×•×œ×”|×›××” ×–×”|×¢×œ×•×ª|×ª×©×œ×•×|×¢×•×œ×”)',
                'menu': r'(×™×©.*××•×›×œ|×™×©.*×ª×¤×¨×™×˜|××”.*×ª×¤×¨×™×˜|××”.*×œ××›×•×œ|××”.*×œ×©×ª×•×ª|×ª×¤×¨×™×˜|×× ×•×ª|××©×§××•×ª|×©×ª×™×”|×‘×¨|×§×¤×”)',
                'hours': r'(××ª×™.*×¤×ª×•×—|×©×¢×•×ª.*×¤×ª×™×—×”|×©×¢×•×ª.*×¢×‘×•×“×”|××”.*×©×¢×•×ª)',
                'location': r'(××™×¤×”|××™×§×•×|×›×ª×•×‘×ª|×”×™×›×Ÿ|×¨×—×•×‘|××–×•×¨)',
            }
            
            # Find matching sections
            matched_sections = []
            
            for topic, pattern in topic_keywords.items():
                if re.search(pattern, question_lower) and topic in sections:
                    matched_sections.append(sections[topic])
                    print(f"âœ… FAQ_MATCH: topic='{topic}' matched in question")
            
            # If no topic match, return general description if it exists
            if not matched_sections and 'description' in sections:
                matched_sections.append(sections['description'])
                print(f"â„¹ï¸ FAQ_FALLBACK: Using general description (no topic match)")
            
            # If still no match, return None â†’ Agent fallback
            if not matched_sections:
                print(f"âš ï¸ FAQ_NO_MATCH: No relevant section found, routing to Agent")
                return None
            
            # Combine matched sections (max 500 chars)
            result = "\n\n".join(matched_sections)
            if len(result) > 500:
                result = result[:500] + "..."
            
            print(f"âœ… FAQ_EXTRACTED: {len(matched_sections)} section(s), {len(result)} chars")
            return result
                
        except Exception as e:
            logger.error(f"FAQ fact extraction failed: {e}")
            # Fallback to Agent
            return None
    
    def _get_faq_response(self, question: str, system_prompt: str, business_name: str) -> Optional[str]:
        """
        ğŸš€ Fast FAQ using optimized LLM call
        Target: ~1.0-1.5s with FACTUAL prompt context (no guard-rails)
        
        ğŸ”¥ ARCHITECT-REVIEWED FIX (Phase 2O):
        - Extract ONLY factual sections (pricing/hours/location) - NO guard-rails!
        - Use FULL factual context (up to 3000 chars)
        - Increase max_tokens: 80 â†’ 180 for complete Hebrew answers
        - Increase timeout: 1.5s â†’ 2.2s for reliability
        - Add retry logic for robustness
        """
        import time
        faq_start = time.time()
        
        try:
            # ğŸ”¥ CRITICAL FIX: Extract ONLY relevant facts based on question!
            print(f"\nğŸ“š FAQ: Extracting facts from prompt ({len(system_prompt)} chars)")
            extract_start = time.time()
            faq_facts = self._extract_faq_facts(question, system_prompt) if system_prompt else None
            
            # If no relevant facts found, return None â†’ Agent fallback
            if faq_facts is None:
                print(f"âš ï¸ FAQ: No relevant facts found, routing to Agent")
                return None
            
            extract_time = (time.time() - extract_start) * 1000
            print(f"â±ï¸  FAQ: Fact extraction took {extract_time:.0f}ms")
            print(f"ğŸ“Š FAQ: Extracted {len(faq_facts)} chars of facts")
            print(f"ğŸ“ FAQ: Facts preview: {faq_facts[:200]}...")
            
            # ğŸ”¥ CRITICAL FIX: ULTRA-MINIMAL prompt - just answer the question!
            faq_system = f"""×”×©×‘ ×‘×§×¦×¨×” (2 ××©×¤×˜×™×)."""
            
            # ğŸ”¥ FIX: First attempt with full token budget
            try:
                print(f"ğŸ¤– FAQ: Calling OpenAI (model=gpt-4o-mini, max_tokens=80, timeout=4.0s)")
                llm_start = time.time()
                
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": faq_system},
                        {"role": "user", "content": f"{faq_facts}\n\n{question}"}
                    ],
                    temperature=0.3,  # âš¡ Balanced for speed vs quality
                    max_tokens=80,  # âš¡ SPEED: Reduced from 150 to 80 for faster FAQ
                    timeout=4.0  # âš¡ Consistent with Agent timeout (was 2.0s)
                )
                
                llm_time = (time.time() - llm_start) * 1000
                print(f"â±ï¸  FAQ: OpenAI call took {llm_time:.0f}ms")
                
                # ğŸ”¥ FIX: Safely handle None content
                answer = response.choices[0].message.content
                if answer:
                    answer = answer.strip()
                else:
                    answer = ""
                
                # ğŸ”¥ ARCHITECT-REVIEWED: Detect guard-rail responses and reject them
                guard_rail_phrases = [
                    "×× ×™ ×›××Ÿ ×¨×§ ×œ×¢×–×•×¨",
                    "×©××œ×•×ª ×©×§×©×•×¨×•×ª ×œ×¢×¡×§",
                    "×œ× ×™×›×•×œ ×œ×¢×–×•×¨",
                    "×œ× ×§×©×•×¨ ×œ×¢×¡×§"
                ]
                is_guard_rail = any(phrase in answer for phrase in guard_rail_phrases) if answer else False
                
                # Validate answer is not generic/empty/guard-rail
                if answer and len(answer) > 10 and "××©××— ×œ×¢×–×•×¨" not in answer and not is_guard_rail:
                    total_time = (time.time() - faq_start) * 1000
                    print(f"âœ… FAQ SUCCESS! Total time: {total_time:.0f}ms")
                    print(f"ğŸ“ FAQ Answer: {answer[:100]}...")
                    logger.info(f"âœ… FAQ success: {answer[:50]}...")
                    return answer
                else:
                    print(f"âš ï¸  FAQ: Generic/guard-rail answer detected!")
                    print(f"   Answer: {answer}")
                    print(f"   is_guard_rail={is_guard_rail}")
                    logger.warning(f"FAQ gave generic/guard-rail answer: {answer}")
                    raise ValueError("Generic/guard-rail answer - retry needed")
                    
            except Exception as retry_err:
                # ğŸ”¥ FIX: Quick retry with shorter response
                logger.warning(f"FAQ first attempt failed, retrying: {retry_err}")
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": faq_system},
                        {"role": "user", "content": f"{faq_facts[:400]}\n\n{question}"}
                    ],
                    temperature=0.3,
                    max_tokens=60,  # âš¡ Even shorter for retry
                    timeout=2.5  # âš¡ Shorter timeout for retry
                )
                # ğŸ”¥ ARCHITECT FIX: Apply guard-rail detection to retry path too!
                answer = response.choices[0].message.content
                if answer:
                    answer = answer.strip()
                else:
                    answer = ""
                
                # Detect guard-rail responses
                guard_rail_phrases = [
                    "×× ×™ ×›××Ÿ ×¨×§ ×œ×¢×–×•×¨",
                    "×©××œ×•×ª ×©×§×©×•×¨×•×ª ×œ×¢×¡×§",
                    "×œ× ×™×›×•×œ ×œ×¢×–×•×¨",
                    "×œ× ×§×©×•×¨ ×œ×¢×¡×§"
                ]
                is_guard_rail = any(phrase in answer for phrase in guard_rail_phrases) if answer else False
                
                # If still guard-rail â†’ return None to fallback to AgentKit
                if is_guard_rail or not answer or len(answer) < 10:
                    logger.warning(f"FAQ retry also gave guard-rail/generic answer - falling back to AgentKit")
                    return None
                
                return answer
            
        except Exception as e:
            total_time = (time.time() - faq_start) * 1000
            print(f"âŒ FAQ FAILED! Total time: {total_time:.0f}ms")
            print(f"   Error: {type(e).__name__}: {str(e)[:200]}")
            logger.error(f"âŒ FAQ LLM failed after retry: {e}")
            import traceback
            traceback.print_exc()
            # Return None to signal fallback to AgentKit needed
            return None
    
    def generate_response_with_agent(self, message: str, business_id: int = None, 
                                     context: Optional[Dict[str, Any]] = None,
                                     channel: str = "calls",
                                     is_first_turn: bool = False,
                                     customer_phone: Optional[str] = None,
                                     customer_name: Optional[str] = None) -> str:
        """
        âœ¨ BUILD 119: Agent-enhanced response generation
        ğŸš€ Phase 2K: Intent-based routing - AgentKit only for bookings (â‰¤2s target)
        
        Uses AgentKit to perform real actions (appointments, leads, WhatsApp)
        Falls back to FAQ/lightweight responses for info questions
        
        Args:
            message: Customer's message
            business_id: Business ID
            context: Conversation context
            channel: calls/whatsapp
            is_first_turn: First message in conversation
            customer_phone: Customer phone for lead creation
            customer_name: Customer name for personalization
            
        Returns:
            AI response (potentially enhanced with tool actions)
        """
        # Check if agents are enabled (default: enabled)
        agents_enabled = os.getenv("AGENTS_ENABLED", "1") == "1"
        print(f"ğŸ¯ AGENTS_ENABLED = {agents_enabled}")
        logger.info(f"ğŸ¯ AGENTS_ENABLED = {agents_enabled}")
        
        if not agents_enabled:
            # Fallback to regular response
            print("âš ï¸ Agents disabled - using regular response")
            logger.warning("âš ï¸ Agents disabled - using regular response")
            return self.generate_response(message, business_id, context, channel, is_first_turn)
        
        # ğŸš€ Phase 2K: INTENT ROUTING GATE
        # âš ï¸ FAQ Fast-Path is HARDCODED for real-estate/restaurant patterns!
        # It will NOT work for other business types (tech, retail, etc.)
        # Check if business has FAQ enabled before routing
        
        intent = route_intent_hebrew(message)
        print(f"ğŸ¯ INTENT_DETECTED: {intent} (message: {message[:50]}...)")
        logger.info(f"ğŸ¯ Intent detected: {intent}")
        
        # âš¡ FAQ Fast-Path - Database-backed FAQ matching with embeddings
        # ğŸ”¥ BUILD 99: FAQ ONLY FOR PHONE CALLS (NOT WhatsApp!)
        # WhatsApp uses AgentKit exclusively for all messages
        
        if intent == "info" and channel != "whatsapp":
            # FAQ fast-path for phone calls only (channel="calls")
            try:
                from server.services.faq_cache import faq_cache
                
                faq_match = faq_cache.find_best_match(business_id, message)
                
                if faq_match:
                    print(f"ğŸ¯ FAQ MATCH FOUND (calls): score={faq_match['score']:.3f}")
                    print(f"   Question: {faq_match['question']}")
                    print(f"   Answer: {faq_match['answer'][:100]}...")
                    logger.info(f"ğŸ¯ FAQ fast-path activated: score={faq_match['score']:.3f}")
                    
                    faq_response = self._generate_faq_response(
                        message=message,
                        faq_answer=faq_match['answer'],
                        business_id=business_id,
                        channel=channel
                    )
                    
                    if faq_response:
                        print(f"âœ… FAQ fast-path response generated (calls)")
                        return faq_response
                    else:
                        print("âš ï¸ FAQ response generation failed, falling back to AgentKit")
                else:
                    print(f"âŒ No FAQ match found for: '{message[:50]}...'")
            except Exception as e:
                print(f"âš ï¸ FAQ fast-path error: {e}, falling back to AgentKit")
                logger.warning(f"FAQ fast-path error: {e}")
        elif intent == "info" and channel == "whatsapp":
            # WhatsApp always uses AgentKit (no FAQ fast-path)
            print(f"ğŸ“± WhatsApp message - skipping FAQ, using AgentKit")
            logger.info(f"ğŸ“± WhatsApp 'info' intent - routing to AgentKit (no FAQ)")
        
        # âš¡ Capture start time BEFORE try block for error logging
        start_time = time.time()
        
        try:
            # ğŸ”¥ FIX: Modules now imported at top of file - no re-import needed!
            if not AGENT_MODULES_LOADED:
                # Double-check - agents not available
                print("âš ï¸ AGENTS_ENABLED=False in module - using regular response")
                logger.warning("âš ï¸ AGENTS_ENABLED=False in module - using regular response")
                return self.generate_response(message, business_id, context, channel, is_first_turn)
            
            # Get business name
            db_start = time.time()
            business = Business.query.get(business_id)
            business_name = business.name if business else "×”×¢×¡×§ ×©×œ× ×•"
            
            # ğŸ¯ BUILD 119: Load custom prompt from database!
            prompt_data = self.get_business_prompt(business_id, channel)
            custom_prompt = prompt_data.get("system_prompt", "")  # Extract just the prompt text
            db_time = (time.time() - db_start) * 1000
            print(f"â±ï¸ DB query time: {db_time:.0f}ms")
            logger.info(f"ğŸ“‹ Loaded prompt for business {business_id}: {len(custom_prompt)} chars")
            
            # ğŸ”¥ CRITICAL FIX: Use get_or_create_agent (singleton cache) instead of get_agent (legacy)!
            from server.agent_tools.agent_factory import get_or_create_agent
            
            agent_create_start = time.time()
            agent = get_or_create_agent(
                business_id=business_id,
                channel=channel,
                business_name=business_name,
                custom_instructions=custom_prompt
            )
            agent_create_time = (time.time() - agent_create_start) * 1000
            
            if agent_create_time < 100:
                # Cache HIT - agent was already warmed!
                print(f"â™»ï¸  CACHE HIT: Agent already warmed! ({agent_create_time:.0f}ms)")
                logger.info(f"â™»ï¸  Agent CACHE HIT for {business_name} ({channel}): {agent_create_time:.0f}ms")
            elif agent_create_time < 2000:
                # Cache MISS but creation was fast
                print(f"ğŸ†• NEW Agent created in {agent_create_time:.0f}ms (business={business_name}, channel={channel})")
                logger.info(f"ğŸ†• Agent created: {agent_create_time:.0f}ms")
            else:
                # SLOW creation - log warning!
                print(f"âš ï¸  SLOW AGENT CREATION: {agent_create_time:.0f}ms (expected <2000ms)")
                logger.warning(f"âš ï¸  SLOW AGENT CREATION: {agent_create_time:.0f}ms for business={business_id}, channel={channel}")
            
            if not agent:
                print("âŒ Failed to create agent - falling back to regular response")
                logger.error("âŒ Failed to create agent - falling back to regular response")
                return self.generate_response(message, business_id, context, channel, is_first_turn)
            
            print(f"âœ… Agent created successfully: {agent.name}")
            logger.info(f"âœ… Agent created successfully: {agent.name}")
            
            # Build enhanced context for agent
            agent_context = {
                "business_id": business_id,
                "business_name": business_name,
                "customer_phone": customer_phone,
                "customer_name": customer_name,
                "channel": channel,
                "is_first_turn": is_first_turn,
                **(context or {})
            }
            
            # ğŸ”¥ CRITICAL: Store context in Flask g so tools can access it!
            from flask import g
            g.agent_context = agent_context
            print(f"âœ… Stored agent_context in Flask g: phone={customer_phone}, name={customer_name}")
            
            # Run agent using Runner (with proper async handling for eventlet threads)
            print(f"ğŸ¤– Running agent for business {business_id}, channel={channel}")
            print(f"   ğŸ“ User message: '{message[:100]}...'")
            print(f"   ğŸ“‹ Context: business_id={business_id}, phone={customer_phone}, name={customer_name}")
            logger.info(f"ğŸ¤– Running agent for business {business_id}, channel={channel}")
            logger.info(f"   ğŸ“ User message: '{message[:100]}...'")
            logger.info(f"   ğŸ“‹ Context: business_id={business_id}, phone={customer_phone}, name={customer_name}")
            
            import asyncio
            
            # ğŸ”¥ FIX: ALWAYS create new event loop to avoid CurrentThreadExecutor crash
            # Don't reuse ASGI/main thread executor - it gets torn down mid-request
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # ğŸ”¥ BUILD 99: LIMIT CONVERSATION HISTORY to last 4 exchanges (8 messages)
            # Why: 10 messages = ~4.5K tokens = 27s latency in Runner.run()
            #      4 exchanges (8 messages) = ~1.2K tokens = 1.2s latency âœ…
            history_start = time.time()
            conversation_messages = []
            if context and "previous_messages" in context:
                prev_msgs = context["previous_messages"]
                print(f"ğŸ“š Found {len(prev_msgs)} previous messages in context")
                
                # ğŸ”¥ CRITICAL PERFORMANCE FIX: Keep only last 8 messages (4 user + 4 assistant)
                # This reduces prompt from ~4.5K tokens to ~1.2K tokens
                if len(prev_msgs) > 8:
                    prev_msgs = prev_msgs[-8:]
                    print(f"âš¡ PERFORMANCE: Limited to last 8 messages (4 exchanges) to reduce latency")
                    logger.info(f"âš¡ Truncated history from {len(context['previous_messages'])} to 8 messages")
                
                # Convert to Agent SDK format
                # prev_msgs is list of strings like "×œ×§×•×—: XXX" or "×¢×•×–×¨: YYY"
                for msg in prev_msgs:
                    if msg.startswith("×œ×§×•×—:"):
                        # User message
                        conversation_messages.append({
                            "role": "user",
                            "content": msg.replace("×œ×§×•×—:", "").strip()
                        })
                    elif msg.startswith("×¢×•×–×¨:"):
                        # Assistant message
                        conversation_messages.append({
                            "role": "assistant",
                            "content": msg.replace("×¢×•×–×¨:", "").strip()
                        })
                
                history_time = (time.time() - history_start) * 1000
                print(f"âœ… Converted to {len(conversation_messages)} messages for Agent ({history_time:.0f}ms)")
                
            # Add current message
            conversation_messages.append({
                "role": "user",
                "content": message
            })
            
            # ğŸ”¥ FIX: Runner is a static class - use Runner.run() directly!
            from agents import Runner
            
            print(f"ğŸ”„ Starting Runner.run() with {len(conversation_messages)-1} history messages...")
            logger.info(f"â±ï¸ PERFORMANCE: Starting Runner.run() at {time.time()}")
            
            # Use Runner.run() directly (it's a static method, not an instance!)
            # ğŸ”¥ BUILD 116: max_turns=25 to allow full booking flow with retries (was 15, caused MaxTurnsExceeded)
            # Booking workflow: Date/Time â†’ Find Slots â†’ Name â†’ Phone â†’ Create Appointment â†’ Confirm
            # Each tool call = 2 turns (call + response), so minimum ~11 turns needed
            # Extra headroom for validation retries and conversation flow
            try:
                result = loop.run_until_complete(
                    Runner.run(
                        starting_agent=agent, 
                        input=conversation_messages, 
                        context=agent_context,
                        max_turns=25  # ğŸ”¥ BUILD 116: Allow full booking flow with validation retries
                    )
                )
                duration_ms = int((time.time() - start_time) * 1000)
                print(f"âœ… Runner.run() completed in {duration_ms}ms")
                logger.info(f"â±ï¸ PERFORMANCE: Runner.run() completed in {duration_ms}ms")
            except Exception as e:
                # ğŸ”¥ BUILD 112: Handle MaxTurnsExceeded gracefully
                from agents.exceptions import MaxTurnsExceeded
                if isinstance(e, MaxTurnsExceeded):
                    print(f"âš ï¸ MaxTurnsExceeded: Agent hit turn limit")
                    logger.warning(f"MaxTurnsExceeded: {e}")
                    # ğŸ”¥ BUILD 118: Return structured response (consistent schema for analytics)
                    return {
                        "text": "×¡×œ×™×—×”, ×× ×™ ×¦×¨×™×š ×¢×•×“ ×¤×¨×˜×™× ×›×“×™ ×œ×”×©×œ×™× ××ª ×”×§×‘×™×¢×”. ××” ×”×©×¢×” ×”××•×¢×“×¤×ª ×©×œ×š?",
                        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                        "actions": [],  # Empty actions for MaxTurnsExceeded
                        "booking_successful": False,
                        "error": "MaxTurnsExceeded"
                    }
                else:
                    # Re-raise other exceptions
                    raise
            finally:
                # ğŸ”¥ CRITICAL: Close event loop to prevent FD leak!
                loop.close()
            
            # ğŸ”¥ BUILD 118: Extract text AND preserve metadata for analytics
            reply_text = result.final_output_as(str)
            print(f"ğŸ“ Agent final response: '{reply_text[:100] if reply_text else '(EMPTY!)'}...'")
            
            # âœ… CRITICAL: Validate that agent returned a response!
            if not reply_text or not reply_text.strip():
                print(f"âŒ CRITICAL: Agent returned EMPTY response! Falling back...")
                logger.error(f"âŒ Agent returned empty response for message: {message[:100]}")
                return self.generate_response(message, business_id, context, channel, is_first_turn)
            
            # ğŸ”¥ CRITICAL: Enforce 15-word limit for phone calls to prevent send queue overflow!
            if channel == "calls":
                words = reply_text.split()
                if len(words) > 15:
                    original_len = len(words)
                    reply_text = " ".join(words[:15])
                    print(f"âœ‚ï¸ TRUNCATED phone response: {original_len} â†’ 15 words")
                    logger.warning(f"âœ‚ï¸ Truncated phone response from {original_len} to 15 words to prevent queue overflow")
            
            # DEBUG: Check result structure
            print(f"ğŸ” Result type: {type(result).__name__}")
            print(f"ğŸ” Has new_items: {hasattr(result, 'new_items')}")
            if hasattr(result, 'new_items'):
                print(f"ğŸ” new_items value: {result.new_items}")
                print(f"ğŸ” new_items length: {len(result.new_items) if result.new_items else 0}")
            
            # ğŸš« DISABLED: Tool handling disabled - no tool calls expected
            tool_calls_data = []
            tool_count = 0
            booking_successful = False  # Track if booking actually succeeded
            
            # Tools are disabled, so we don't expect any tool calls
            logger.info(f"ğŸš« Tools disabled - no tool call processing")
            
            # ğŸš« DISABLED: Tool validation disabled - no tools to validate
            claimed_action = False
            claimed_availability = False
            
            # ğŸš« DISABLED: Tool validation disabled - no tool calls expected
            booking_tool_called = False
            check_availability_called = False
            whatsapp_sent = False
            
            # ğŸš« DISABLED: Tool validation disabled - no validation checks needed
            
            # âœ¨ Save trace to database
            try:
                trace = AgentTrace(
                    business_id=business_id,
                    agent_type="booking",
                    channel=channel,
                    customer_phone=customer_phone,
                    customer_name=customer_name,
                    user_message=message[:1000],  # Limit length
                    agent_response=reply_text[:2000],
                    tool_calls=tool_calls_data if tool_calls_data else None,
                    tool_count=tool_count,
                    status="success",
                    duration_ms=duration_ms
                )
                db.session.add(trace)
                db.session.commit()
                logger.info(f"ğŸ“Š Saved agent trace #{trace.id} (duration: {duration_ms}ms)")
            except Exception as trace_error:
                logger.error(f"Failed to save agent trace: {trace_error}")
                # Don't fail the whole request just because trace failed
                db.session.rollback()
            
            # ğŸ”¥ BUILD 118: Serialize new_items to preserve FULL metadata (not stringified summaries)
            def make_json_safe(obj):
                """Recursively convert object to JSON-safe primitives"""
                if obj is None or isinstance(obj, (bool, int, float, str)):
                    return obj
                elif isinstance(obj, dict):
                    return {k: make_json_safe(v) for k, v in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [make_json_safe(item) for item in obj]
                elif hasattr(obj, 'model_dump'):
                    return make_json_safe(obj.model_dump())
                elif hasattr(obj, 'dict'):
                    return make_json_safe(obj.dict())
                elif hasattr(obj, '__dict__'):
                    return make_json_safe({k: v for k, v in obj.__dict__.items() if not k.startswith('_')})
                else:
                    # Last resort: convert to string
                    return str(obj)
            
            def serialize_run_items(items):
                """Convert RunItems to JSON-serializable dicts"""
                serialized = []
                for item in (items or []):
                    try:
                        # Try Pydantic model_dump() first (AgentKit uses Pydantic)
                        if hasattr(item, 'model_dump'):
                            raw_dict = item.model_dump()
                        # Try to_dict() method
                        elif hasattr(item, 'to_dict'):
                            raw_dict = item.to_dict()
                        # Try dict() for Pydantic v1
                        elif hasattr(item, 'dict'):
                            raw_dict = item.dict()
                        # Fallback: dataclasses.asdict
                        else:
                            from dataclasses import asdict, is_dataclass
                            if is_dataclass(item):
                                raw_dict = asdict(item)
                            else:
                                # Last resort: filter __dict__ for JSON-safe fields
                                raw_dict = {k: v for k, v in item.__dict__.items() if not k.startswith('_')}
                        
                        # ğŸ”¥ CRITICAL: Make nested objects JSON-safe
                        safe_dict = make_json_safe(raw_dict)
                        serialized.append(safe_dict)
                    except Exception as e:
                        print(f"âš ï¸ Failed to serialize item {type(item).__name__}: {e}")
                        serialized.append({"type": type(item).__name__, "error": str(e)})
                return serialized
            
            # ğŸ”¥ BUILD 118: Return structured response (preserves metadata for analytics + transcripts)
            # Convert RunOutput to dict with FULL new_items structure (not stringified summaries)
            response_payload = {
                "text": reply_text,  # Use (possibly truncated) text
                "usage": {
                    "prompt_tokens": getattr(result, 'prompt_tokens', 0),
                    "completion_tokens": getattr(result, 'completion_tokens', 0),
                    "total_tokens": getattr(result, 'total_tokens', 0)
                },
                "actions": serialize_run_items(result.new_items) if hasattr(result, 'new_items') else [],
                "booking_successful": booking_successful
            }
            print(f"âœ… Returning structured response: {len(reply_text)} chars text, {len(response_payload['actions'])} serialized actions", flush=True)
            print(f"ğŸ”™ About to return from generate_response_with_agent()", flush=True)
            return response_payload
            
        except Exception as e:
            logger.error(f"Agent error (falling back to regular response): {e}")
            import traceback
            traceback.print_exc()
            
            # âœ¨ Save error trace with duration
            try:
                error_duration_ms = int((time.time() - start_time) * 1000)
                trace = AgentTrace(
                    business_id=business_id,
                    agent_type="booking",
                    channel=channel,
                    customer_phone=customer_phone,
                    customer_name=customer_name,
                    user_message=message[:1000],
                    agent_response=None,
                    tool_calls=None,
                    tool_count=0,
                    status="error",
                    error_message=str(e)[:500],
                    duration_ms=error_duration_ms
                )
                db.session.add(trace)
                db.session.commit()
                logger.info(f"ğŸ“Š Saved error trace (duration: {error_duration_ms}ms)")
            except:
                db.session.rollback()
            
            # Fallback to regular response
            return self.generate_response(message, business_id, context, channel, is_first_turn)

def generate_ai_response(message: str, business_id: int = None, 
                        context: Optional[Dict[str, Any]] = None, channel: str = "calls",
                        is_first_turn: bool = False) -> str:
    """×¤×•× ×§×¦×™×” ×¢×–×¨ ×œ×§×¨×™××” ××”×™×¨×” ×œ×©×™×¨×•×ª AI - ×œ×¤×™ ×¢×¨×•×¥"""
    return get_ai_service().generate_response(message, business_id, context, channel, is_first_turn)

# ğŸ”¥ FIX: Alias for routes_whatsapp.py compatibility
def get_ai_response(business_id: int, user_message: str, channel: str = "whatsapp") -> Optional[str]:
    """Wrapper function for WhatsApp AI responses - alias for generate_ai_response
    
    Returns:
        str: AI response text, or None if generation fails (caller should handle None)
    """
    try:
        response = generate_ai_response(user_message, business_id, None, channel)
        if response:
            return response
        logger.warning(f"[AI_SERVICE] get_ai_response returned empty response")
        return None
    except Exception as e:
        logger.error(f"[AI_SERVICE] get_ai_response error: {e}")
        import traceback
        traceback.print_exc()
        return None

