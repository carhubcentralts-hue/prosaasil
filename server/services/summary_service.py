"""
Summary Service - AI-powered FULLY DYNAMIC conversation summarization
×©×™×¨×•×ª ×¡×™×›×•× ×—×›× ×•×“×™× ××™ ×œ×—×œ×•×˜×™×Ÿ - ××–×”×” ×›×œ ×¡×•×’ ×¢×¡×§ ×•×©×™×—×” ××•×˜×•××˜×™×ª!
BUILD 144 - Universal Dynamic Summaries
"""
import os
import logging
from typing import Optional

log = logging.getLogger(__name__)


def summarize_conversation(
    transcription: str, 
    call_sid: Optional[str] = None,
    business_type: Optional[str] = None,
    business_name: Optional[str] = None
) -> str:
    """
    ×¡×™×›×•× ×“×™× ××™ ×œ×—×œ×•×˜×™×Ÿ ×©×œ ×©×™×—×” - ××–×”×” ××•×˜×•××˜×™×ª ××ª ×¡×•×’ ×”×©×™×—×” ×•×”×¢×¡×§!
    BUILD 144 - Universal Dynamic Summaries
    BUILD 183 - CRITICAL FIX: Don't hallucinate summaries when no user spoke!
    
    GPT ××–×”×” ×‘×¢×¦××•:
    - ×¡×•×’ ×”×¢×¡×§ (×›×œ ×ª×—×•× - ×”××¢×¨×›×ª ××–×”×” ××•×˜×•××˜×™×ª!)
    - ××˜×¨×ª ×”×©×™×—×”
    - ×¤×¨×˜×™× ×¨×œ×•×•× ×˜×™×™×
    - ×¤×¢×•×œ×•×ª × ×“×¨×©×•×ª
    
    Args:
        transcription: ×”×ª××œ×•×œ ×”××œ× ×©×œ ×”×©×™×—×”
        call_sid: ××–×”×” ×©×™×—×” ×œ×œ×•×’×™×
        business_type: ×¨××– ×¢×œ ×¡×•×’ ×”×¢×¡×§ (××•×¤×¦×™×•× ×œ×™ - GPT ×™×–×”×” ×‘×¢×¦××•)
        business_name: ×©× ×”×¢×¡×§ (××•×¤×¦×™×•× ×œ×™)
        
    Returns:
        ×¡×™×›×•× ××§×¦×•×¢×™ ×“×™× ××™ ×‘×¢×‘×¨×™×ª (80-150 ××™×œ×™×)
        Returns EMPTY STRING if no actual user speech occurred!
    """
    # ğŸ”¥ BUILD 183: Early exit if no transcription
    if not transcription or len(transcription.strip()) < 10:
        log.info(f"ğŸ“Š [SUMMARY] Skipping - no/short transcription for call {call_sid}")
        return ""  # Return empty, NOT fake text!
    
    # ğŸ”¥ BUILD 183 CRITICAL: Check if USER actually spoke in the conversation
    # If only AI spoke (greeting) but user hung up immediately, don't generate summary!
    user_spoke = False
    user_content_length = 0
    
    # Check if transcript has speaker tags (old format: "×œ×§×•×—:", "× ×¦×™×’:") or is continuous (new Whisper format)
    has_speaker_tags = any(
        prefix in transcription 
        for prefix in ['×œ×§×•×—:', 'user:', 'User:', 'Customer:', '× ×¦×™×’:', 'agent:', 'Agent:']
    )
    
    if has_speaker_tags:
        # OLD FORMAT: Parse by speaker tags
        for line in transcription.split('\n'):
            line = line.strip()
            # Check for user speech markers
            if line.startswith('×œ×§×•×—:') or line.startswith('user:') or line.startswith('User:') or line.startswith('Customer:'):
                # Extract content after the prefix
                content = line.split(':', 1)[1].strip() if ':' in line else ""
                # Filter out noise/silence markers
                noise_patterns = ['...', '(×©×§×˜)', '(silence)', '(noise)', '(×¨×¢×©)', '(×œ× ×©××¢)', '(inaudible)']
                if content and len(content) > 2:
                    is_noise = any(noise in content.lower() for noise in noise_patterns)
                    if not is_noise:
                        user_spoke = True
                        user_content_length += len(content)
    else:
        # NEW FORMAT: Continuous transcript without tags (from Whisper)
        # If transcript is long enough, assume real conversation happened
        user_content_length = len(transcription.strip())
        # Consider it a real conversation if > 50 chars (not just greeting)
        if user_content_length > 50:
            user_spoke = True
            log.info(f"ğŸ“Š [SUMMARY] Continuous transcript detected ({user_content_length} chars), treating as real conversation")
    
    # ğŸ”¥ BUILD 183: If no meaningful user speech, return empty (no hallucination!)
    if not user_spoke or user_content_length < 5:
        log.info(f"ğŸ“Š [SUMMARY] Skipping - NO USER SPEECH detected for call {call_sid} (user_spoke={user_spoke}, content_len={user_content_length})")
        return ""  # CRITICAL: Return empty, don't hallucinate!
    
    log.info(f"ğŸ“Š Generating universal dynamic summary for call {call_sid} (user_content: {user_content_length} chars)")
    
    try:
        from openai import OpenAI
        
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        business_context = ""
        if business_name:
            business_context = f"\n\n×©× ×”×¢×¡×§: {business_name}"
        if business_type:
            business_context += f"\n×ª×—×•× ×”×¢×¡×§ (×¨××–): {business_type}"
        
        prompt = f"""××ª×” ××•××—×” ×¡×™×›×•× ×©×™×—×•×ª ×¢×¡×§×™×•×ª ×‘×¢×‘×¨×™×ª. ×¡×›× ××ª ×”×©×™×—×” ×”×‘××” ×‘×¦×•×¨×” **××§×¦×•×¢×™×ª, ××¤×•×¨×˜×ª ×•×“×™× ××™×ª**.

ğŸ¯ **×”×•×¨××•×ª ×—×©×•×‘×•×ª:**
1. **×–×”×” ××•×˜×•××˜×™×ª** ××ª ×¡×•×’ ×”×¢×¡×§/×”×©×™×¨×•×ª ××ª×•×›×Ÿ ×”×©×™×—×” ×¢×¦××”
2. **×”×ª×× ××ª ××‘× ×” ×”×¡×™×›×•×** ×œ×¡×•×’ ×”×©×™×—×” ×©×–×™×”×™×ª
3. **×—×œ×¥ ××ª ×›×œ ×”×¤×¨×˜×™× ×”×¨×œ×•×•× ×˜×™×™×** ×œ×ª×—×•× ×”×¡×¤×¦×™×¤×™
4. **×›×ª×•×‘ ×¡×™×›×•× ×©×™××•×©×™ ×œ××™×© ×”××›×™×¨×•×ª/×”×©×™×¨×•×ª**

ğŸ“‹ **××‘× ×” ×”×¡×™×›×•× ×”× ×“×¨×© (80-150 ××™×œ×™×):**

1. **×¡×•×’ ×”×¤× ×™×™×” ×•×”×ª×—×•×**: (×–×”×” ××•×˜×•××˜×™×ª - ××” ×¡×•×’ ×”×¢×¡×§ ×•××” ×”×œ×§×•×— ××—×¤×©?)

2. **×¤×¨×˜×™× ×¡×¤×¦×™×¤×™×™×**: (×”×ª×× ×œ×ª×—×•× ×©×–×™×”×™×ª!)
   - ×–×”×” ××ª ×”×ª×—×•× ××•×˜×•××˜×™×ª ×•×—×œ×¥ ××ª ×”×¤×¨×˜×™× ×”×¨×œ×•×•× ×˜×™×™×
   - ×¢×‘×•×¨ ×©×™×¨×•×ª×™×: ×¡×•×’ ×”×©×™×¨×•×ª, ××™×§×•×, ×“×—×™×¤×•×ª
   - ×¢×‘×•×¨ ××•×¦×¨×™×: ×¡×•×’ ×”××•×¦×¨, ×›××•×ª, ××¤×¨×˜
   - ×¢×‘×•×¨ ×¤×’×™×©×•×ª: ×¡×•×’ ×”×¤×’×™×©×”, ×–××Ÿ ××•×¢×“×£, × ×•×©×
   - ×¤×¨×˜×™× × ×•×¡×¤×™× ×¡×¤×¦×™×¤×™×™× ×œ×ª×—×•×
   
3. **×¤×¨×˜×™ ×”×œ×§×•×—**: ×©× ×•×××¦×¢×™ ×”×ª×§×©×¨×•×ª (×× × ××¡×¨×•)

4. **×¡×˜×˜×•×¡ ×•××¢×§×‘**:
   - ×”×× × ×§×‘×¢×” ×¤×’×™×©×”/×ª×•×¨/×¤×¢×•×œ×”? (××ª×™?)
   - ×“×—×™×¤×•×ª: ×’×‘×•×”×”/×‘×™× ×•× ×™×ª/× ××•×›×”
   - ×¤×¢×•×œ×•×ª × ×“×¨×©×•×ª ××”×¢×¡×§

5. **×”×¢×¨×•×ª ×—×©×•×‘×•×ª**: ××™×“×¢ × ×•×¡×£ ×¨×œ×•×•× ×˜×™ ×œ×ª×—×•×
{business_context}

ğŸ“ **×ª××œ×•×œ ×”×©×™×—×”:**
{transcription}

ğŸ“ **×¡×™×›×•× ××§×¦×•×¢×™ ×“×™× ××™:**"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": """××ª×” ××•××—×” ×¡×™×›×•× ×©×™×—×•×ª ×¢×¡×§×™×•×ª ×‘×¢×‘×¨×™×ª. 
                    
×”×™×›×•×œ×•×ª ×©×œ×š:
- ××–×”×” ××•×˜×•××˜×™×ª ×›×œ ×¡×•×’ ×¢×¡×§ ×•×©×™×¨×•×ª (×›×œ ×ª×—×•× ××¤×©×¨×™!)
- ××ª××™× ××ª ××‘× ×” ×”×¡×™×›×•× ×œ×ª×—×•× ×”×¡×¤×¦×™×¤×™
- ×—×•×œ×¥ ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘×¦×•×¨×” ×—×›××”
- ×›×•×ª×‘ ×¡×™×›×•××™× ×©×™××•×©×™×™× ×•××§×¦×•×¢×™×™×

×›×œ×œ×™×:
- ×¡×™×›×•× 80-150 ××™×œ×™× ×‘×¢×‘×¨×™×ª
- ××‘× ×” ×‘×¨×•×¨ ×¢× ×›×•×ª×¨×•×ª
- ×”×ª××§×“ ×‘××™×“×¢ ×©×™××•×©×™ ×œ××™×© ×”××›×™×¨×•×ª/×”×©×™×¨×•×ª
- ××œ ×ª××¦×™× ××™×“×¢ ×©×œ× × ×××¨ ×‘×©×™×—×”"""
                },
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0.3
        )
        
        summary = response.choices[0].message.content
        if summary:
            summary = summary.strip()
            word_count = len(summary.split())
            
            if word_count < 50:
                log.warning(f"âš ï¸ Summary too short ({word_count} words) for {call_sid} - using fallback")
                return _fallback_summary(transcription)
            elif word_count > 200:
                log.warning(f"âš ï¸ Summary too long ({word_count} words) for {call_sid} - truncating")
                words = summary.split()
                summary = " ".join(words[:180]) + "..."
            
            log.info(f"âœ… Universal dynamic summary generated for {call_sid}: {word_count} words")
            return summary
        else:
            log.warning(f"âš ï¸ Empty summary for {call_sid}")
            return _fallback_summary(transcription)
            
    except Exception as e:
        log.error(f"âŒ Summary generation failed for {call_sid}: {e}")
        return _fallback_summary(transcription)


def _fallback_summary(transcription: str) -> str:
    """
    ×¡×™×›×•× fallback ×“×™× ××™ (×‘××§×¨×” ×©×œ ×›×©×œ ×‘-AI)
    """
    words = transcription.strip().split()
    
    summary_parts = []
    summary_parts.append("**×¡×•×’ ×”×¤× ×™×™×”**: ×¤× ×™×™×” ×¢×¡×§×™×ª")
    
    if len(words) >= 80:
        content = " ".join(words[:70])
        summary_parts.append(f"\n\n**×ª×•×›×Ÿ ×”×©×™×—×”**: {content}...")
    elif len(words) >= 40:
        summary_parts.append(f"\n\n**×ª×•×›×Ÿ ×”×©×™×—×”**: {transcription.strip()}")
        summary_parts.append("\n\n**×¤×¨×˜×™×**: ×œ× ×¦×•×™× ×• ×¤×¨×˜×™× ××œ××™× ×‘×©×™×—×”")
    else:
        summary_parts.append(f"\n\n**×ª×•×›×Ÿ ×”×©×™×—×”**: {transcription.strip()}")
        summary_parts.append("\n\n**×¤×¨×˜×™×**: ×”××™×“×¢ ×‘×©×™×—×” ×”×™×” ××•×’×‘×œ, ×™×© ×¦×•×¨×š ×‘××¢×§×‘ × ×•×¡×£")
        summary_parts.append("\n\n**×¤×¨×˜×™ ×§×©×¨**: ×œ× × ××¡×¨×• ×¤×¨×˜×™ ×§×©×¨ ××¤×•×¨×©×™×")
    
    summary_parts.append("\n\n**×¡×˜×˜×•×¡ ×•××¢×§×‘**: ×œ× × ×§×‘×¢×” ×¤×’×™×©×”. ××•××œ×¥ ×œ×—×–×•×¨ ×œ×œ×§×•×— ×•×œ×§×‘×œ ×¤×¨×˜×™× × ×•×¡×¤×™×.")
    summary_parts.append("\n\n**×”×¢×¨×”**: ×¡×™×›×•× ××•×˜×•××˜×™ (××¢×¨×›×ª AI ×–×× ×™×ª ×œ× ×–××™× ×”)")
    
    fallback = "\n".join(summary_parts)
    word_count = len(fallback.split())
    log.info(f"ğŸ“‹ Fallback summary created: {word_count} words")
    return fallback


def extract_lead_info(transcription: str, business_type: Optional[str] = None) -> dict:
    """
    ×—×™×œ×•×¥ ××™×“×¢ ×—×©×•×‘ ××”×©×™×—×” - ×“×™× ××™ ×œ×—×œ×•×˜×™×Ÿ
    
    Args:
        transcription: ×”×ª××œ×•×œ ×”××œ×
        business_type: ×¨××– ×¢×œ ×¡×•×’ ×”×¢×¡×§ (××•×¤×¦×™×•× ×œ×™)
        
    Returns:
        dict ×¢× ××™×“×¢ ×¨×œ×•×•× ×˜×™
    """
    if not transcription or len(transcription.strip()) < 10:
        return {}
    
    try:
        from openai import OpenAI
        
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        prompt = f"""×—×œ×¥ ××™×“×¢ ××”×©×™×—×” ×”×‘××”.

×ª××œ×•×œ:
{transcription}

×–×”×” ××•×˜×•××˜×™×ª ××ª ×¡×•×’ ×”×¢×¡×§/×”×©×™×¨×•×ª ×•×”×—×–×¨ JSON ×¢× ××™×“×¢ ×¨×œ×•×•× ×˜×™.

×”×—×–×¨ JSON ×‘×¤×•×¨××˜:
{{
  "detected_business_type": "×¡×•×’ ×”×¢×¡×§ ×©×–×™×”×™×ª",
  "request_type": "×¡×•×’ ×”×‘×§×©×”/×¤× ×™×™×”",
  "key_details": "×¤×¨×˜×™× ×¢×™×§×¨×™×™× ×¨×œ×•×•× ×˜×™×™× ×œ×ª×—×•×",
  "customer_name": "×©× ×”×œ×§×•×— ××• null",
  "urgency": "×’×‘×•×”×”/×‘×™× ×•× ×™×ª/× ××•×›×”",
  "meeting_scheduled": true/false,
  "next_action": "×¤×¢×•×œ×” ××•××œ×¦×ª"
}}
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "××ª×” ××•××—×” ×œ×—×™×œ×•×¥ ××™×“×¢ ××©×™×—×•×ª ×¢×¡×§×™×•×ª. ×–×”×” ××•×˜×•××˜×™×ª ××ª ×¡×•×’ ×”×¢×¡×§ ×•×”×—×–×¨ ×¨×§ JSON ×ª×§×™×Ÿ."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200,
            temperature=0.1
        )
        
        import json
        result = json.loads(response.choices[0].message.content)
        log.info(f"âœ… Lead info extracted dynamically: {result}")
        return result
        
    except Exception as e:
        log.error(f"âŒ Lead info extraction failed: {e}")
        return {}
